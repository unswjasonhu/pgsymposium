<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="profile" href="http://gmpg.org/xfn/11">
<link rel="pingback" href="https://pgabstractblog.wordpress.com/xmlrpc.php">

<title>POSTGRADUATE RESEARCH SYMPOSIUM ABSTRACT</title>
<meta name='robots' content='noindex,follow' />
<script type="text/javascript">
/* <![CDATA[ */
function addLoadEvent(func){var oldonload=window.onload;if(typeof window.onload!='function'){window.onload=func;}else{window.onload=function(){oldonload();func();}}}
/* ]]> */
</script>
<link rel='stylesheet' id='all-css-0' href='https://s2.wp.com/_static/??-eJx9kN1uwzAIRl9oHqrWVevFtGdJHOaSGoOCrahvP7JoP1Wq3FiAzjGfDbOGKKViqcAtaG6JioExZbzpJCPGet89R7MneKxluqLBiFW7eA3f3Qbvs6Q/QVLCQVoNn5KzzDDTkHB3R5QJfc7a1YVgHKjDjOzYnsZ6+rGW8uIBt9n+8bM6HfpeJzQLfjI1DvXii3Y9t5Yo6+PBwfvJqhIUqeS2/RZ7dyYsOK34w3LjrjFBWw9Mpfi/Wr1lXLgPfj8cjy+nw+vb+Tx+AfBLwPM=' type='text/css' media='all' />
<link rel='stylesheet' id='minnow-opensans-css'  href='https://fonts.googleapis.com/css?family=Open+Sans%3A300%2C400%2C700%2C700italic%2C400italic%2C300italic%7COpen+Sans+Condensed%3A700%2C700italic&#038;subset=latin%2Clatin-ext' type='text/css' media='all' />
<link rel='stylesheet' id='all-css-2' href='https://s2.wp.com/_static/??-eJx9i8sKwjAQRX/IOERU7EL8lnYY45R5hCYh+PdW3NRNN5d74BzoOaBbJatQX6RUILcJlM28AxtCqW+h0DO6HrGUA2wKbSFLS2wFSH3m3+5piTyI41jZ7Q/CU0Ze9tKFJvG03gSrtcFv9NB7PMfLdYin2zB/AO6oTz8=' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-3' href='https://s1.wp.com/wp-content/mu-plugins/global-print/global-print.css?m=1444132114g' type='text/css' media='print' />
<link rel='stylesheet' id='all-css-4' href='https://s1.wp.com/_static/??/wp-content/mu-plugins/post-flair/sharing/sharing.css,/wp-content/themes/h4/global.css?m=1435590157j' type='text/css' media='all' />
<script type='text/javascript'>
/* <![CDATA[ */
var LoggedOutFollow = {"invalid_email":"Your subscription did not succeed, please try again with a valid email address."};
/* ]]> */
</script>
<script type='text/javascript' src='https://s0.wp.com/_static/??-eJyFzs0KwjAMAOAXsiub8+BBfJb9ZCW1bWqTrujTW0EP4lAIJCRfQnSJCsPk8gysbY1rhnR7pcbyTv8CyqNJg0DjMbzxREEgyNN6GtGBygxpMLVXDy204SKxeGCuaGP6+RKGFaH8ZRYkDtNFJWC8f10dHRkVXTYYWNfawExZ1ELOUdEFZwNSd87+1Pb9vuvaY3uwD0gZb60='></script>
<link rel='stylesheet' id='all-css-0' href='https://s2.wp.com/wp-content/mu-plugins/highlander-comments/style.css?m=1377793621g' type='text/css' media='all' />
<!--[if lt IE 8]>
<link rel='stylesheet' id='highlander-comments-ie7-css'  href='https://s2.wp.com/wp-content/mu-plugins/highlander-comments/style-ie7.css?m=1351637563g&#038;ver=20110606' type='text/css' media='all' />
<![endif]-->
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://pgabstractblog.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />
<link rel='canonical' href='' />
<link rel='shortlink' href='http://wp.me/p6Sfj3-2' />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="article" />
<meta property="og:title" content="" />
<meta property="og:url" content="" />
<meta property="og:description" content="Engineering Postgraduate Research Symposium Abstract Submission School: Mechanical School SID: 3071951 Surname: ALVANDI Given name (s): SAMIRA Research Theme: (1)  Advanced manufacturing (2) Simula..." />
<meta property="article:published_time" content="2015-10-23T00:07:08+00:00" />
<meta property="article:modified_time" content="2015-10-23T00:07:08+00:00" />
<meta property="og:site_name" content="UNSW PG" />
<meta property="og:image" content="https://s0.wp.com/i/blank.jpg" />
<meta property="og:locale" content="en_US" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta name="twitter:card" content="summary" />
<meta property="fb:app_id" content="249643311490" />
<meta property="article:publisher" content="https://www.facebook.com/WordPresscom" />
<link rel="shortcut icon" type="image/x-icon" href="https://s2.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="https://s2.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon-precomposed" href="https://s0.wp.com/i/webclip.png" />
<link rel='openid.server' href='' />
<link rel='openid.delegate' href='' />
	<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>
		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="UNSW PG" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-task" content="name=Subscribe;action-uri=https://pgabstractblog.wordpress.com/feed/;icon-uri=https://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s2.wp.com/i/favicon.ico" /><meta name="title" content="POSTGRADUATE RESEARCH SYMPOSIUM&nbsp;ABSTRACT " />
<meta name="description" content="Engineering Postgraduate Research Symposium Abstract Submission School: Mechanical School SID: 3071951 Surname: ALVANDI Given name (s): SAMIRA Research Theme: (1)  Advanced manufacturing (2) Simulation and modelling Name of Supervisor: Prof.Sami Kara   .  Co-supervisor : Dr.Wen LI Title:     Development of Environmental Value Stream Model for Assessing Sustainability in Manufacturing The manufacturing industry is under high pressure&hellip;" />
<style type="text/css" id="syntaxhighlighteranchor"></style>
</head>

<body class="single single-post postid-2 single-format-standard mp6 customizer-styles-applied highlander-enabled highlander-light">
<div id="page" class="hfeed site">
	<a class="skip-link screen-reader-text" href="#content">Skip to content</a>

	<header id="masthead" class="site-header" role="banner">

		<div class="site-branding">
													<h1 class="site-title"><a href="" rel="home">POSTGRADUATE RESEARCH SYMPOSIUM ABSTRACT</a></h1>
			<h2 class="site-description"></h2>
		</div>
				<div class="slide-menu">
			
	</header><!-- #masthead -->

	<div id="content" class="site-content">

	<div id="primary" class="content-area">
		<main id="main" class="site-main" role="main">

		
			
<article id="post-2" class="post-2 post type-post status-publish format-standard hentry category-uncategorized">
	<header class="entry-header">
		<h1 class="entry-title"></h1>
		<!-- .entry-meta -->
	</header><!-- .entry-header -->

	<div class="entry-content">
		<table width="100%">
<a name="FS01"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS01: Development of Environmental Value Stream Model for Assessing Sustainability in Manufacturing</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>SAMIRA ALVANDI</p>
<p>Prof.Sami Kara; Dr.Wen LI</p>
<p>Mechanical &amp; Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>The manufacturing industry is under high pressure to improve and maintain product quality and process performance under stringent environmental and safety regulations. Additionally, emerging requirements for the reduction in environmental footprint, material, energy, and water consumption have added to these pressures. Companies are more eager than ever to find a tool that facilitates their efforts towards sustainability within their manufacturing systems and to improve their economic and environmental performance.</p>
<p>In this regard, a holistic view of the entire manufacturing system is needed in which the dynamic behaviour of the production processes, supporting services and their interrelationships are considered. Due to the fact that the underlying system is too complex, it is vital to utilise simulation to model the system. Furthermore, a simulation model can be used to study the optimal performance of the system in terms of existing trade-offs between economic and environmental performance measures.</p>
<p>This research attempts to address the shortcomings of current approaches in regards to holistic and multi-dimensional sustainability evaluation of a manufacturing system. This is to enable complex and data intensive analysis of multiple performance measures (economic &amp; environmental) in order to facilitate decision making in the area of manufacturing system improvement and optimisation.</p>
<p>This is achieved by:</p>
<p>a)     Developing a framework that can provide a breakdown of material and energy consumption during manufacturing of a product (from single machine through to aggregated factory level) for better transparency of where, when and how the resources are consumed.</p>
<p>b)     Using simulation to handle the complexity and dynamics involved in modelling and calculation of the material and the energy flows within the manufacturing system.</p>
<p>a.     This supports “hot spot” analysis of environmental and economic performance through generating a value stream map.</p>
<p>b.     This supports “what-if” scenario building by varying product types and batch sizes, changes in process parameters like set up time, job sequencing / load levelling and/or process chain variation.</p>
<p>c)     Using simulation-optimisation with the aim of finding the optimum solution based on batch sizing, load levelling and the process chain that will result in the most desirable outcome in terms of both economic and environmental aspects.</p>
<p>The manufacturing industry is under high pressure to improve and maintain product quality and process performance under stringent environmental and safety regulations. Additionally, emerging requirements for the reduction in environmental footprint, material, energy, and water consumption have added to these pressures. Companies are more eager than ever to find a tool that facilitates their efforts towards sustainability within their manufacturing systems and to improve their economic and environmental performance.</p>
<p>In this regard, a holistic view of the entire manufacturing system is needed in which the dynamic behaviour of the production processes, supporting services and their interrelationships are considered. Due to the fact that the underlying system is too complex, it is vital to utilise simulation to model the system. Furthermore, a simulation model can be used to study the optimal performance of the system in terms of existing trade-offs between economic and environmental performance measures.</p>
<p>This research attempts to address the shortcomings of current approaches in regards to holistic and multi-dimensional sustainability evaluation of a manufacturing system. This is to enable complex and data intensive analysis of multiple performance measures (economic &amp; environmental) in order to facilitate decision making in the area of manufacturing system improvement and optimisation.</p>
<p>This is achieved by:</p>
<p>d)     Developing a framework that can provide a breakdown of material and energy consumption during manufacturing of a product (from single machine through to aggregated factory level) for better transparency of where, when and how the resources are consumed.</p>
<p>e)     Using simulation to handle the complexity and dynamics involved in modelling and calculation of the material and the energy flows within the manufacturing system.</p>
<p>a.     This supports “hot spot” analysis of environmental and economic performance through generating a value stream map.</p>
<p>b.     This supports “what-if” scenario building by varying product types and batch sizes, changes in process parameters like set up time, job sequencing / load levelling and/or process chain variation.</p>
<p>f)      Using simulation-optimisation with the aim of finding the optimum solution based on batch sizing, load levelling and the process chain that will result in the most desirable outcome in terms of both economic and environmental aspects.</td>
</tr>
</tbody>
</table>
<a name="FS02"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS02: Micronisation of Copper Indomethacin Using Supercritical Microfluidic Processes</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yue Huang</p>
<p>Neil R. FOSTER</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Supercritical fluids (SCFs) find useful application in performing green chemistry and engineering, especially in pharmaceutical industry. The concern for sustainable environment was the main impetus for using SCFs. Micro-structured devices have attracted increasing interest in intensifying processes in recent years. However, these tools were only very recently made compatible for high pressure or high temperature processes.</p>
<p>In this study, the hypothesis that dense gas based microfluidic processes can be developed as an intensified and effective technique to micronize drug particles was tested using a poorly water-soluble pharmaceutical: copper indomethacin. Carbon dioxide (CO2) was used as an antisolvent to conduct precipitation of the drugs within a microfluidic system under supercritical conditions. The experiments were designed with the aim to optimize the process; the effects of operating parameters, namely temperature, pressure, flow rates of drug solution and CO2 were manipulated to reduce particle size and improve product yield. The Taguchi Method (TM) and Response Surface Methodology (RSM) were introduced to improve operating conditions and run statistical analysis. In the study of synthesis and micronisation of copper indomethacin, particles of diameter less than 5μm were produced at a yield of 80% and the purity of the product was upwards of 95%. The results from the particle size distribution analysis indicated that 90% of the product consisted of particles that were less than 8μm in diameter. The statistical analysis results also demonstrated that temperature and pressure were two significant factors affecting product yield while the flow rates of CO2 and solution had minor impact.</p>
<p>Working under the dense gas or supercritical conditions at micro-scale combines the advantage of size reduction provided by micro-devices to the unique properties of the dense gas or supercritical fluids, and brings benefits to particle engineering.</p>
<p>Key words: dense gas technology, microfluidic processes, micro-mixer, copper indomethacin, process intensification</td>
</tr>
</tbody>
</table>
<a name="FS03"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS03: Characterisation of the Interface Friction in the Cold Rolling</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chuhan Wu</p>
<p>Professor Liangchi Zhang</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>Cold strip rolling plays a vital role in the process of metal forming. However, a deep understanding of the interface contact of cold strip rolling has not yet been obtained due to the complex asperity-asperity contact between the roll and strip surfaces and the coupled solid-fluid interactions. Successful control and optimisation of rolling performance cannot be achieved without a comprehensive understanding of the mechanisms of surface roughness degradation and the role of lubricant. Thus, this research aims to develop an effective method of exploring such mechanisms and then to establish a feasible solution towards high performance rolling. </p>

<p>In the current study, a multi-scale analysis method has been established successfully to characterise the interface friction in cold rolling. An equivalent interface layer has been introduced between the roll and strip interfaces to integrate the microscopic asperity deformation with the asperity/lubricant interaction. Within the interface layer, a force equilibrium equation is used to achieve the asperity-lubrication, which accommodates different lubrication regimes in a unified approach. The surface asperities in contact usually undergo large plastic deformation in the cold rolling, leading to the significant variation of the strip surface topography. Because of the random distribution of asperity, a statistical analysis of the microscopic deformation of surface asperity has been performed within the interface layer to capture the surface topography evolution. The interface stress carried by the asperity and lubricant consequently brings out macroscopic plastic deformation of the strip. In order to address such a process, finite element analysis (FEA) has been carried out to obtain the bulk strip deformation. Thus, the difficulty of integrating the microscopic asperity/lubricant interaction with the macroscopic strip deformation is fully overcome. </p>

<p>With the aid of the established model, the normal pressure and shear stress in the rolling bite has been verified with experimental measurements. The hydrodynamic pressure has been given by the current model also. It has been found that the existence of the lubricant has significant effect on the rolling performance because of the effect of asperity-lubricant interaction. Normally, a higher lubricant viscosity can reduce the interface friction coefficient, which results in a decrease in the normal pressure. Meanwhile, the real contact area at the interface also decreases as a large fraction of the interface load will be shared by the pressurized lubricant. Moreover, it is expected that the method is also applicable to other contact sliding processes involving complex lubrication regimes and surface asperity deformation. </p>
</tr>
</tbody>
</table>
<a name="FS04"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS04: Particle Engineering and Formulation of Phenolic Compounds with Dense Gas Technology</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Firman Kurniawansyah</p>
<p>Prof. Neil R. Foster – Dr. Raffaela Mammucari</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Phenolic compounds have been reported to exhibit effective action against free radicals. Consequently, phenolic compounds possess potential in the treatment of various metabolic disorders such as cancer, diabetic, cardiovascular, osteoporosis and malaria. The application of the compounds, however, has been hindered by their intrinsic physicochemical properties. Most phenolic compounds have poor solubility and stability in the human gastro-intestinal pathway. As a result, phenolic compounds have low oral bioavailability.</p>

<p>Addressing the drawbacks such as those exhibited by phenolic compounds usually focuses on the strategy of processing and formulation. The processing can contribute to improvement of the physical properties of the compounds, while formulation can facilitate the modification of compound interactions with the targeted site of delivery.</p>

<p>In this study, particle engineering and formulation were applied to phenolic compounds for development as pulmonary and oral composite products. The method of dense gas anti-solvent technology was used throughout the study, using compressed CO2 as processing medium. In addition, process scale-up to evaluate feasibility of the method both technically and economically was conducted. The applicability of the dense gas method for improvement of the physicochemical property of phenolic compounds for pharmaceutical applications was confirmed. The processing scheme applied has been demonstrated to be feasible at larger scale with promising economics.  </p>
</tr>
</tbody>
</table>
<a name="FS05"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS05: Synthesis of Hollow Capsules Using Miniemulsion Templating Based on Dopamine and Derivatives</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yujian Zhai</p>
<p>Per B. Zetterlund, Anthony M. Granville</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Polymer capsules have seen applications across diverse fields such as biotechnology, chemistry and catalyst system. Especially in the field of nanomedicine, polymer capsules can be ideal carrier systems for therapeutic and diagnostic regimes. Recently inspired by the strong adhesive secreted by mussels, many researchers have focused on polydopamine which has good biocompatibility. Furthermore, polydopamine has shown versatile surface modification properties due to their latent hydroxyl and amino functionalities. In the present work, toluene miniemulsion droplets were used for the first time as templates to prepare polydopamine capsules. This one-step method can produce much smaller capsules when compared with other templating techniques. A traditional sonication method was employed to produce an aqueous miniemulsion of toluene, which was subsequently used as a template for the polymerization of dopamine in the aqueous phase and subsequent coating of the template droplets. The diameter of the capsules can be controlled within the approximate range of 35 &ndash; 230 nm by variations to the experimental methodology, such as changing the amount of toluene and dopamine concentration. The shell thickness increased with increasing dopamine concentration and decreasing amount of toluene.  This method has also been used to prepare 5,6-dihydroxy-1H-indazole(DHI) and dopamine copolymer capsules. The DHI has a similar catechol structure to dopamine. Previous work has confirmed that it is capable of self-polymerization through the same reaction of polydopamine formation. This experiment proved that this copolymer can be fabricated by using miniemulsion droplets as templates. Some effect factors were also investigated during the copolymer preparation. </p>
</tr>
</tbody>
</table>
<a name="FS06"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS06: Design and Synthesis Macromolecular Scaffolds for Carbon Monoxide Delivery</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Diep Nguyen</p>
<p>A/Prof. Cyrille Boyer</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Carbon monoxide (CO) has been well-known for its bad reputation as &ldquo;the silent killer&rdquo; stemming from its deleterious effect. When inhaled, it binds to the hemoglobin in the red blood cells and replaces oxygen from oxyhemoglobin which leads to a reduction in the amount of oxygen delivery in the tissue and subsequently death.[1] Despite its toxicity, CO is now increasingly accepted as a signalling molecule in mammalian organisms and is involved in regulating physiological and pathophysiological pathways. It plays significant roles in anti-inflammatory, anti-hypertensive, and anti-apoptotic pathways. Preclinical evidence in animal models has proven the beneficial effects of controlled CO gas administration.[2] The significant therapeutic effects of controlled CO inhalation have been proven in animal models of disease.[2] For example, the anti-inflammatory activity of CO has been studied in phase II clinical trials. Additionally, inhalation of CO has been applied in the field of organ transplantation and preservation. Yoshida et al.[3] reported that CO exhibited remarkable protection against transplant-induced renal ischemia/reperfusion (I/R) injury in pigs. <br />
However, the therapeutic application of CO gas administration has remained questionable resulting from its systemic toxicity at high concentration and difficulties in delivering and storing gaseous CO to the target tissue in a controlled manner. To overcome these problems, a wide range of CO-releasing molecules have been designed, and some have emerged as potential therapeutic agents. The commercially available tricarbonyldichlororuthenium(II) dimer (CORM-2) is one of the most common CO releasing molecules used in biological studies. This compound exerted significant biological properties including anti-inflammatory[4-7] and antimicrobial activities.[8-10] In vitro studies demonstrated the anti-inflammatory effect of this compound in the modulation of ischemia/reperfusion inflammatory response in small intestine,[6] inhibition of vascular proadhesive phenotype in the liver of septic mice[4] as well as alleviation of neuropathic pain.[11] However, the clinical use of this compound as a therapeutic agent poses problems associated with its short CO-releasing half-life and poor solubility in aqueous solution. These drawbacks can be addressed by using macromolecular scaffolds such as polymeric nanoparticles as CO carriers. <br />
Polymeric nanocarriers have attracted great attention in the field of controlled drug delivery because of their interesting properties. Polymers can be used to enhance the solubility of CORMs. For example, polymeric micelles encapsulating CORM-2 are soluble in water whereas CORM-2 has poor water solubility.[12] Additionally, polymeric systems have the capacity to load high amounts of CO donors, leading to a significant increase in CO concentration, and consequently improved clinical response.[12] Another advantage of polymeric systems is the ability to deliver CO to a target site of action by passive targeting or active targeting.[13-16]<br />
Inspired by both the excellent biological activities of CORM-2 and many the advantages of polymer materials, we aimed to construct novel multifunctional polymeric nanoparticles that possess features allowing the controlled and selective delivery of CO by conjugation or encapsulation of polymeric systems to CORM-2. This work opens up new strategies for the development of CO releasing scaffolds in biological applications.<br />
</p>
</tr>
</tbody>
</table>
<a name="DF01"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF01: Bioinformatics Prediction of Antibody-Antigen Interactions
</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Lingxiao Zhou	</p>
<p>Bruno Gaeta</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Antibodies are proteins secreted by B-lymphocytes that protect us by binding foreign or harmful antigens - foreign or harmful substances that induce an immune response. Antibody-antigen binding is part of the immune response. It is highly specific and an essential component of the body&rsquo;s defense mechanisms. The use of data mining to understand antibody-antigen binding is hampered by lack of data: the number of potential antigens is huge (millions) and difficult to sample, and due to the limitations of the current technology, less than 400 antibody-antigen complexes in the PDB (protein databank) have been studied. Currently, developing an efficient method to predict antibody-antigen interactions is a significant challenge.</p>

<p>To this end, we have developed a pharmacophore-assisted Iterative Closest Point (ICP) method and an N-dimensional Iterative Closest Point (ICP) method that are both able to cluster antibody-antigen binding samples to reflect their binding specificities. The implementation of the methods takes computational representations of antibody-antigen complexes as input and produces distance matrices that can be used for clustering. The methods integrate both alignment-dependent and alignment-independent concepts.</p>

<p>Both methods were successfully validated using antibody-antigen datasets. Given the wider applicability of these methods to protein interactions they were also used to investigate how diverse ligands can bind to different receptor sites with varying binding affinities. We applied our methods to three sets of experimental data, including 31 globulin-binding steroids, 4 groups of selected antibody-antigen complexes and 8525 protein-protein interaction (PPI) dimers. Results were visualised using dendrograms. For the steroid dataset, the resulting classification of ligands shows good correspondence with existing classifications. For the antigen-antibody datasets, the classification of antigens reflects both antigen type and binding antibody. The application in PPI dimers was also successfully grouping binding proteins according to their binding partners. Overall the method runs quickly and accurately for classifying the datasets based on their binding properties.</p>

<p>Antibodies are proteins secreted by B-lymphocytes that protect us by binding foreign or harmful antigens - foreign or harmful substances that induce an immune response. Antibody-antigen binding is part of the immune response. It is highly specific and an essential component of the body&rsquo;s defense mechanisms. The use of data mining to understand antibody-antigen binding is hampered by lack of data: the number of potential antigens is huge (millions) and difficult to sample, and due to the limitations of the current technology, less than 400 antibody-antigen complexes in the PDB (protein databank) have been studied. Currently, developing an efficient method to predict antibody-antigen interactions is a significant challenge.</p>

<p>To this end, we have developed a pharmacophore-assisted Iterative Closest Point (ICP) method and an N-dimensional Iterative Closest Point (ICP) method that are both able to cluster antibody-antigen binding samples to reflect their binding specificities. The implementation of the methods takes computational representations of antibody-antigen complexes as input and produces distance matrices that can be used for clustering. The methods integrate both alignment-dependent and alignment-independent concepts.</p>

<p>Both methods were successfully validated using antibody-antigen datasets. Given the wider applicability of these methods to protein interactions they were also used to investigate how diverse ligands can bind to different receptor sites with varying binding affinities. We applied our methods to three sets of experimental data, including 31 globulin-binding steroids, 4 groups of selected antibody-antigen complexes and 8525 protein-protein interaction (PPI) dimers. Results were visualised using dendrograms. For the steroid dataset, the resulting classification of ligands shows good correspondence with existing classifications. For the antigen-antibody datasets, the classification of antigens reflects both antigen type and binding antibody. The application in PPI dimers was also successfully grouping binding proteins according to their binding partners. Overall the method runs quickly and accurately for classifying the datasets based on their binding properties.</p>
</tr>
</tbody>
</table>
<a name="DF02"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF02: Elasticmark: An Elasticity Evaluation Framework for Cloud Platforms</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Sadeka Islam</p>
<p>Anna Liu</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Elasticity is the unique selling proposition of the cloud. It ensures rapid adjustment of resource capacity in response to varying workload intensity so that the application can meet its QoS objectives with minimal operational expenses. It is a crucial attribute that all commercial cloud providers frequently claim to possess in their offerings. However, existing literature and practitioners' blogs haven't yet provided any meaningful and systematic guidance to evaluate the elasticity of the cloud platform from the consumer's viewpoint. The lack of an elasticity evaluation framework makes it difficult for the consumer to diagnose and avoid elasticity issues, compare elasticity claims from different sources and choose the most elastic cloud platform well-suited to her application and workload profiles.</p>

<p>As such, this thesis proposes a novel benchmarking framework ElasticMark that reflects the elasticity of the cloud platform as a single figure of merit. It takes the consumer's perspective on running this benchmark by incorporating her application and workload profiles and then encapsulating the consumer's business objectives into the elasticity metric based on observations accessible via the cloud APIs. The core framework is comprised of a penalty based elasticity measurement model and a standard workload suite with time-varying workload patterns. The measurement model derives the elasticity metric based on the financial penalty rates resulting from over-provisioning (unutilized resources) and under-provisioning (inadequate resources) when the cloud-hosted application is exposed to a set of fluctuating workloads. ElasticMark also includes a workload model in order to assist the consumer generate representative prototypes of her application-specific fine-scale bursty workloads and get a reliable estimate about the cloud platform's elasticity behavior. Furthermore, this framework advocates a practical methodology to ensure repeatable and reproducible elasticity benchmarking results in presence of the environmental non-determinisms of the cloud. The framework has been validated against a widely-used commercial cloud service to make sure that the elasticity metric is a good reflection of the low-level adaptability characteristics and observed phenomena. It has also been proven effective in comparing and contrasting the elasticity of multiple cloud platforms as well as pinpointing anomalies in their adaptive behaviors.</p>
</tr>
</tbody>
</table>
<a name="DF03"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF03: Privacy in Social Networks</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Vidyalakshmi Vidyalakshmi</p>
<p>Dr. Raymond Wong</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Participation on social networks has surpassed all other online activities to be one of the top internet activities. With the information disclosure on social networks, comes the risk of this information falling into wrong hands leading to privacy problems. Information posted on social networks is seen by the people in a user&rsquo;s network of friends. This research concentrates on known problems of oversharing of information, solutions to ego user's (Ego user is the social network user in question) information sharing needs in a single social network and across multiple social networks and the risk to ego user&rsquo;s privacy through his friends.</p>

<p> To control visibility of posts and profile information, most social networks have the concept of grouping of friends into Lists or Circles, by which it will be easier to send the right information to the intended friends. But, the groups have to be updated often with addition/deletion of friends. Also, a friend may be part of many groups (eg., University Science Club, Close friends), leading to complicated sharing scenarios. To assist user, we propose a solution that utilizes both context of friendship (University friend, Colleague) and Tie-strength (Tie-strength, is a measure of closeness i.e., friend can be an acquaintance or a close friend) in arriving at on-demand grouping of friends, which can be used for sharing information on social networks. </p>

<p> An ego user may be very privacy oriented or may be interested to share information with all his friends. With the current privacy settings on social networks, ego user&rsquo;s attitude towards privacy and communication cannot be accounted for although it has a major influence on who sees what information. We propose a framework for calculating privacy scores of friends so as to assist ego user in taking an informed decision while sharing information with them. Privacy scoring considers both privacy attitude and communication attitude of the ego user. According to a recent finding more than half of the online adults use multiple social networking sites. In such a scenario, users need to create and manage groups on multiple social networks. Privacy scoring framework caters to this need of the ego user to use the same scoring across multiple social networks.</p>

<p> Even though an ego user is very privacy oriented and shares attributes to his private network, his friends may share their attributes publicly. We propose a model utilizing the attributes (College, Employment, Age, Location) of friends in inferring ego user's attributes. The model can infer attributes even when only 20% of ego user&rsquo;s friends have shared their attributes. Our work on directed social networks (Twitter, Google+ where a user can follow anyone), has also shown that follower network (a user follows you) or following network (you follow a user) infers the ego users' attribute better than the friend network (a user follows you and you follow him back). A scenario where anyone can follow you and your attributes (eg., location) can be inferred using their attributes is a real threat to privacy on social networks.<br />
</p>
</tr>
</tbody>
</table>
<a name="DF04"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF04: Improving Utilization Efficiency of Training Data in Action Recognition and Object Detection</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yingying Liu</p>
<p> Prof. Arcot Sowmya, Dr. Yang Wang, A.Prof. Wei Wang</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Action recognition and object detection are fundamental computer vision tasks. A common goal for both is to increase the utilization efficiency of annotated training data,&#160;which are used to build models that&#160;assign labels to test data. This goal can be addressed by increasing the capability of the feature description methods and the efficiency of the learning methods. </p>

<p>Increasing the capability of the feature description method is the most direct way of improving the utilization efficiency of training data. In this thesis, temporal pyramid histograms are proposed to address the problem of missing temporal information in the classical bag of features description method used in action recognition. Experiments have shown that the proposed description method outperforms the classical bag of features method in action recognition. </p>

<p>Active learning and weakly supervised learning have been utilized to increase the learning efficiency, by either reducing the amount of annotated training data or reducing their complexity.</p>

<p>Active learning methods select the most informative training data in each interaction, and therefore require less training data to attain comparable performance to the passive learning methods. In this thesis, an active learning method for object detection that exploits the distribution of training data is presented. Experiments have shown that the proposed method outperforms a passive learning method and a state-of-art active learning method. </p>

<p>Weakly supervised learning methods can utilize training data with weak annotations. In this thesis, a weakly supervised object detection method is proposed to utilize training instances with probabilistic labels. Base detectors are used to create training instances from training data with weak annotations. Then the training instances are assigned estimated probabilistic labels. A Generalized Hough Transform based object detector is extended to utilize the training instances with probabilistic labels. The proposed method has been shown to outperform both a comparison method that assigns strong labels to training instances and a state-of-art weakly supervised object detection method. Moreover, the proposed method also attains comparable performance to supervised learning methods.<br />
</p>
</tr>
</tbody>
</table>
<a name="DF05"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF05: A New Approach to the Unscented Estimation and its Application to Real-time On-Board Navigation of Nano-Satellites Using Multi-GNSS Receiver</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Sanat Biswas</p>
<p>Andrew Dempster</p>
<p>Electrical & Telecommunications Engineering</td>
</tr>
</tbody>
</table>
<p>
 Nano-satellites are well recognized for reducing space mission cost and allowing measurements at multiple data points at a time. However, due to the size and weight constraint, nano-satellites cannot supply large amount of power to their sub-systems and their computation capability is substantially less than larger satellites. Navigation is an integral part of every satellite mission and many sophisticated satellite applications demand a robust and highly accurate navigation solution. Use of Global Navigation Satellite System (GNSS) for real-time on-board navigation of Low Earth Orbit (LEO) satellites is an intriguing and cost effective technique. The Extended Kalman Filter (EKF) is a widely used estimation algorithm, which combines the knowledge of the dynamics of the user vehicle motion with the GNSS measurements more accurate position solution. However, the EKF approximates the error covariance by linearization and thus it has limited accuracy. A more rigorous technique is the Unscented Kalman Filter (UKF) which calculates the propagated mean and error covariance of the state vector more accurately than the EKF. Unscented Kalman Filtering for continuous time systems involves propagation of multiple sigma points at each time step, which incurs a substantial amount of processing time. Due to the limited computation capability, implementation the UKF for navigation purpose is difficult in nano-satellites. <br />
This thesis proposes a new approach to the state vector propagation for the UKF prediction stage. In this method, only the a posteriori state vector of the previous time step is propagated to the current time step and the other sigma points at the current step are calculated approximately from the information of the previous step using the first order Taylor series approximation. The UKF with this strategy is referred as the Single Propagation Unscented Kalman Filter (SPUKF). The error in the a priori state computation using the new method is in the second order terms of the Taylor series expansion and leads to lower accuracy. To reduce the error in the a priori state prediction, a method inspired by Richardson Extrapolation is adopted. A UKF with the aforementioned state propagation strategy and the multidimensional extrapolation method is referred to as the Extrapolated Single Propagation Unscented Kalman Filter (ESPUKF).<br />
The reduction in computation time for both the new filters were analysed theoretically and to compare the performances of the respective filters with the original UKF, a benchmark non-linear estimation problem is used. The reduction in computation time for the SPUKF and the ESPUKF are 90.5% and 85.5% respectively. These two filters were then utilized for a LEO satellite position estimation using multi-GNSS receiver and 92.6% and 86.8% reduction in computation time with respect to the UKF was observed. The significant reduction in processing time shows a plausibility of implementation of a UKF in real-time navigation of nano-satellites. These fast, robust and accurate techniques will aid the nano-satellite navigation system in advanced missions in low, medium and higher earth orbits.</p>
</tr>
</tbody>
</table>
<a name="DF06"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF06: Analysing, Classification and Ranking of X-Ray Crystallography Images</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Thamali Lekamge</p>
<p>Arcot Sowmya</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>
X-ray crystallography is one of the main techniques used to analyse and understand the three dimensional structure of a protein at atomic resolution. These crystallisation experiments usually require hundreds and thousands of individual experiments. The experiments are also time dependent and require monitoring. Automatic imagers are used to record the steps of these crystallisation experiments and they generate millions of images. Crystallographers are expected to analyse the resulting images as the image generators play no part in interpreting the images. Between 10,000 and 20,000 images are generated daily in the medium-throughput crystallization laboratory within CSIRO. According to the estimates of crystallography experts, around 50 million images are generated via crystallisation experiments annually. As the amount of data generated is huge and there is not enough resources to analyse every single image by the naked eye, many experimental images are never inspected. <br />
Additionally, although numerous experiments are carried only very few produce crystals. Some experiments can lead into different non-crystalline paths such as producing precipitation, skin or other results. Even though crystal production is the main target of the crystallography experiments, experiments can also provide important information such as conditions conducive to crystallisation and stability of a protein. Therefore, identifying the end product in each image is important.<br />
The experiments are time dependent with the time varying from a few hours to a few weeks. The automatic image capture process records different stages of the experiments, but the time intervals between captured frames are not identical, with the time intervals (frame rate) varying from hours to weeks. The number of frames in the experiments is also varying between 8 and 16 in each.<br />
Both the original images and the difference images between images in an image sequence of a single experiment were considered. To find the best set of difference images the differences between different combinations of images in a sequence were tried, such as (I1-In), (I2-In), and (In-In-1), where the number of images Ii in a sequence is n. Statistical features of each image were computed and classification performed using support vector machines, decision trees and random forests. From the results, the combination of original images and the difference images between the 1st image in a sequence and the rest (I1-In) gives the best results. <br />
Clearly, there are two separate data sets for each experiment: namely features computed from the original image and the best difference image. Therefore Multi-View Learning was used with co-training as the feedback mechanism, for classification of these images. We obtained a classification accuracy of 97.8%. Results indicated that accuracy increases with each iteration of Multi-View Learning.<br />
As the crystallography experiments are time dependent, sequence learning / stream learning techniques are then used to analyse and classify the time sequence of an experiment rather than just single frames. The goal is to learn the patterns that lead to production of crystals as the end product and be able to predict the final outcome of an experiment after studying the first few images of a sequence. </p>

<p>
</p>
</tr>
</tbody>
</table>
<a name="EI01"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI01: A Surface-enabled Aqueous Metal-ion Capacitor with Oxidised Carbon Nanotubes and Metallic Zinc Electrodes</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yuheng Tian</p>
<p>Da-Wei Wang</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>An aqueous metal ion capacitor comprising of a zinc anode, an oxidised carbon nanotubes (oCNTs) cathode and a zinc sulphate electrolyte is reported. Since the shuttling cation is Zn2+, this typical metal ion capacitor is named as zinc-ion capacitor (ZIC). The ZIC integrates the divalent zinc stripping/plating chemistry with the surface-enabled pseudocapacitive cation adsorption/desorption on oCNTs. The surface chemistry and crystallographic structure of oCNTs were extensively characterized by combining X-ray photoelectron spectroscopy, Fourier-transformed infrared spectroscopy, Raman spectroscopy and X-ray powder diffraction. The function of the surface oxygen groups in surface cation storage was elucidated by a series of electrochemical measurement and the surface-enabled ZIC showed better performance than the ZIC with an un-oxidized CNT cathode.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI02"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI02: Plasmonic Light Trapping for Solar Cells</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Claire Disney</p>
<p>Prof. Martin Green & Dr. Supriya Pillai</p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>In order to increase the competitiveness of photovoltaics, we must reduce their cost without compromising efficiency. Light trapping structures can be used to increase absorption into a solar cell to achieve the same performance with thinner cells. Plasmonic light trapping relies on the differing nanoscale properties of materials to preferentially scatter light into the solar cell while minimising absorption in the light trapping structure1. The design of these structures can be quite sensitive to certain design parameters. Thus optimisation is necessary to select the best design to implement, as well as to assess the applicability of different types of light trapping structures. <br />
We performed 3D finite-difference-time-domain (FDTD) simulations using Lumerical2 software to analyse the performance of a variety of different plasmonic light trapping structures, with a particular focus on rear structures for thin silicon solar cells. Investigations also aimed at understanding the underlying mechanisms of enhancement and their dependency on design choices. Key structures investigated include hexagonal arrays of nano-holes or embedded dielectric nano-spheres in a rear metal layer. These types of structures are of particular interest due to their relative simplicity to fabricate, while maintaining tight control over design parameters3. Simulations were completed varying a number of design parameters, and the idealised photocurrent (Jsc), and scattering profile for each case was calculated and compared to existing light trapping structures.</p>

<p>By comparing the Jsc for each parametric variation, sensitivity of device performance to multiple parameters has been investigated. Variables considered include the hole or sphere size and array period, metal and solar cell type and thickness, as well as the impact of changes in the spacer layer and incoming angle of light. Further simulations investigated the mechanisms of enhancement leading to the optimal designs. We have also considered the impact of real world conditions such as semi-random arrays of different sized nano-features, and the impact of line-of-sight issues in metal application.</p>

<p>Through our simulations, we have been able to develop a number of initial design principles for the investigated structures and assessed their suitability for implementation in solar applications. We found significant performance enhancement for a variety of different designs with varying dimensions and materials. Our maximum increase in Jsc for a 2 &#956;m thick Si cell was 4.02 mA/cm2 or 24.4% relative to the case of a rear silver mirror. This suggests that these may be promising structures for use with solar cells. Additionally, it was found that absorption losses in the metal associated with resonant modes in the nanostructured metals, which had previously been thought to limit the applicability of such structures, are also associated with a strong increase in large angle scattering. When designed correctly, the benefits of the scattering significantly outweigh the parasitic absorption, further supporting the use of these types of light trapping structures.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI03"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI03: Concentrated Wound Interior Permanent Magnet Machines – Analysis, Modelling and Control</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Mohammad Farshadnia</p>
<p>Prof. John Fletcher, Dr. Rukmi Dutta</p>
<p>Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>Exciting features of permanent magnet (PM) synchronous machines make them apt for many consumer and industrial applications. Performance of PM machines has been improved over time by introducing interior permanent magnet (IPM) rotor topologies that are inherently salient in structure. Characteristics of PM machines can be further enhanced if a concentrated-wound (CW) stator is utilized. CW IPM machines are more cost-effective and are often preferred in compact and high power density drive systems that are commonly used in electric vehicles, traction drives, and the wind energy industry.<br />
Accurate modelling of machine dynamics is essential to analysis and precise controller design of CW IPM machines. Conventionally, machine parameters, such as permanent magnet flux linkage, self- and mutual-inductances, and the back-EMF are considered sinusoidal, leading to the well-known dq model of the machine. However, by introducing CW stators and IPM rotor structures, the assumption of sinusoidal machine parameters is no longer valid. This is due to the spatial harmonics in the magneto-motive force (MMF) of the stator, the non-sinusoidal rotor PM flux linkage, and the salient structure of the IPM rotor. It is shown in the literature that the use of the conventional dq model results in deficiencies in the control of the CW IPM machine.<br />
A comprehensive mathematical model of the CW IPM machine is proposed here that takes into account the key physical aspects of a CW IPM machine. The performed analysis challenges the exiting theories applied to CW IPM machines, as they are commonly borrowed from the theories surrounding conventional PM machines. The performed analysis comprises three stages:<br />
First, the CW stator topology is analysed and general mathematical models are proposed for calculation of the MMF produced by different CW configurations.<br />
Next, the IPM rotor is analysed and a geometry-based model is proposed that describes the local saturation in the rotor iron and the flow of flux inside and outside the rotor. This model is used to obtain the PM flux density in the air-gap as well as the equivalent air-gap function of the machine. Commonly, IPM rotors are treated similar to inset-PM rotors and the same analysis is used to describe their behaviour. By using the proposed method, a more accurate model of the rotor is obtained that is in contradiction with the conventional models used for IPM rotors.<br />
Lastly, the derived mathematical formulas for the CW stator and the IPM rotor are combined to find the detailed inductance, detailed back-EMF, and detailed torque equations for CW IPM machines. All the obtained results are validated through finite element analysis and experiments on a prototype CW IPM machine.<br />
The derived detailed geometry-based formulas that describe the machine characteristics are simplified and transformed to the dq reference frame to obtain an extended dq model that describes the CW IPM machine dynamics. The proposed dq model is evaluated through measurements on the prototype CW IPM machine and is used in control schemes for improved control of the machine. </p>

<p>
</p>
</tr>
</tbody>
</table>

<a name="EI04"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI04: Microblog Topic Emerging Outbreak Monitoring</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Victor W Chu</p>
<p>Dr Raymond K. Wong</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Studies of the causes and effects of diseases and their propagation patterns have always been the focuses of epidemiology. However, epidemiology can be literally interpreted as &ldquo;the study of what is upon the people&rdquo; according to the meaning of its root words. In relation to today's social activities, social media is booming at an unprecedented scale providing various platforms for people to connect with each other. For example, Twitter is a platform for short text communications on social media.</p>

<p>A recent study of collective attention on Twitter showed that an epidemic spreading of hashtags is predominantly driven by external factors. Hence, it is difficult to model such a system. To address this problem, an epidemic time-series model is proposed that considers both endogenous and exogenous drivers to provide a better model. The measure of contagiousness is also standardised by estimated effective-reproduction-number. It is obtained incrementally by Bayesian parameter estimation on our proposed model. </p>

<p>Hashtag analysis is known to suffer from a problem of low hashtag rate. A different approach is taken by investigating into the propagation of hastag-induced topics on Twitter by topic modelling. Unlabelled but related tweets are identified via topic inference making the information from a whole dataset accessible. Therefore, one can accurately profile and categorise emerging topics, and generate alerts on potential outbreaks using our proposed contagiousness measure. </p>

<p>In summary, my contributions to microblog epidemiology are: <br />
&bull; Small available sample problem is overcome by topic models.<br />
&bull; The propagation patterns of hashtag-induced topics is shown to be emerging-epidemic like.<br />
&bull; An emerging epidemic model is enhanced to cater for both endogenous and exogenous factors.<br />
&bull; A Bayesian parameter estimation method is proposed to obtain a standardised contagiousness measure.<br />
&bull; Lastly, a microblog emerging outbreak monitoring framework is proposed.</p>

<p>The effectiveness of the proposed model in predicting emerging outbreaks is shown by extensive experiments on Twitter datasets with reduced distortions by external factors.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI05"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI05: Applications of a Numerical Weather Prediction Model on Building Energy Management and Characterisation</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Dimitris Lazos</p>
<p>Merlinde Kay, Alistair Sproul </p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>Energy management for a building involves components that are weather dependent, such as the heating and cooling loads, the on-site energy generation from photovoltaic panels and the implementation of demand response measures. Research suggests that optimisation of such energy systems may result in significant financial savings of up to 50% compared to a deterministic approach that does not consider weather inputs. Hence, information about the prevailing and future weather conditions as well as the building&rsquo;s responses to specific weather patterns is valuable for optimisation algorithms and decision making related to building design and operations. In most cases, information about the weather is obtained from external sources, such as weather stations or meteorology bureaus. However, the ability to generate on-site weather predictions and analyse climate patterns is desirable in order to accurately capture local effects and tailor inputs for custom energy management applications.<br />
The use of a numerical model is proposed, which will enable both the generation of highly accurate localised weather predictions and a range of characterisation applications for the local climatic conditions and the building thermal responses. The numerical model is based on the Atmospheric Pollution Model (TAPM) software, which is able to downscale synoptic scale weather data to high spatial resolution domains at the location of the building. There are four applications using the outputs from the numerical model.<br />
Firstly, weather predictions for short term horizons (up to 24 hours ahead) are generated. The novel model proposed in the thesis is based on post-processing the numerical model predictions to increase overall accuracy. The hybridisation of numerical and statistical predictions allows taking advantage of the strengths of both models: numerical predictions generally are able to model abrupt changes more accurately, while statistical predictions perform better in forecasting regular weather patterns. The hybridisation allowed for up to 40% increase in accuracy of weather variable forecasts compared to the individual models.<br />
Secondly, by altering the parameters of the numerical model simulations, it is possible to obtain an ensemble of weather forecasts. This is a novel application in the context of building energy management and allows taking account of the inherently chaotic nature of weather variable evolution. More importantly, the proposed ensemble model&rsquo;s computational electric power and running time requirements are very low.<br />
Thirdly, the numerical model is used to evaluate the potential for precooling at the location of the building. Precooling is a process that utilises low overnight temperatures during summer nights to reduce energy costs and shave load peaks the day after. A novel algorithm is developed for calculating the potential and value of precooling at any location&mdash;it was found that energy savings of up to 10% may be realised in various inland locations in Sydney at no significant costs.<br />
Finally, numerical model inputs enable the characterisation of the thermal behaviour of the building. The characterisation is based on developing a thermal network model of case studies of UNSW buildings and estimating the parameters through fitting the numerical model inputs to the energy consumption and onsite generation output data.<br />
</p>
</tr>
</tbody>
</table>
<a name="FS07"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS07: Tackling the Efficiency Limit of Photovoltaic Cells: The Power of Colloidal Nanoparticles</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Zhilong Zhang</p>
<p>Shujuan Huang, Gavin Conibeer
</p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>Photovoltaics &ndash; the technology that converts sunlight into electricity directly, is one of the key candidates for clean, renewable and sustainable energy in the future. The global market in photovoltaics has grown rapidly in the past decade, which was driven by the development of technology and hence cost reduction, and now the price of electricity generated by photovoltaics is comparable to that by fossil fuels. The improvement in power conversion efficiency of photovoltaics is important to further price reduction, however nowadays the efficiency of commercialized photovoltaic cells are approaching to the theoretical efficiency limit of 31%. Further improvement in efficiency has become more and more challenging. </p>

<p>One main reason which accounts for 50% loss in the power conversion efficiency limit of photovoltaics is the thermalization. In a conventional phototolvatic cell, photons with energy equal to or higher than the band gap of the cell material can be absorbed and converted into electricity. However, the excess energy above the band gap is usually lost into thermal energy and dissipated as heat. We aim to tackle this energy loss and hence it is possible to break the power conversion efficiency limit of photovoltaics. </p>

<p>Colloidal nanoparticles &ndash; extremely tiny particles with size down to a millionth of a metre (i.e. 1 nanometre), exhibit many interesting and useful properties. They have been shown theoretically and experimentally that are promising in optoelectronics and of course, photovoltaics. For example, crystalline nanoparticles with size smaller than the Bohr radius of the material exhibit quantum confinement effect and hence are called quantum dots (QDs). QDs have many promising properties related photovoltaics. Two of them, phonon bottleneck effect and multiple exciton generation (MEG) effect are discussed. Phonon bottleneck in QDs enables them to maintain the population of hot carriers &ndash; those electron-hole pairs generated by high energy photons. The longer-lived hot carriers in QDs make them very promising materials for hot carrier solar cell, which extracts carriers with high energy before they thermalize, as a result the open-circuit voltage of the cell is improved. MEG effect in QDs however tend to utilize the energy of hot carriers to generate more lower-energy carriers. As a result the excess energy is converted into higher short-circuit current of the photovoltaic cell.</p>

<p>We exploit the synthesis, device fabrication, background physics, as well as some new concepts of applying colloidal nanoparticles in photovoltaics. The nanoparticles studied, such as lead chalcogenides (PbS, PbSe and PbTe) QDs are discussed as they are promising for solar cells with hot carrier and MEG. Some other supporting materials for the devices, such as zinc oxide, titanium dioxide and silicon dioxide nanoparticles are also discussed. Results on the prototype of cells made these colloidal nanoparticles will be presented. Other works on light trapping and management for photovoltaics using nanoparticles are also included in the presentation. The future development of nanoparticles for photovoltaics will also be discussed</p>
</tr>
</tbody>
</table>
<a name="FS08"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS08: Micro and Macro Structure Optimization of CZTS Based Thin Film Solar Cell</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Fangzhou Zhou</p>
<p>Xiaojing Hao</p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>The newly emerged kesterite Cu2ZnSn(S,Se)4 (CZTS(e)), derived from CuInxGa(1-x)Se2(CIGS), has been considered as one of the most promising absorber materials for thin-film solar cells, due to its merits like abundant constituent materials, non-toxic composition, and good photo-electrical properties (high absorption coefficient of 10-4 cm-1 and an adjustable bandgap). The current trends of the development of CZTS based solar cells are discovering a low-cost fabrication method and optimising the device structure.<br />
To find a low cost method, a solution-based deposition method is investigated. The precursor solution was then spin-coated onto Mo-coated soda lime glass (SLG) substrates. Precursors are converted to CZTS by using a rapid thermal processing system (RTP), in the presence of elemental sulphur pellets. The effects of composition of the precursor, sulfurization ramping-up rate, background pressure and pre-annealing conditions on the CZTS thin-films are investigated. <br />
In order to optimise the device structure, we evaporate an ultra-thin carbon layer on Mo-coated SLG prior to the deposition of the CZTS precursor. This intermediate layer is tested on both non-vacuum (sol-gel) and vacuum (sputtering) methods.<br />
The structure, morphology, and composition of CZTS thin-film is measured by X-ray diffraction, Raman spectroscopy, scanning electron microscopy, transmission electron microscopy, and energy dispersive spectroscopy. The various sulfurization parameters result in grain size and composition ratio changes, which may be explained in terms of secondary phases. Regarding the structure optimization, the insertion of a carbon intermediate layer does not cause side effects on film quality.<br />
The optic-electronic properties of CZTS based solar cells are measured using current density-voltage characterization and external quantum efficiency. The various sulfurization parameters have great influence on device performance, and one best combination of parameters is selected to yield a high efficiency device. The carbon intermediate layer boosts the short circuit current and device conversion efficiency for both non-vacuum and vacuum methods. Furthermore, introduction of this layer does not lead to any deterioration of either open circuit voltage or fill factor.</p>
</tr>
</tbody>
</table>
<a name="FS09"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS09: LiH, LiAlH4, AlH3 at Nanoscale: Synthesis, Stabilisation and Hydrogen Storage Properties</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Lei Wang</p>
<p>Kondo-Francois Aguey-Zinsou</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>With a growing world population and an increasing standard of living, it is not a question of if but when the world will run out of fossil fuels. Existing and upcoming battery technology can only solve part of the problem, due to its relatively low energy density, it is not able to replace fossil fuel in long range large energy storage applications, such as trucks, jumbo jets, and cargo ships.</p>

<p>Hydrogen is the ultimate energy carrier, as it contains the most energy per unit, and only generates water at the point of use to produce electricity.&#160; A major technical and engineering challenge remaining unsolved is a practical system to store hydrogen, which is normally a volatile and flammable gas.</p>

<p>Current technologies of hydrogen storage are not feasible to enable hydrogen to compete with conventional energy carriers such as oil and gas on affordability, convenience and safety. Solid state metal hydride is a well-recognised promising option to store hydrogen with advantages of safety and compactness. Still it has unsolved problems of poor reusability and harsh operating conditions. Nevertheless, it has now been discovered that the hydrogen storage performance of these hydrides can be remarkably improved by making them into nano size.&#160; </p>

<p>This research is about developing methods to synthesis light metal hydrides (LiH, LiAlH4, AlH3) in nanoscale, with the aim to obtain a stable and controllable hydrogen storage nanomaterial with a high system hydrogen capacity. </p>

<p>Normally, all these three materials at their bulk size are not reversible to store hydrogen under mild conditions. By using various nanosizing technologies, we have synthesised them into nanoscale materials. Brand new hydrogen storage performances have been discovered. All three nanoscale materials are found to reversibly store hydrogen with improved reaction kinetics, but with reduced hydrogen densities. This finding is likely due to nanosizing effects imparting shorter hydrogen diffusion distance hence easier hydrogen-metal reaction.</p>

<p>I believe these nanomaterials will equip and inspire us to build a sustainable energy future.<br />
</p>
</tr>
</tbody>
</table>
<a name="FS10"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS10: Photo-Thermal-Catalysis in Redox Reactions</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Tze Hao Tan</p>
<p>Rose, Amal; Jason, Scott; Yun Hau, Ng; Rob, Taylor</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Solar energy remains an untapped energy source which is limited to photovoltaic and solar thermal heating applications. While photo-catalysis demonstrates potential to supplant conventional thermal-catalytic reactions as a sustainable chemical process, current technologies are still unfeasible for up scaling due to low throughput of chemical products. Thus, this research focuses on an alternative photochemical reaction process, photo-thermal-catalysis (PTC). As the name suggests, PTC utilises both light and high temperature in the catalytic reaction, creating a synergistic effect between photo- and thermal-catalysis. Not to be confused with photothermal effects, PTC is based on light induced surface charge heterogeneity, whereby photo-generated charge carriers are able to constructively participate in thermal-catalytic reactions. The scope of this research is broken down into two major sections, namely the application of PTC in oxidation and reduction reactions. </p>

<p>For the oxidation reaction, we looked at applying of PTC in volatile organic compound (VOC) degradation using a plasmonic Au/TiO2 photocatalyst. The study elucidates the contributions of plasmonic and bandgap excitation effects in PTC by probing the impact of different light excitation wavelengths on the thermal-catalytic activity of Au/TiO2. The oxidation of ethanol, a known VOC, was selected as it is a simple reaction which will assist with identifying the mechanisms. PTC results and detailed probing of post-reaction surface carbon species on Au/TiO2 indicated that photo-enhancement under UV illumination was due to congruent roles of the photo- and thermal- catalysis, while photo-enhancement under visible light illumination was due to plasmonic-mediated electron charge transfer from the Au deposits to the TiO2 support.</p>

<p>For the reduction reaction, we tackled the Holy Grail of renewable energy, conversion of carbon dioxide (CO2) to a valuable fuel. Conversion of CO2 is known to be a challenging reaction due to its high activation energy, which results in an extremely low output for photo-catalytic reactions, usually in the range of few &micro;mol/gcat.h. Using heat as a thermodynamic driving force, we were capable of producing up to 30 mmol CH4/gcat.h at 400 oC using visible light illuminated PTC, with LaNi5/TiO2 as the active catalyst. This observation corresponded to an over 1000 times increase in conversion rate compared to photo-catalysis alone and PTC results showed that visible light illumination was accountable for 50% of the CO2 converted. Ongoing research is being conducted to understand the role of photons in the reaction pathway of CO2 conversion to CH4.<br />
</p>
</tr>
</tbody>
</table>
<a name="FS11"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS11: TiO2-based Catalysts for Photocatalytic Hydrogen Generation: Synthesis, Application and Mechanism Studies</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Fenglong Wang</p>
<p>Rose Amal</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Solar to chemical energy conversion by photocatalysis is regarded as a promising way to alleviate our dependency on the non-renewable fossil fuels. In this project, highly efficient TiO2-based catalysts were synthesized for photocatalytic hydrogen production. The mechanisms of the photocatalytic hydrogen generation process were thoroughly studied by GC-IRMS, in-situ NMR and the enhanced charge separation process was investigated by optical pump THz probe technique. </p>

<p>First, gold embedded boron-doped TiO2 photocatalysts were synthesized by a sol-gel hydrothermal method. The TEM images display that the gold nanoparticles were embedded into the B-TiO2 framework. Hydrogen evolution under light irradiation showed that doping of boron into TiO2 enhanced the photocatalytic activity compared to the bare TiO2. A further remarkable improvement of the activity was observed over the gold embedded boron-doped TiO2 framework. The specific synergy between the doped B-species and embedded gold nanoparticles would significantly contribute to the enhancement of photocatalytic activity. For the first time, isotopic tracer studies indicated that the produced hydrogen mainly originated from water rather than methanol. Combined with the HPLC analysis of the liquid intermediates/products, a possible reaction pathway for the photocatalytic hydrogen production from water/methanol mixture has been proposed.[1] </p>

<p>With the aim to further improve the hydrogen generation activity and get a deeper understanding of the photocatalytic hydrogen evolution process, 2-3 nm fine metal nanoparticles (Au, Pt and alloyed Au-Pt) with a narrow size distribution were deposited on active TiO2. Compared to the bare TiO2, a remarkable enhancement of up to 10-fold for photocatalytic hydrogen evolution was achieved on the alloyed nanocomposites. By using core level and valence band XPS analysis, two electronic properties are shown to contribute to the promoted photocatalytic activity: stronger metal-support interaction between the alloyed structures and TiO2 and higher electron population on the Au-Pt/TiO2 photocatalysts in comparison with the bare TiO2. Moreover, an improved charge separation over TiO2 using Au-Pt nanoparticles was clearly evidenced by the significant increase of photocurrent responses. For the first time, in situ 13C and 1H NMR spectroscopy was applied to monitor the gas-liquid-solid photocatalytic reactions. Via a two-electron oxidation pathway, the surface-adsorbed methanol was firstly oxidized to formaldehyde, followed by spontaneous hydrolysis and methanolysis to methanediol and methoxymethanol.[2] </p>

<p>In order to extend the absorption edge of the catalysts into the visible light region, a facile refluxing wet-chemical approach is employed for the fabrication of In2S3/Pt-TiO2 heterogeneous catalysts for hydrogen generation under visible light irradiation. It is found that the composite catalyst shows higher hydrogen production than the in situ deposited Pt-In2S3. UV-Visible diffuse reflectance and valence band X-ray photoelectron spectra are recorded to elucidate the band alignment between In2S3 and TiO2. The improved charge separation is apparent from photoelectrochemical transient photo-current measurements and optical pump-terahertz probe spectroscopic studies. The migration of the photo-induced electrons from the conduction band of In2S3 to TiO2 and subsequently into the Pt nanoparticles is found to occur within 5 picoseconds. Based on the experimental evidence, a charge separation process is proposed accounting for the enhanced activity on the In2S3/Pt-TiO2 composite catalysts.[3] </p>

<p>References <br />
[1] Fenglong Wang, Yijiao Jiang, Anil Gautam, Yarong Li, Rose Amal, ACS Catalysis, 2014, 4, 1451-1457. <br />
[2] Fenglong Wang, Yijiao Jiang, Douglas J. Lawes, Graham E. Ball, Cuifeng Zhou, Zongwen Liu, Rose Amal, ACS Catalysis, 2015, 5, 3924-3931. <br />
[3] Fenglong Wang, Zuanming Jin, Yijiao Jiang, Ellen H. G. Backus, Shi Nee Lou, Mischa Bonn, Dmitry Turchinovich, Rose Amal, The Journal of Physical Chemistry Letters, in preparation. <br />
</p>
</tr>
</tbody>
</table>
<a name="DF07"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF07: Digitally-assisted Sigma Delta Modulation Circuits</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Astria Irfansyah</p>
<p>Prof. Torsten Lehmann, Dr. Tara Julia Hamilton</p>
<p>Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>Complementary metal-oxide semiconductor (CMOS) integrated circuit technological scaling has made it feasible to implement integrated circuits in the digital domain to take advantage of the higher speed and better noise handling offered. On the other hand, maintaining analog circuit performance such as accuracy, gain, and signal-to-noise ratio in these newer CMOS processes will become a challenge since these will generally degrade with scaling. Digitally-assisted analogue circuit techniques have been viewed as a potential method to retain analogue circuit performance in these newer processes through digital tuning or correction. </p>

<p>This thesis investigates possible schemes in sigma-delta modulation circuits where such digitally-assisted analog circuits seem appropriate. For compatibility with current and future CMOS processes, we investigate the use of simple structures in data conversion circuits. In our study of the continuous-time sigma-delta modulator (CT-SDM), we focus entirely on the use of an inverter-based transconductor, referred to as Nauta's transconductor, with digital configurability and tuning. In our study of the sigma-delta digital-to-analog converter (DAC), we investigate a new simple structure made of only resistors and switches implemented as inverters, as the final stage DAC of a complete sigma-delta DAC with multi-stage noise-shaping and dynamic element matching. Test chips in CMOS 180 nm process have been implemented for these purposes, and our thesis includes experimental measurement results to verify our analysis and design strategy.</p>

<p>In our DAC study, we first formulate a design trade-off and optimisation method for the basic cascode structure as well as an improved output impedance current-steering DAC, which highlights the impracticality of the former method though theoretically it has been shown that it can still attain the required output impedance. A new circuit methodology consisting of only resistors and switches was proposed. It tries to overcome the code-dependent finite output impedance problem of the conventional current-steering DAC structure. These DACs were then integrated into a complete 16-bit multi-stage noise-shaping sigma-delta DAC, with dynamic element matching, and implemented in a CMOS 180 nm process. Chip measurement has been obtained and indicates a lower than expected signal-to-noise ratio for the implemented chip. The main cause of this performance degradation, however, has been confirmed to be not caused by the circuit itself, but, rather, by the final top-level chip implementation that also includes different circuits on the same die.</p>

<p>As the main focus on our study of continuous-time sigma-delta modulators (CT-SDM) for analog-to-digital (ADC) application, we investigate the feasibility of an inverter-based operational transconductance amplifier (OTA), referred to here as Nauta&rsquo;s OTA, that relies on output conductance cancellation to increase DC gain in the integrator stages of the CT-SDM. Implementations of digitally configurable Nauta&rsquo;s OTA were used in our study to understand its behaviour within the system, and we propose a tuning technique based on phase measurement to estimate its DC-gain within the structure of the sigma-delta modulator circuit. A test chip of a second-order continuous-time sigma-delta modulator, consisting of Nauta&rsquo;s OTA integrator stages with additional tuning support circuitry, has been implemented in a CMOS 180 nm process and is awaiting measurement results. In addition, separate circuit measurements and algorithm testing have been performed using discrete components along with our digitally-configurable Nauta&rsquo;s OTA, confirming our proposed tuning technique of the OTA in active-RC integrator circuits. This proposed strategy has also been confirmed to be operational in sub-threshold voltage supply. From these results, we aim to propose a design strategy that optimises the figure-of-merit of the CT-SDM through digital tuning with minimal overhead or performance penalty.<br />
</p>
</tr>
</tbody>
</table>
<a name="DF08"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF08: Collaborative Approach for Indoor Localization</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Marzieh Jalal Abadi</p>
<p>Professor Mahbub Hassan</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Indoor localization is currently essential for a variety of new applications and their performance will significantly improve by using accurate position information of mobile nodes in real time. Although many network-based techniques have been proposed, they require infrastructure supports which may not be always available, such as in emergency situation. Besides, they usually need high power and high-density infrastructure to offer additional coverage. Pedestrian dead reckoning (PDR) is the infrastructure-free localization method that determines the position of people using inertial sensors. The principle of PDR is to recursively update the position of the pedestrian after a step has been taken using the last known position, the estimated heading and the step length. Heading estimation in indoor environments is highly problematic and degrades the performance of PDR. Gyroscope and magnetometer are typically employed for heading estimates: the former can be used for a short time and suffers from accumulated errors, while the magnetometer estimation is highly erroneous due to the presence of magnetic anomalies in indoor environments.<br />
The first contribution of this thesis is a novel cooperative architecture among peer nodes to improve the accuracy and reliability of the position information in PDR. We exploit the spatial diversity of magnetic perturbation to combat heading error and introduce the fusion of multiple estimates from subjects walking in the same direction. Collaboration between pedestrians is enacted by device-to-device WiFi communication to share their individual estimates. The proposed architecture consists of three components. The first component exploits machine learning algorithm and signal strength. It detects pedestrians walking along the same direction and identifies them as a &ldquo;group&rdquo;. The second component uses another machine learning algorithm and magnetometer measurements to detect whether a heading estimate is within a specific error margin. The erroneous heading will be then discarded and only the estimates that overcome both components pass for fusion. Finally, in the third component, the consensus algorithm is utilized to fuse the estimates of the group members. <br />
The second contribution of this thesis addresses optimum parameters of the cooperative approach to minimize the cost per node. Despite the additional accuracy and coverage offered via cooperative network, we address the issues of this approach such as energy consumption, rate of fusion update (overhead) and optimum number of cooperating nodes (coverage). We create a cost function that composes of two components: power consumption and accuracy. Energy consumption and accuracy of a node relate to the number of cooperative nodes and we propose two models for both components. Then we solve the optimization problem for the cost function to achieve optimum number of cooperative nodes. We also propose a ranking algorithm for an efficient scheduling of the nodes to balance energy consumption of the network. <br />
For the third contribution, we implement the proposed architecture and its optimum parameters over a complicated path to evaluate the robustness and rate of fusion update (complexity). All the proposed models and evaluations are based on the datasets obtained from several real experiments. The results are analyzed and discussed in detail.</p>
</tr>
</tbody>
</table>
<a name="DF09"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF09: Computer Vision System for Agricultural Yield Estimation
</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Sisi Liu</p>
<p>Mark Whitty</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>A visual yield forecasting system is presented in this research for achieving satisfactory crop estimation for the grower and winemaker. Currently, industrial standard forecasts are generated by manual sampling of bunch weights, grape size and grape numbers. Seasonal predictions take significant time and effort. Thus, only a very small proportion of the vines are sampled. In order to reduce the intensive labour of sampling, decrease the cost of labour training and increase the accuracy of final yield prediction, a computer vision system is proposed for forecasting crop production. This system covers five major research topics: <br />
&bull; Biological baseline for scheduling video sampling and yield estimation. <br />
&bull; Image processing for shoots, bunch, berry detection and counting.<br />
&bull; Unsupervised learning for feature optimization and object detection.<br />
&bull; Data visualization for mapping the production variation in vineyards.<br />
&bull; Automatic visual system design. <br />
The primitive problems that need to be addressed in this visual system are classified according to the following 3 issues: <br />
&bull; The current traditional yield estimation approach is not accurate for yield forecasting so this system should be able to provide a reliable biological baseline for yield estimation. <br />
&bull; Current image processing methods applied in bunch detection cannot be robustly applied to red and white grapes. Also prior work has not been conducted on shoots and inflorescences detection on the vine. Counting flowers, berries and bunches and tracking berry development on the vine are vital issues for the grape grower that provide them with crop information. Hence, this system should be able to detect all aforementioned objects by computer vision techniques with certain accuracy. <br />
&bull; Currently all information about yield estimation and production variation are manfully processed, therefore, this system needs to present an automated way to integrate all the information extracted from video throughout the season to provide an accurate yield map. <br />
From the perspective of these issues, this research focuses on defining and confirming the biological components which could be calibrated to the final grape yield, extracting relevant data for these factors from video automatically, assembling a video sampling schedule into the prediction system for long term tracking of grape growth, and designing a prediction system. This estimation mechanism is more accurate being based on the information extracted from these digital images from video as it eliminates the subjective bias of human beings. Also it increases the scope of sampling on a large scale. This promising approach not only brings significant economic benefits for wineries and grape farmers but also provides a visual system which can help farmers to better maintain their vineyards and reduce natural resource consumption. </p>

<p>
</p>
</tr>
</tbody>
</table>
<a name="DF10"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF10: Learning and Reasoning in the Robotic Disassembly of End-of-life Electronics</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Wei Chen</p>
<p>Prof. Sami Kara</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>With the fast-paced development and obsolescence of consumer electronics, end-of-life (EOL) treatment of these products has become a concern. Many electronic components contain substances that are hazardous to human health and the environment. The recovery of materials is desirable in view of future resource scarcity. International bodies and countries, including Australia, are increasingly introducing regulations to encourage safe EOL treatment methods. Disassembly allows the separation of components or material fractions from the product. This has seen limited automation outside of original equipment manufacturers because, unlike the manufacturing industry, recyclers deal with products that arrive in varying conditions, unpredictable lot sizes of each product and a general lack of product design information. The challenges of system inaccuracy and poor product condition lead to random failure of disassembly operations. This research investigates the potential for a robotic system to address these specific challenges. To reduce setup costs, the user demonstrates the location of actions and fasteners such as screws on a camera image. Acquired product information is saved in an ontology and can be recalled at a later time, potentially by other systems. The recorded product information can be used to automatically generate options for actions, whereby redundant options can substitute where an operation has failed. These concepts allow a robotic system to gain the flexibility required in the disassembly domain that is lacking in typical assembly robotics</p>
</tr>
</tbody>
</table>
<a name="DF11"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF11: Sensorless Control of Interior Permanent Magnet Machine By Using PWM Synchronous Position Estimation Method</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Deqi Guan</p>
<p>Prof. Faz Rahman</p>
<p>Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>Sensorless control of interior permanent magnet synchronous machines (IPMSM) has received wide attention in modern electrical drive systems due to its lower cost, robustness and reliability. Due to the absence of a shaft mounted mechanical position sensor, the rotor position needs to be estimated synchronously when the motor is running. However, for the existing sensorless control methods, the machine cannot operate over a wide speed range and the torque bandwidth is reduced compared to the sensor-based control method. This project will develop and extend the latest sensorless method&mdash;Fundamental PWM Excitation (FPE)&mdash;which has already been attempted on a Surface Mounted Permanent Magnet Synchronous Machine (SPMSM) or IPMSM. Due to the more complicated rotor configuration, applying FPE to IPMSM is much more complex and the mutual inductance between each phase cannot be ignored. Compared to the existing sensorless control method, FPE has higher torque bandwidth thanks to the high bandwidth current sensoring and can drive over a wider speed range including low speed and standstill. The principle of FPE is based on relating the measured incremental inductance, using the rate of change of currents between two or more adjacent PWM current pulses, to the rotor position. Because FPE uses the normal fundamental PWM sequence as test vectors, the current derivatives for two active vectors and the null vectors are measured over successive normal PWM switching cycles. Therefore, no special test vectors is required. The main difficulty of this method is that parasitic effects arises when an IGBTs&rsquo; state switches, and these effects will cause the current to oscillate a few microseconds every time the IGBTs turn on and off. The current derivative required for the rotor position estimation cannot be measured until this oscillation disappear. If the length of the PWM vector is less than the duration of current oscillation, voltage vector extension must be implemented in order to make an accurate measurement of the current derivative. Since the extension of voltage vectors will bring significant distortion to the motor current, compensation must be taken to reduce this effect. <br />
In order to measure an accurate current derivative, high bandwidth AMR current sensors are used, and these send the current signal to an FPGA via a high bandwidth ADC. This kind of current sensor employs anisotropic magnetro-resistance technology and has bandwidth as high as 2 MHz. The current derivatives are derived by extracting the underlying gradient using a curve fitting algorithm on the measured stator currents, which are sampled at high frequency. Once the current derivative for the required voltage vector is calculated, the position vector can be estimated through the relationship between incremental inductance of each phase and the current derivative. Extensive simulation results are shown to verify the effectiveness of the proposed method, which exhibits high dynamic and steady-state performances over a wide speed range, including at very low speed and standstill, as well as higher torque bandwidth.</p>
</tr>
</tbody>
</table>
<a name="DF12"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF12: Exploiting Similarity of Structure in Reinforcement Learning with QBOND</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Sean Harris</p>
<p>Bernhard Hengst, Maurice Pagnucco</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Robots and AI agents are becoming increasingly prevalent in our ordinary lives, solving a large variety of tasks. To be a truly robust system, an agent must be able to learn from its interactions with the world and improve its future behaviours without requiring a human to reprogram it.<br />
An agent can learn from its experience through reinforcement learning, where it examines its state, takes an action, and receives a reward. The problem with this approach is that it doesn&rsquo;t scale well to large problems where the agent must try many actions in many states before learning to behave sensibly. My approach, called QBOND, exploits the underlying repetition in real world problems that often arises due to the laws of physics. My approach is to decompose a problem into small generic regions, learn policies for each generic region, then bond them together again to solve the overall task. Using generic regions allows policies to be learned for each region, and then the learning from that region to be reused for every other similar region. This can drastically reduce the learning required and allow an agent to quickly learn to solve a large complicated task.<br />
The first key contribution of QBOND is that it can decompose tasks across wide boundaries, not being restricted to obviously separable tasks such as navigating between two rooms via a narrow doorway. The second contribution is its ability to integrate discounting, and thus solve continuing tasks, where there isn&rsquo;t a specific end goal where the problem terminates. The final contribution is a method for learning a set of policies to solve each generic region.<br />
I have demonstrated my approach on two challenging simulation tasks. The first is to balance a pole on a moving cart driving on a table. The second is to drive a race car around a racetrack as fast as possible. These two tasks are both continuing, with no defined &ldquo;finish&rdquo; point, and can be arbitrarily large, depending on how big the table is, or how long the race track is.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI06"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI06: Energy-efficient Human Activity Recognition for Wearable Devices</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Sara Khalifa</p>
<p> Mahbub Hassan
</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Recent advancements in wearable devices enable a wide era of activity-aware services in various domains, including healthcare, indoor positioning, and fitness management. Particularly, wearable sensors-based Human Activity Recognition (HAR) has recently become the focus of intense research and development, thus producing a wealth of tools and algorithms to accurately detect human activities (e.g., physical and linguistics) from data collected by the wearables. </p>

<p>However, the major challenge of wearable sensors is the battery lifetime. While it is possible to extend the battery lifetime by providing more energy-efficient solutions, battery-powered sensors cannot provide sustained HAR without the need for battery replacement. This motivates us to explore Energy Harvesting (EH) solutions. EH is commonly referred to the conversion of ambient energy such as solar, kinetic, vibration&#8230; etc., into electrical energy. EH eliminates the need for battery replacement and significantly enhances the versatility of consumer electronics. However, EH generally suffers from low power output, which may challenge the power requirement of the wearables components, such as the accelerometer used to sense physical activities or the microphone used to sense human voice.</p>

<p> To address these challenges, our research investigates four fundamental contributions. </p>

<p>Firstly, we propose the novel concept of adaptive-HAR, which relies on the observation that only a few activities are possible at any given time. Therefore, instead of using a single complex classifier, we design several simple classifiers and switch to the right classifier depending on the current context. Our adaptive HAR significantly reduces both the sampling and the classification overheads without sacrificing accuracy. </p>

<p>Secondly, we propose a novel paradigm for energy harvesting HAR. Our novel approach employs kinetic energy harvesting (KEH) and infers human physical activities, such as walking, running, &#8230;, etc, directly from the KEH patterns without using any accelerometer. By not using accelerometer, a significant percentage of the limited harvestable energy can be saved. We also show that we can go beyond into the fine details of the KEH walking signals and identify how many steps the user has taken. </p>

<p>Thirdly, we propose a new framework that guarantees energy neutrality for HAR using KEH patterns. We basically transmit an unmodulated signal, called an activity pulse, using the accumulated power in a fixed-length time window. Because different activities generate power at different rates, the transmission and receiving signal strengths are different among different activities. Thus, those signal strengths can be used to classify the activities. Energy neutrality is guaranteed because the transmission power of the activity pulse only uses the amount of energy harnessed in the last time window, and no additional energy is required to power any sensing or classification components in the wearable device.</p>

<p>Finally, we investigate vibration energy harvesting (VEH) as a potential new source of information for detecting hotwords, such as &ldquo;OK Google&rdquo;, used by popular voice control applications to distinguish user commands from other conversations. Unlike existing sensors, like microphones, or accelerometers, VEH does not consume as much power. Thus, the proposed method provides zero power audio sensing for hotword detection which enables pervasive voice control at minimum energy cost.</p>
</tr>
</tbody>
</table>
<a name="EI07"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI07: Distributed Processing for Energy Management in Smart Grids</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Mediwaththe Gedara</p>
<p>Prof. Aruna Seneviratne and Dr. David Smith (NICTA supervisor)</p>
<p>Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>Rising electricity demands drive utilities to exert various demand-side management approaches, such as dynamic pricing methods, in electricity networks in many parts of the world. Decreasing costs and increasing applications of energy storage devices, and high penetration of renewable energy generations such as solar photovoltaic (PV) power generation, offer vast opportunities for demand-side electricity management, especially in residential areas. In particular, small-scale community energy storage (CES) devices that are located close to residential energy users can be exploited for effective small-scale demand-side management such as in residential-gated communities. Moreover, these small-scale energy storage devices can be effectively employed for storing surplus energy generated from distributed household PV panels so that the stored energy can be dispatched to support users&rsquo; peak energy demands.</p>

<p>The key objective of this research is to investigate novel demand-side management techniques for residential neighbourhood area networks using CES devices and household-distributed PV power generation where residential energy users with PV power generation capabilities are given incentives to trade energy with the CES device. In prior research, significant effort has been devoted to demand-side management with various distributed energy resources such as renewable energy resources and energy storage devices. In particular, some of them are based on centralised control requiring accurate power information of individual distributed energy resources for robust operation. In a similar way, energy trading with the CES device can be implemented using a centralised approach. However, obtaining private energy information of residential users is often not practical and manipulating energy decisions of users by a central controller is less pragmatic.</p>

<p>In this thesis, a novel decentralised energy trading system between a CES device and residential energy users for demand-side management is described. The energy transactions between users and the CES device are analysed using non-cooperative game theory incorporating complex and self-interested behaviour of each individual entity of the system. In the first step, a non-cooperative repeated energy trading game is developed to analyse individual energy trading decisions of users who interact non-cooperatively with each other to minimize personal energy costs. Secondly, the same energy trading technique is extended to a multi-level decision making system where the community energy storage controller also intends to maximise revenue by leading the energy trading game for non-cooperative users. This hierarchical rational interaction between the CES controller and the users is explored using non-cooperative Stackelberg game theory. Non-cooperative game-theoretic analysis of the decentralised energy trading system is shown to have important outcomes such as significant energy cost savings for independent participating users by attaining Pareto-optimal Nash-equilibrium energy trading strategies that also help to regulate peak energy demand on the grid. Current research explores impacts of realistic energy user behaviour in making their participating decisions on the game-theoretic hierarchical energy trading system by devising user-behavioural framework based on Prospect theory.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI08"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI08: Smart Grid Modelling and Control</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Thai Vo</p>
<p>Dr Jayashri Ravishankar & Prof John Fletcher</p>
<p>Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>
The emergence of Micro-Grids (MGs) and renewable energies in the Australian electricity network is anticipated to be a possible pathway of achieving clean energy generation, outlined by Mandatory Renewable Energy Target (MRET). MGs are distributed generation systems that are designed to operate as self-contained, local, electrical power grids, often incorporating on-site renewable energy sources such as wind turbines. However, in MGs, unbalanced network conditions cannot be neglected. The presence of non-zero negative sequence voltages with unbalanced operation can lead to significant fluctuations in currents, powers and other related parameters. By investigating the behaviours of networks under this condition and proposing an effective solution to tackle problems, this research has been focusing on intelligent control for power electronic interfaces within MGs.</p>

<p>With the remarkable development of DSP computational power and installation cost reduction in recent years, our research will utilise the contemporary controller (Linear Varying Controller) for the electric machine and power electronics field. This type of controller can deal with a system that has parameters fluctuating in the time domain but within a predictable boundary. The methodology includes mathematical model formulation, simulations, testing and experimental verification of the proposed controller.</p>

<p>So far, with the proposed controller, the study has analysed the system response under extreme conditions that may happen in a normal network and in MGs. The system models include the Doubly-Fed Induction Machine and Rotor Side Converter (RSC) sharing DC link with Grid Side Converter. In order to obtain convincing results, the simulations were done with different platforms: Matlab/SIMULINK (SIMULINK mathematical blocks and Matlab *.m file code) and DiGSILENT/Power Factory. The code-based simulation is a step towards the final DSP implementation as the syntax of m code in Matlab is quite similar to C codes in a DSP development environment.</p>

<p>In terms of experimental verification, the controller has been explored and applied to the power electronics interface connecting to the outside network. The inverter-based interface is the most common used in renewable generation. With the support of the dSPACE 1104 Development Board (DSP TMS320F240), the control algorithm has been deployed, validating the system response predicted by the design process and simulation results.</p>

<p>During the design and implementation process of this research, it is noteworthy that:</p>

<p>(i) The power electronics interface that was being investigated is universal and can be applied to any type of DC/AC conversion structure used for solar panels, wind turbines or fuel-cells.<br />
The mathematical model of the interface and controller design are general enough so that they can be re-used for the general cases of an induction machine, which are common in industry, especially in wind generation technologies.</p>
</tr>
</tbody>
</table>
<a name="EI09"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI09: Nickel Catalysts for the Carbon Dioxide Reforming of Methane: Modulating Support Properties Via Flame Spray Pyrolysis</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Emma Lovell</p>
<p>Dr Jason Scott and Professor Rose Amal</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>Since 1750 global concentrations of the most destructive greenhouse gases have increased significantly (carbon dioxide by 40&#160;%; methane by 150&#160;%) [1]. A key area of interest in the energy sector is the efficient consumption and utilisation of fossil fuels, particularly natural gas, which despite being the &lsquo;cleanest&rsquo; of the fossil fuels, is costly and energy intensive to store and transport. Concurrent with an increase in natural gas usage, accounting for approximately 24 % of worldwide global energy consumption in 2014, known reserves have increased by over two and half times since 1980 and hence there are further opportunities for future utilisation of methane as an energy source [2]. The carbon dioxide, or dry, reforming of methane has been extensively studied due to its ability to consume methane and carbon dioxide to produce synthesis gas (H2 and CO). Synthesis gas can subsequently be converted to a variety of liquid fuel products including synthetic diesel via the Fischer-Tropsch reaction or methanol, which resolves the issue of storage and transport. <br />
The development of a catalyst that is not only active and stable but also economical and simple to produce is essential to implement the dry reforming system on an industrial scale. As nickel (Ni) catalysts for the dry reforming of methane have a tendency to deactivate due to carbon formation and/or sintering, tailored catalyst design is essential in the development of a catalyst that can be commercially viable. Flame spray pyrolysis (FSP) is a scalable, reproducible method for high-throughput nanomaterial production [3] that has not been extensively studied for the dry reforming of methane. <br />
In this thesis, FSP was utilised to produce supports for Ni catalysts for the dry reforming of methane. The impact of varying precursor feed rate on silica particle size and surface properties were evaluated with respect to Ni dispersion and catalytic activity and stability. The results showed that varying flame synthesis conditions can result in more active and stable catalysts with high surface area and residual carbon species both aiding in Ni dispersion, thus enhancing activity. However, the silica supported samples still suffered significant deactivation due to carbon formation. To overcome this, FSP was used to synthesise ceria-zirconia/silica mixed metal oxides in varying ratios. Ceria-zirconia is a high oxygen storage capacity material that has the ability to oxidise any formed carbon, in principle overcoming sample deactivation. The results showed that altering the ratio of silica to ceria-zirconia within the flame provides a range of differing structures. By varying the ratio the Ni properties as well as catalytic activity and stability were positively impacted. The project demonstrates the ability of FSP to produce effective Ni catalyst supports for the dry reforming of methane with tuneable properties to enhance catalytic performance, and thus shows the potential for development on an industrially-viable scale. </p>

<p>[1] P.K. Pachauri and L.A. Meyer (eds.). Intergovernmental Panel on Climate Change, (2014). <br />
[2] F. Holz, P.M. et al., Review of Environmental Economics and Policy, 9 (2015) 85&ndash;106. <br />
[3] L. Madler et al., Journal of Aerosol Science, 33 (2002) 369&ndash;389.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI10"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI10:Towards High Specific Heat Molten Salts, Nanosalts, for Thermal Energy Storage of Concentrated Solar Thermal Power Generation</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Hamed Riazi</p>
<p>Dr Robert Taylor, Dr Sara Mesgari</p>
<p>Mechanical &amp; Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>Specific heat is a key thermal property in energy systems and is directly linked to heat storage and transfer. Concentrated solar thermal (CST) power plants which incorporate thermal storage show promise to deliver cheap, renewable and reliable electricity without harmful emissions. As compared to wind and photovoltaic installations, which are intermittent, CST systems (with thermal storage) can provide a good match with energy demand. At present, molten salts represent the dominant thermal storage medium. However, the low specific heat value of molten salts is a disadvantage for these heat transfer fluids. Increasing the specific heat of molten salts could allow for a sizable reduction in storage volume (or for more energy to be stored in the same volume). Hence, enhancing the specific heat capacity of molten salts by nanoparticle dispersion has emerged as a means to reduce the capital costs of thermal storage for concentrating solar thermal power plants, enabling cheaper solar electricity. Nanosalts, molten salts doped with nanoparticles, have been a source of controversy in the last few years, with &ndash;19% to +118% change in specific heat being reported. This study addresses these discrepancies and provides researchers with better knowledge of how to control the preparation methods to yield reliable specific heat enhancements.</p>

<p>This study proposes that much of the discrepancy can be related back to nanoparticle morphology and preparation methods, including sonication time/intensity on the specific heat of nanosalts. To date, these issues have not been systematically controlled in the literature. It was found that it is necessary to control the nanoparticle synthesis process and the sonication energy independently to achieve reliable enhancements. Results also show that different particle morphologies result in different optimum sonication power/energy requirements, and result in a differing minimum particle size distribution which directly impacts the change of the effective specific heat of the nanosalts. In this study stable nanosalts were produced with a 17.6% enhancement in specific heat. This finding is important because thermal energy storage systems represent up to one third of the total solar thermal power plant capital costs. Thus, stable nanosalts represent a feasible pathway towards generating cheaper solar electricity. </p>
</tr>
</tbody>
</table>
<a name="EI11"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI11: The Study of Soil-Water Characteristic Curve in Deformable Porous Media</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Amin Yousefnia Pasha</p>
<p>Prof. NASSER KHALILI & Dr. ARMAN KHOSHGHALB</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Soil-water characteristic curve (SWCC) builds a fundamental relationship between the amount of water available in a soil and its matric suction and has extensive practical applications in different science and engineering disciplines. SWCC serves as the key relationships in the mechanics of unsaturated soils that is used to estimate other unsaturated soil property functions (e.g. permeability and shear strength) in geotechnical engineering practice. It is also used extensively in geo-environmental engineering practice to predict the flow and transport of multiple fluids in porous media as well as in soil physics and agricultural-related science for the estimation of water storage and thermal property functions. Past experimental evidence has shown that SWCC evolves with mechanical stress and structural changes in soil matrix known as hydro-mechanical coupling effect in porous media. Models currently available in the literature for capturing the volume change dependency of the SWCC are mainly phenomenological in nature requiring an extensive experimental programme for parameter identification which renders them unsuitable for practical applications. This research was devoted to explore the topic through experimental investigations as well as theoretical modeling approaches. <br />
In the experimental part of the thesis, strain- and load-controlled unsaturated consolidometer tests have been performed in the Geotechnical Research Laboratory of the School of Civil &amp; Environmental Engineering. The novelty of my experimental studies was that for the first time I obtained water retention curve of the soils in constant-volume conditions which serves as important dataset to investigate the pure effects of suction and volume change on the hydraulic properties of unsaturated soils. <br />
The modeling works of the thesis has led to two models, based on independent approaches, to account for the volume change dependency of the SWCC in a deformable soil. The first approach is based on energy considerations. The model does not require any additional material parameter apart from the parameters specifying the SWCC for the reference volumetric strain and incorporates the effects of hydraulic hysteresis and volume change dependency of the scanning curve which is rarely addressed in the literature. A unique feature of the model is its ability to capture the change in the hydraulic path of the soil from scanning to main in a mechanical loading event. The second model is based on a fractal approach considering the changes in pore shape with loading. A feature of this model is that it relates the SWCC parameters to easily quantifiable parameters such as the fractal dimension of soil particle size distribution that can be obtained simply from a particle grading test. Also, the fractal dimension of pore size distribution (PSD) varies with void ratio in the presented model, an aspect frequently neglected in the literature.<br />
The application of the above models is also demonstrated through comparison between the results from the model and experiments. Very good agreement is observed highlighting the ability of the proposed models in capturing the volume change dependency of the SWCC.</p>
</tr>
</tbody>
</table>
<a name="FS12"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS12: Investigation of Dynamic Crack Coalescence Using a Gypsum-like 3D Printing Material</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chao Jiang</p>
<p>Gaofeng Zhao & Nsser Khalili</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Dynamic crack coalescence attracts great attention in rock mechanics. However, specimen preparation in experimental study is a time-consuming and difficult procedure. In this work, a gypsum-like material by Powder bed and inkjet 3D printing (3DP) technique was applied to generate specimens with preset cracks for split Hopkinson pressure bar (SHPB) test. From micro X-ray CT test, it was found that the 3D printing technique could successfully prepare specimens that contain preset cracks with a width of 0.2 mm. Basic mechanical properties of the 3D printing material, i.e., the Elastic modulus, the Poisson's ratio, the density, the compressive strength, the indirect tensile strength, and the fracture toughness, were obtained and reported. Unlike 3D printed specimens using Polylactic acid (PLA), these gypsum-like specimens can produce failure patterns much closer to those observed in classical rock mechanical tests. Finally, the dynamic crack coalescence of the 3D printed specimens with preset cracks were captured using a high-speed camera during SHPB tests. Failure patterns of these 3D printed specimens are similar with the specimens made by Portland Cement Concrete (PCC). Our results indicate that sample preparation by 3D printing is highly competitive due to its quickness in prototyping, precision and flexibility on the geometry, and high material homogeneity. </p>

<p>
</p>
</tr>
</tbody>
</table>
<a name="FS13"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS13: Experiment and Numerical Simulation Study of Shear Performance of Rock Bolt Under Static and Dynamic Loading Condition</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Li Li</p>
<p>Paul Hagan, Serkan Saydam, Bruce Hebblewhite</p>
<p>Mining Engineering</td>
</tr>
</tbody>
</table>
<p>With the depth increasing of underground mining, high stress and dynamic disturbance are becoming two prominent and complicated characteristics. Underground excavations are subjected to complex stress caused by nearby mining, but may also be affected by repeated dynamic loading from blasting and seismic events. It is well known that the stability of underground openings depend on both ground stress and geological structures, moreover, the underground support plays a significant role in maintaining the stability of surrounding wall by helping the rock to carry its inherent loads and to reinforce the rock mass.<br />
The less understanding of ground support scheme integrated performance in dynamic conditions have several reasons,<br />
&bull; The occurrence of a rockburst is unpredictable in its time, location, magnitude, and the scale of damage (Brown, 2004), therefore it is uncertain how the rockburst affect the behaviour of rockbolt system<br />
&bull; The characteristic of rock mass under high stress and dynamic load condition is remain unclear, in addition the weakness joint may become the channel of energy release and complicate the behaviour of rockmass<br />
&bull; The rock-support dynamic interaction mechanism is very complex due to multiple uncertainty factors mixed together<br />
Stacey (2011) had pointed out that there is a lack of understanding of the mechanisms of action and interaction of support elements in a seismic condition, especially the dynamic shear behaviour. This thesis will concentrate on the mechanical behaviour of support systems before and after the tunnel subject to dynamic loading. The support system includes single element and various combinations of surface support and reinforcement.<br />
Therefore, a series double shear test were conducted under static loading condition (strain rate 10-5 /s) and dynamic loading condition (strain rate 1-10 /s), consisting of three rock blocks (20*30*30cm each) reinforced with rockbolts. Other parameters, such as bolt diameter, angle between bolt and normal to the joints, and dynamic load magnitude would be studied by numerical simulation (Flac3D) in order to reduce the amount of samples. <br />
There are three points why I have adopted the above methods to complete my thesis:<br />
(1) Dynamic damages frequently happened in deep mining<br />
(2) Structure planes control failure models of tunnel<br />
(3) Reinforcement with surface support can develop a better effect to control displacement<br />
Only the performance and mechanics mechanism can be clearly understood, design of ground support system could be more practical and effective. The objectives of my research are: <br />
(1) Evaluate the static and dynamic shear capability of ground support element and interaction with jointed rock mass;<br />
(2) How dynamic load effects the development of existing fracture;<br />
(3) Investigate the shear deformation behaviour of rockbolt in resisting joint movement by evaluating the load-displacement characteristic and energy absorption capacity.<br />
The purpose of this study was to improve the mechanistic understanding of this interaction between the rock mass and the support system to improve the design consideration of support system in rockburst environments.</p>

<p>
Brown, E, 2004. The dynamic environment of ground support and reinforcement, in Ground Support in Mining and Underground Construction (pp 3-15 (Taylor &amp; Francis).<br />
Stacey, T R, 2011. Support of excavations subjected to dynamic loading, in Proceedings 12th ISRM International Congress on Rock Mechanics, Beijing, China (International Society for Rock Mechanics: ISRM).</p>
</tr>
</tbody>
</table>
<a name="FS14"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS14: Modelling of Ultrasonic Reactors</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yezaz Gadi Man</p>
<p>Dr. Francisco Trujillo ; Dr. Patrick Spicer</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Ultra-sonication can be considered as a process intensification in which the liquid in a vessel is processed by an immersed vibrating sonotrode. The modelling and design of an ultrasonic reactor, which is needed to scale up such reactors, is based on understanding the complex nonlinear interaction between the acoustic pressure field and the generated cavitation bubbles in the liquid. These net interactions result in an acoustic-hydrodynamics flow which is generally called acoustic streaming. The acoustic streaming is defined as the time-independent motion resulting from the attenuation of the sound wave where momentum of the wave is transferred to the liquid. Attenuation is due to the sound wave scattering by inertial cavitation bubbles which are formed below the sonotrode. The acoustic streaming has potential modern applications in food processing, therapeutic treatment and diagnosis, microfluidic devices, sensors, micro-reactors, semiconductor manufacturing, biomass, pharmaceutical, wastewater treatment and chemical processing.</p>

<p>
A nonlinear Helmholtz equation, which is derived from governing equations of a bubbly liquid, is applied to simulate the pressure field inside the reactor. The nonlinear Helmholtz equation comprises a complex wave number that is a function of the number density of bubbles, energy dissipation of cavitation bubbles and local magnitude of acoustic pressure. A correction of classical radial bubble dynamics models was developed to account for pressure inhomogeneities in the bubbles. The numerical prediction of radial bubble dynamics with inhomogeneous pressure gives a better agreement with experimental data than homogeneous pressure formulations. The Helmholtz equation was coupled with the bubble dynamics model via energy dissipation factors. The generation and dissolution of the bubbles are described based on the density of bubbles; which is defined through a step function. Thence the nonlinear Helmholtz equation was solved in the commercial multiphysics solver COMSOL. Furthermore, we coupled the pressure field in the liquid phase with vibration of the ultrasonic horn. This frequency domain solution was used to calculate the forces which generate acoustic streaming. Those forces were derived from the divergence of Reynolds stresses which is calculated from the particle velocity obtained from the solution of the pressure field. Acoustic forces were incorporated into a k-&#1013; turbulence model to obtain acoustic streaming. The simulated streaming velocity was validated with laser Doppler anemometry (LDA) experimental data published in the literature for a two-dimensional axisymmetric cylindrical vessel. We found a good agreement between the velocity profiles, but below the sonotrode the simulated velocities are relatively higher. This is due to step function which resulted in high concentration of bubbles. To improve the results we propose to model the transport of bubbles via a convection-diffusion equation. The developed transport equation includes velocity and diffusivity of bubble field. The mobility of cavitation bubbles in Einstein-Smoluchowski diffusivity equation was derived from momentum equation of a bubbly liquid. It is expected that the obtained bubble concentration profile will improve acoustic streaming results.<br />
</p>
</tr>
</tbody>
</table>
<a name="FS15"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS15: Evaluating Cable-stayed Bridges Subjected to Blast Loading</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Komeil Hashemiheidari</p>
<p>Mark A. Bradford</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Over the past two decades, blast loads have been recognised as one of the extreme loading events that must be considered in the design of important structures such as cable-stayed bridges. However, the design provisions for blast-resistant bridges are limited due to inadequate knowledge of the local and global dynamic response of the bridge components under the blast loading scenarios. The air blast which is generated by the explosion of bombs or fuel tankers can result in partial collapse of the bridge and significant loss of life and property as well as social and economic devastation. <br />
To design blast-resistant bridges, an understanding of the blast wave propagation and its effects on the bridge structures are required. Computer simulations can be used effectively to simulate the explosion and structure, so as to study the interaction between. Recent advances in numerical methods and developments in high-performance computer facilities have enabled engineers to simulate complicated blast scenarios in a viable, efficient and cost-effective manner and subsequently to provide useful benchmark data for safeguarding design of critical infrastructure in the future. To reduce the computational expense of conducting blast analyses on large or complex bridges, the sub-structuring technique, in which only part of the structure is modelled, has been used in current practice. However, the simplifying assumptions adopted in these sub-structuring methods can lead to erroneous results. Accordingly, this study develops detailed finite element models of an entire cable-stayed bridge subjected to blast loading using the LS-DYNA FE code. Based on the best practice data obtained from a thorough review of the literature, the blast load estimation, material modelling and detailed numerical simulation are conducted. Three different explosive sizes, i.e. small (01W), medium (04W) and large (10W), are considered (W being the TNT equivalent explosive weight index) and placed at different locations above the deck or adjacent to the pylon to determine the influence of the size and location of the blast loads on the global and local response of the bridge components. The results of the FE simulations are used to characterise the type and extent of damage on the pylon and deck, and also investigate the potential cable loss and progressive collapse scenarios associated with. Furthermore, the values of the so-called demand-to-capacity ratio (DCR) at different sections along the pylons are computed to establish a damage criterion. In addition to the DCR values, the maximum strain criterion is used to evaluate and compare the blast performance of the pylon with a modified octagon cross-section compared to conventional rectangular box sections. Different protective methods such as sacrificial concrete blocks and CFRP application on pylon are proposed and compared to find the best available option to improve the performance of existing cable stayed bridge subjected to blast loads. Results also showed that the shorter cables (closer to the pylons) were more vulnerable to direct rupture due to the dramatic changes in their axial force during the explosion. Hence, new Dynamic Amplification Factor (DAF) is proposed to prevent any instability in the structure.</p>
</tr>
</tbody>
</table>
<a name="FS16"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS16: Vibration Based Gear Wear Monitoring</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chongqing Hu</p>
<p> A/Prof. Zhongxiao Peng; Prof. Robert Bond Randall</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
Gear tooth wear is an inevitable phenomenon that has a significant influence on gear dynamics. The direct results of gear wear include unsteady gear ratio, power transmission losses, and high vibration and noise levels. Although vibration analysis has been widely used to diagnose localised gear tooth faults, its techniques for gear wear monitoring are not well-established. This research aims to develop a set of vibration indicators to evaluate the effects of wear on gear transmission, and to identify the gear wear states. 
	
The effects of tooth wear on gears can be described through variations in gear transmission errors (TEs). As known, TEs come from the geometric deviation of the tooth surface, the elastic deformation inside the contact area and the tooth deflection. Wear modifies the tooth surface topography and the gear tooth profile, through which the load distribution and the contact area are also changed. This means the effects of tooth wear will be reflected by changes in the vibration signals. 

An averaged logarithmic ratio (ALR) is developed to evaluate the effects of tooth wear. First, a gear state vector is extracted from time synchronous averaged gear signals to describe the gear state. This gear state vector consists of the amplitude modulation ratios obtained from all available tooth meshing harmonics and their sidebands. Then, the ALR is proposed to measure the distance between two gear state vectors. This vibration indicator can be applied in two ways. To evaluate the effects of wear on gear transmissions, a fixed reference is utilised in its definition, and the indicator ALR shows the changes in the gear transmissions relative to the reference conditions. The larger the value, the more serious the gear is worn. To identify the gear wear state, a moving reference is used in the definition to show the changes in gear transmissions within short periods, and a cumulated value is employed to represent the total distance travelled by the gear state. When wear state changes, it will modify the gear state at a different rate, giving rise to different levels of the ALR values. The efficiency of these vibration indicators is demonstrated using experimental results from three sets of tests, in which the gears experienced different wear processes. In addition to gear wear monitoring, the proposed indicator can be used as a general parameter to detect the occurrence of other faults, such as a tooth crack or shaft misalignment, because these faults would also change the gear vibrations. 



</tr>
</tbody>
</table>
<a name="FS17"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS17: Design of Rocket Nozzle Contours Using Conjugate Circular Arcs</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Kyll Schomberg</p>
<p>John Olsen</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>As a society, we demand constant and cost-effective access to a range of earth&rsquo;s orbits for our daily telecommunications and entertainment needs. Currently, this access is solely dependent on chemical rocket propulsion systems that produce thrust by converting the thermal energy of the combustion products into kinetic energy using a supersonic nozzle. In spite of the integral role of a supersonic nozzle in a rocket propulsion system, current nozzle design methods are over half a century old, resulting in an increasing disparity between the operational requirements and design characteristics. This discrepancy appears to be caused by a reluctance of the industry to move away from the traditional design methods, and is responsible for high structural loading during engine start-up in addition to a loss of thrust throughout the nozzle of up to 15%.</p>

<p>The use of a finite series of conjugate circular arcs represents a simplified approach to rocket nozzle design, and is aligned with current operational requirements that require a large degree of design control. Previous attempts to evaluate the use of circular arcs in nozzle design have produced mixed results, with the general consensus being that the arc-based method is inferior to established procedures. However, all existing negative results can be traced back to the use of a constrained geometry and greatly simplified analysis methods. Recent increases in available computational resources have led to a comprehensive analysis tool, and it is under this premise that the use of conjugate circular arcs for rocket nozzle design has been reassessed. </p>

<p>A comprehensive design method that is based on conjugate circular arcs has been defined for producing a rocket nozzle contour. The development of a general approach is unprecedented, and the presented arc-based method can naturally extend to any single throat nozzle concept. A numerical model based on the Reynolds-averaged form of the Navier-Stokes equations has been used to compare three existing subscale rocket nozzles to a range of equivalent arc-based designs. Verification and validation of the model was achieved using the experimental pressure distribution and flow field structure of the existing nozzles. Both steady and unsteady simulations were used to evaluate operating thrust to within 0.05% as well as the separation characteristics and implied structural loading in all designs.</p>

<p>The results confirmed that constraining the potential geometric range of the arc-based nozzle will likely produce an inferior design. However, an unconstrained approach indicated that an increase in thrust of the order of 0.1-0.5% is possible using the arc-based design method. In addition, the existing nozzle geometry and performance could be replicated using circular arcs, suggesting that the design parameters are of greater importance than the contour generation method itself. The adverse separation effects responsible for inducing structural loading could be avoided in the arc-based nozzle by utilising a multiple segment approach to the turning curve. An increase in nozzle thrust and favourable separation conditions would be an advantage to any existing nozzle design, and would likely improve both the performance and safety of any equivalent rocket engine.</p>

<p>
</p>
</tr>
</tbody>
</table>
<a name="EI12"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI12: People’s Safety Decision in A Virtual Construction Simulator</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Xueqing Lu</p>
<p>Steven Davis</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>The construction industry generates a high level of noise. Due to health issues, hearing protection devices are used to reduce the noise construction workers are exposed to. However, the effect that construction sound has on workers&rsquo; safety decisions is unknown. This study investigates the effect sound and priming have on people&rsquo;s safety decisions using a virtual construction simulator, in order to improve the design of a virtual reality simulator to be used in training. A virtual reality (VR) simulation is the imitation of a real-world process or system in a virtual environment. It has been considered to assist training in many fields. It is found that the addition of background sound to a high fidelity virtual reality simulator enhances people&rsquo;s sense of presence by creating a high level of normality. More people make safe decisions when background sound is present in the virtual construction simulator, and salient sound is helpful in recognizing the related risks. Safe behaviours learnt in a virtual construction environment can effectively reduce unsafe behaviours in a corresponding physical environment. Results also show that the addition of priming factors to a VR simulator contributes to people&rsquo;s sense of presence in a virtual environment. When people are primed by the concept of safety under a reasonable level of construction noise, they are more likely to perceive more risks and observe the environment carefully. Participants&rsquo; safety behaviours are also explained in psychology.</p>
</tr>
</tbody>
</table>
<a name="EI13"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI13 Multi-Objective Optimisation in Construction</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ahmed Hammad</p>
<p>Ali Akbarnezhad, David Rey, Steven Davis</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Sustainable design and planning in civil and construction engineering has been gaining a growing interest amongst the community over the past decades. In particular, concern is directed at the adverse economic, social and environmental impacts resulting from work activities taking place during the construction process. Ways of mitigating these undesirable effects, in terms of forming and analysing qualitative causal models, have been discussed substantially in the current literature. Alternatively, quantitative approaches relying on mathematical optimisation have emerged as a promising tool for decision-making support in construction management. In this research, mathematical models are proposed to evaluate scenarios wherein several objective functions, addressing economic, social and environmental issues, such as monetary cost and noise, and operational constraints are considered. In particular, we focus on two Location/Layout Problems found in the fields of construction and transportation, namely the Site Layout Problem and the Transit Line Station Location Problem. Mixed Integer Linear and Mixed Integer Non-Linear Programming models are developed to cover the current gaps in the literature, in terms of minimising noise as an objective function and in terms of distance mapping between facilities, especially when obstacles present a hurdle to travel. Building Information Models are utilised to provide estimates of parameters used in the Site Layout Planning models. A decomposition algorithm is proposed and implemented to solve large instances involving a greater number of facilities on the construction site. The instances tested incorporate a discretisation of the available planar space into grids according to the concept of l1 visibility. A comparison is then conducted to assess the computational performance of the decomposition algorithm developed against the Mixed Integer Non-Linear Programming model, formulated originally to solve the problem to global optimality.<br />
</p>
</tr>
</tbody>
</table>
<a name="EI14"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI14: Technology Adoption Decision Making in Construction</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Samad Sepasgozar</p>
<p>Steven Davis</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Due to the complexity, high risk, and conservative character of the construction industry, many technologies do not become widely adopted. Even though vendors make determined efforts to overcome this and disseminate their technologies, the customers&rsquo; decision making processes for adopting construction technologies at the organisational level largely remain unknown. This thesis investigates the extremely complex issues related to the current practices of technology adoption in construction. It tests the hypothesis that construction companies follow a specific logical process linked to need, project objectives, characteristics of the adopting organisation, and the characteristics of the new technology to be adopted. The study explores how construction companies make the decision to uptake a new technology by focusing on customer and vendor activities, their interactions, contributing factors, and people involved in the process.</p>

<p>Over a period of four years, seven technology exhibitions were visited to immerse the author in the customer-vendor market community and collect substantial first-hand data regarding the strategies of the vendors. This was coupled with conducting 147 semi-structured interviews spread across Australia and North America. The credibility of the results is increased by providing detailed descriptions of the process. Finally, key factors and individuals involved in the process were identified and ranked using the Analytical Hierarchy Process.</p>

<p>The major original contributions of this thesis are the Construction Technology Adoption Framework (CTAF) and the recognition of the vendor dissemination strategy spectrum. CTAF is a framework that delineates the stages of the process that customer organisations use when deciding to adopt a new technology and the parallel vendor activities. It is extensively validated by thematic analysis of the interviews and factor analysis. The vendor dissemination strategy spectrum consists of five classes of patterns of vendor activities that relate to the technology type being sold and the size of the vendor. <br />
The thesis offers a framework covering the key factors that vary across this spectrum: Physical appearance, Interpersonal relationship, and Technology demonstration (PIT). It is extensively validated by both fuzzy and hard cluster analysis methods. </p>

<p>The thesis also introduces the Downtime, Interpersonal relationship, and Technology operation quality (DIT) framework, and a study of the customer organisation factors relevant to technology adoption. The DIT framework delineates the implementation factors that are important in making new technology decisions. The customer organisation factors include the personnel roles in the organisation, the effect of organisational structure, and the effect of attitude to technology. These contributions are cross validated between customer and vendor responses and between Australia and North America. </p>

<p>The significance of these contributions is that they enable vendors to understand how to match their dissemination strategies with customer expectations in each stage of the technology adoption process. It also provides a benchmark for new construction companies to use the current best practice in decision making. Understanding the CTAF decision framework also helps industry bodies and government organisations that seek to generally raise the use of new technology in the industry. The theoretical significance is that it unites the vendor perspective with the customer perspective and covers a wide range of construction technologies. The scope of the thesis is limited to technologies in the construction industry in developed nations. Future research is warranted to more clearly delineate any differences with developing nations or related industries.</p>
</tr>
</tbody>
</table>
<a name="EI15"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI15: Estimation of Life Cycle Carbon of Residential and Office Building</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Zahra Moussavi Nadoushani</p>
<p>Dr. Ali Akbarnezhad, Assoc. Professor Tommy Wiedmann</p>
<p>Civil & Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Abstract<br />
The building and construction industry contributes up to 30% of global annual greenhouse gas emissions. Such emissions occur in all phases of building life cycle including material extraction and processing, transportation, construction, building operation and end of life phase. While a number of studies have been conducted previously on estimating the embodied carbon and carbon emissions incurred in operation phase of buildings, little has been done to investigate the contribution of other phases to the life cycle carbon. On the other hand, the emissions and energy use incurred in every phase of a construction project are highly affected by the decisions made in other phases. Therefore, any decision made with regards to the types of materials, architectural and structural designs, construction methods and deconstruction strategies should account for the effects of such choices on the life cycle carbon emissions of the building.<br />
This study is broken down into several parts, such that each part illustrates the importance of decisions made in each phases on that individual phase as well as on the overall life cycle. Construction and end of life phases were identified as two major overlooked phases in previous studies. Thus in the first part of this research a computational method was developed to identify the optimal concrete recycling method by considering the trade-off between costs, energy use and carbon emissions of concrete recycling in a particular project. Following that a comprehensive carbon emission prediction framework is presented to simulate the three main construction activities that have the highest contribution to the overall construction carbon emissions i.e. earthmoving, placing concrete and on-site transportation by tower cranes. Using quantity take-offs from BIM, information obtained from work break down structure (WBS), carbon inventories and discrete event simulation (DES), the proposed framework predicts the integrated performance of cost, productivity and carbon emission in the planning stage. This framework would be beneficial for evaluating the influence of different construction strategies, thereby supporting decision-makers in selecting more effective construction operations in terms of reducing the carbon emissions without detracting from project productivity and final costs.<br />
Finally, to illustrate the impact of a decision on the overall life cycle of a building, the selection of the structural system for buildings was chosen and investigated in this study. As expected considerable differences between the life cycle carbon of different structural systems were observed. The outcome of this study could also contribute to achieving a basic understanding of the effect of various design parameters on the life cycle carbon of buildings</p>
</tr>
</tbody>
</table>
<a name="EI16"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI16: Cascaded H-Bridge Converter Based STATCOM System Operating Under Extreme Low-Frequency Voltage Ripple on Capacitors</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ghiaz Farivar</p>
<p>Prof. Vassilios G Agelidis, Branislav Dr. Hredzak</p>
<p>	Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>Recently, the high reliability of film capacitors became a major driving force to replace the electrolytic type capacitors in converters. However, relatively low capacitance of the film capacitors, compared to that of the electrolytic types at the same volume, results in a large low-frequency voltage ripple on the capacitors and hence poses many technical challenges. Reducing the capacitors&rsquo; capacitance results in increased low frequency voltage ripple on the capacitors which, in turn, affects performance of the control system, increases semiconductor&rsquo;s stress and deteriorates current quality. Therefore, a fast control system has to be designed which can maintain the thin capacitors&rsquo; voltage, be immune to disturbances due to presence of the low frequency voltage ripple on the capacitors, compensate for the adverse effects of the capacitors voltage ripple on the grid current, and limit the maximum and minimum allowed capacitor voltages within the stable operating region of the system. In this project we aimed to identify the effects of the large, low-frequency voltage ripple on operation of a CHB converter based STATCOM. An advanced control system is designed to operate the CHB converter based STATCOM close to the theoretically highest possible voltage ripple and hence with the smallest possible capacitance. This will not only increase reliability of the converter but also reduce its cost. The developed Thin-Capacitance STATCOM (TC-STATCOM) system achieved more than 50% reduction in size of the reactive components compared to current systems available in the market. Reduced voltage stress on the semiconductor devices reduced the switching losses significantly and hence increased the efficiency (close to 40%). All these benefits were achieved by sacrificing some of the operating region on the conventional system as well as using the developed advanced control system. Therefore, the proposed STACOM concept has a different operating region characteristic than the conventional STACOM system which makes it a new FACTS device i.e TC-STATCOM. A small experimental prototype system is constructed to verify the proposed TC-STATCOM system. <br />
</p>
</tr>
</tbody>
</table>
<a name="EI17"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI17: Investigating Effects of p-n Junction Geometry in Silicon Solar Cells</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Xinrui An</p>
<p>Prof. Allen Barnett</p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>Introductory summary</p>

<p>A new, high efficiency silicon solar cell, with use of limited p-n junction area in the emitter, has been designed and is being developed. . Simplified test structures on planar wafers with a front metal grid are fabricated to demonstrate the potential of such a design to enhance the open circuit voltage (Voc) above the limit of a full-area emitter solar cell. Characterisation techniques have been used to confirm lower dark saturation current contribution from the emitter (I0e) in the test structures than in their full-area emitter counterparts. The finished solar cells, though suffering relatively large Voc losses caused by surface recombination at the p-type Si/SiO2 interface, have demonstrated the potential of limited junction area as a practical means to increase Voc well above 700 mV with better p-type surface passivation schemes than have been.</p>

<p>Purpose of my thesis: To demonstrate the dependence of Voc on junction area and geometry for high efficiency silicon solar cells with defined emitter and contact areas as well as advanced passivation schemes for both n- and p-type surfaces. Voc is then compared against that of a full-area emitter solar cell with identical rear structures to illustrate the effect of reduced junction area. From various test structures, the dark saturation components in different regions of the cell are to be separately quantified so that, on a cell level, Voc can be predicted. Secondary effects of varying the p-n junction geometry on the cell short circuit current (Isc) and fill factor (FF) are also to be investigated.</p>

<p>Approach: Test structures on float-zone (FZ) substrates, as shown in Figure 1, are fabricated. Implied Voc is measured with quasi-steady-state photo-conductance (QSSPC) method at various stages in fabrication and then compared to that of a standard passivated emitter and rear locally-diffused (PERL) cell. To define emitter regions on the substrate, a pre-grown oxide is applied to the front surface followed by photolithographic processes during which emitter areas are opened for the subsequent phosphorus diffusion. The rear Al contact is fabricated using the processes for standard PERL cells. In addition, photoluminescence (PL) is used to monitor the uniformity of diffusion and other fabrication.<br />
 </p>

<p>Scientific Innovation and Relevance: Voc is arguably the most important output parameter for a solar cell. In parallel with most approaches on minimising surface recombination, this work focuses on reducing I0e with the use of limited junction area, which in theory would maintain high minority carrier collection probabilities and, thus, efficiency, given well passivated surfaces and a long electron diffusion length in the bulk region. With little recombination at surfaces and contact regions, the limited junction area approach has the potential to significantly increase Voc on a well-designed cell structure.</p>

<p>Results and Conclusion: QSSPC and PL measurements have confirmed lower I0e in the limited junction test structures than their full-area emitter counterparts. On the cell level, though suffering Voc losses caused by surface recombination at the p-type Si/SiO2 interface, simulation has shown that Voc well above 700 mV is achievable with high quality p-type surface passivation reported by various groups.</p>
</tr>
</tbody>
</table>
<a name="EI18"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI18: A Real Time Temperature Monitoring and Diagnostic System for a Forced Cooled Supercapacitors String</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Vita Lystianingrum</p>
<p>Prof. Vassilios G. Agelidis</p>
<p>Electrical Engineering & Telecommunications</td>
</tr>
</tbody>
</table>
<p>The need for continuous temperature monitoring and thermal management is unquestionable as operating temperature is a critical factor affecting many electrical/electronic components and devices, including energy storage devices such as batteries and supercapacitors (SCs). On the other hand, many applications require the battery or SC cells to be stacked up to meet the required voltage or power level, which introduces uneven temperature distribution among the cells. Hence, thermal model based temperature monitoring and management has been widely utilised, using state estimation to allow the use of the minimum number of temperature sensors. </p>

<p>While the lumped thermal model for a single cylindrical battery or SC cell and its extension to multicells, string or module have been covered in technical literature, the multicells model has not been experimentally validated. In this work, the experimental validation of the existing lumped thermal model of multi cylindrical cells, based on an experimental setup of a forced cooled SC string consisting of eight 650 F, cells is presented. The determination of the parameters of such thermal model is discussed and the validated model is used as a basis to design a closed loop observer for estimating string temperature. Experimental results are presented to confirm its applicability for predicting instantaneous temperatures of each cell in the string. </p>

<p>The optimal number and location of thermal sensors in a forced-cooled string is also discussed in this work, based on the observability analysis. Different Gramian-based observability criteria are used to evaluate and compare observability degree of placement of different numbers and different combinations of sensor locations in the string. The observability degree is further verified by evaluating the estimator performance for the different sensor locations. The most suitable observability criteria to ascertain the optimal location of such thermal sensors are recommended.</p>

<p>Furthermore, a method based on multiple model estimator (MME) is proposed for an abnormal over-heating detection scheme in a supercapacitor string with a minimum number of sensors. The performance of the estimator/detector is evaluated based on simulations and experiments. Preliminary indication from the experimental results shows satisfactory performance of the estimator/detector system.</p>
</tr>
</tbody>
</table>
<a name="EI19"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI19: High Efficiency Tandem Solar Cell on a Silicon Substrate: Optical Analysis, Design, and Performance Improvements</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Brianna Conrad</p>
<p>Allen Barnett, Ivan Perez-Wurfl</p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>Solar photovoltaics is becoming a major energy source, with exponential growth driven by a combination of falling production costs and rising efficiencies. Low cost, good energy conversion efficiency, material abundance, and mature technology have led silicon solar cells to dominate the market. However, the highest efficiencies are being achieved by more expensive, multijunction devices consisting of several stacked cells and based around III-V materials such as GaAs and GaInP. If high efficiencies can be achieved at lower cost, further reductions in the price of solar generated electricity will open new applications and fuel additional growth. We are combining the benefits of these technologies by developing multijunction devices with thin III-V layers on Si substrates to improve on the record efficiencies of Si photovoltaics.<br />
 Adding III-V materials to a Si cell is made difficult by mismatch in the lattice dimensions between the two materials, which causes defects to form in the crystal structure. This work focuses on a two-junction (&ldquo;tandem&rdquo;) device with GaAsP and SiGe cells. The materials have been chosen to produce the highest possible efficiency while being lattice-matched. A compositionally graded buffer allows growth of the SiGe cell on the Si substrate with high material quality in both solar cells.<br />
 The use of new material compositions makes it necessary to determine optical and electrical properties of these materials before refining the device design for the highest possible efficiency. Analysis of spectroscopic ellipsometry measurements taken on specially prepared samples is used to determine the complex index of refraction of the materials. The results show that, although usually considered lost, some of the light absorbed in the passivating window&mdash;a thin top layer necessary for high performance&mdash;contributes to power generation. They also allow the optimization of the window's thickness in conjunction with the design of an antireflection coating, substantially increasing the GaAsP cell's current. The addition of device layouts balancing losses due to shading and resistance makes the potential efficiency of a tandem device grown on the best SiGe single-junctions to date 29% at high concentration.<br />
 Further improvements are possible from refining the device's vertical structure and the fabrication process. To achieve this, recombination rates within the device, which cause current and voltage losses when they are high, must be determined. This is done by fitting a model of spectral response from these parameters and the absorption coefficients to experimental results from full and partial devices. The extracted parameters are used to model alternative device designs and optimize materials, layer thicknesses and surface treatments. With initial changes to the device design, tandem efficiency is increased for devices at both low and high concentration. <br />
 In the long term, the GaAsP/SiGe tandem has the potential to achieve 32% efficiency with cheap and abundant silicon as the bulk of the material consumed. Furthermore, high performance over a range of incident angles and the lightweight nature of silicon mean that, as efficiencies rise beyond those of silicon cells, potential applications expand to include both low-level concentration systems and space applications.</p>
</tr>
</tbody>
</table>
<a name="EI20"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI20: Study of Beam Splitting Optical Filters Applied to Solar Hybrid PV/T Collectors</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Jose Crisostomo Munoz</p>
<p>Dr. Robert A Taylor</p>
<p>Mechanical & Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>Efficient utilization of the solar spectrum has been intensively addressed by solar researchers in recent decades. For instance, many efforts have been made to improve PV cell efficiency and to allow their operation at high concentration ratios (CRs). As an alternative to generating electricity, solar thermal collectors can achieve efficient utilization and a wide range of operational temperatures. Hybrid photovoltaic/thermal (PV/T) collectors using beam splitting represents a mutually beneficial solution whereby photovoltaic cells can be thermally decoupled from the thermal receiver allowing it to operate at high temperatures while the PV receiver is illuminated only with the solar spectrum region that matches well with its spectral response. </p>

<p>In this research, a general methodology for the optimal spectral splitting parameters in hybrid PV/T collectors is developed. The methodology proposes an objective function which corresponds to the ratio between the weighted sum of the electrical and heat outputs in the hybrid configuration to the electrical output from a stand-alone concentrated photovoltaic (CPV) system (of the same PV cell type) under the same illumination. The results reveal that Si PV cells are the best candidate; with around 45% more weighted power delivery in comparison with a Si CPV (@ 10 suns). </p>

<p>To split the light for Si PV, two different approaches have been investigated to achieve the optimal spectrum partitioning (e.g. transmission from 732 to 1067 nm): (1) dichroic mirrors as reflective band-stop filters and, (2) nanofluids (NFs) as band-pass filters. The design of (1) was addressed by using two different multilayers structures: SiNx/SiO2 and TiO2/SiO2. For (2), Ag/Si core/shell nano-discs were suspended in water creating a spectrally selective absorbing fluid. </p>

<p>SiNx/SiO2 filters were tested in an indoor experiment revealing that the PV cells illuminated with the reflected light from the filters operate on average at 9.2% absolute higher efficiency than the same cells without the filter. Furthermore, for the best filter, the measured hybrid output is ~9% (relative) higher than the electrical output of a PV cell exposed to the same light source without beam splitting. TiO2/SiO2 filters were tested under sunlight with the main goal to investigate them as an add-in for a solar thermal rooftop concentrating collector. The results indicate that only for low temperatures (<50&deg;C) the hybrid configuration delivers more power and performs with greater efficiencies than in the original collector. </p>

<p>A detailed optical/heat transfer model was developed to estimate the hybrid output with different NFs and CRs. Moreover, an experimental set-up was designed and built to validate the model. The results with water as the heat transfer fluid match well with the model where stagnation temperatures up to 80&deg;C were obtained. The testing with NFs is being currently conducted.</p>

<p>Overall, according to the temperature required in the thermal receiver, these methods could be promising approaches for harvesting solar energy especially in unused rooftop space for the demand of electricity and heat in buildings</p>
</tr>
</tbody>
</table>
<a name="EI21"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI21: Ultra Thin Silicon Solar Cells</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Jianshu Han</p>
<p>Prof. Allen Barnett</p>
<p>Photovoltaic & Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>Solar energy has become a key player in the global energy market. Conventional solar cells are 150-200 microns thick. Theoretically reduction in solar cell thickness brings many benefits, such as:<br />
&bull; Lower Si cost<br />
&bull; Higher output voltage<br />
&bull; Flexible module</p>

<p>However the technology is challenging because:<br />
&bull; Thinner device => less light absorption => less output current<br />
&bull; Cell becomes increasingly fragile</p>

<p>The ultra-thin silicon solar cell (UTSi Cell) is engineered to reap all the benefits of a thin device, and to overcome its inherent problems. The silicon layer (or active device) of an UTSi cell is only 20 microns thick. This is a high quality monocrystalline layer grown by an epitaxial process. With all the necessary solar cell components, such as front and rear contact and anti-reflection coating, this layer functions the same in every way that a more conventional cell does, but is 8-10 times thinner. This 20 micron device is bonded to a 100 micron thick steel sheet, which provides both flexibility and robustness better than most wafer based solar cells.</p>

<p>My thesis focuses on the fabrication and analysis of the UTSi cell. Our cell design incorporates several advanced fabrication technologies such as high quality SiNx passivation, laser doping and light induced plating. The best device fabricated to date is 90cm2 in size with 15.9% efficiency. 3D modelling has shown that this structure could reach 18.4% efficiency in the near future, and is capable of 22-23% efficiency in the long term.<br />
</p>
</tr>
</tbody>
</table>
<a name="FS18"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS18: Energy Storage and Return Prostheses: Developing a Biomechanical Model of Amputee Sprinting</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Stacey Rigney</p>
<p>Lauren Kark, Anne Simmons</p>
<p>Graduate School of Biomedical Engineering</td>
</tr>
</tbody>
</table>
<p>Lower-limb energy storage and return (ESAR) prostheses are constructed of carbon-fibre composite to enable energy storage and return during the stance phase of dynamic activities just as running and jumping. It has been established that the highly elastic properties of carbon-fibre allow an approximate replication of the shock absorbing nature of the muscles and joints of an able-bodied runner. However, the impact on gait of the elastic behaviour of ESAR prostheses is not well documented. This is because conventional gait analysis relies upon rigid-body mechanics to describe the kinetics of limbs and joints, and ESAR prostheses adhere poorly to the underlying assumptions. This thesis aimed to develop alternative modelling techniques to overcome the limitations of current methods. Three methods were developed: finite element analysis, multiple-link segmentation and phenomenological modelling. Each new model was compared to a conventional two-link segment model using gait data from elite lower-limb amputee sprinters. Finite Element Analysis (FEA) provides a process for characterisation of the prosthesis deformation under load and ground reaction forces. Numerical models of a range of commercially available ESAR prostheses were developed from in vitro mechanical testing and conventional gait analysis. Residual limb displacement data from gait analysis was used as boundary conditions in dynamic in vivo simulations to predict ground reaction forces as well as residual limb reaction forces. Based on these simulations, a general phenomenological model of ESAR prosthesis behaviour was the developed and incorporated into the Hill-type model software OpenSim as a new method for the analysis of amputee running and sprinting. A multiple-link segment model was also developed and compared with the new phenomenological and conventional two-link segment models. The newly developed methodologies allow for the analysis of amputee running and sprinting without using the existing shank-ankle-foot rigid-body mechanics model to describe prosthesis behaviour. Consequently, the models enable the calculation of variables not available through traditional methods alone, such as residual limb impact forces and moments. Additionally, the lower limb kinematics and kinetics calculated using the new models were significantly different (p &#60; 0.005) than that obtained via conventional gait analysis, highlighting the possibility that current methods mask the true function of RSPs. This research has the potential to optimise sporting technique as well as providing more accurate data for health care professionals to develop more efficient and effective leg prosthesis designs, improving athletic performance and well-being of amputee runners. </p>
</tr>
</tbody>
</table>
<a name="FS19"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS19: Convergence of Smartphone Technology and Algorithms that Estimate Physical Movement for Telehealth Assisted Rehabilitation</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Michael Del Rosario</p>
<p>Dr. Stephen Redmond, Co-Supervisor: Prof. Nigel Lovell</p>
<p>Graduate School of Biomedical Engineering</td>
</tr>
</tbody>
</table>
<p>The ubiquity of smartphones makes them a suitable platform for deploying applications that can positively impact healthcare. Signals from the smartphone&rsquo;s sensors (i.e. the magnetic and inertial measurement unit (MIMU) as well as the barometric pressure sensor) can be analysed by models developed with &lsquo;state-of-the-art&rsquo; machine learning algorithms to estimate physical movement whilst the device is located on the body. Since the orientation of the smartphone can change relative to the body, incorporating an attitude and heading reference system (AHRS) into pre-existing algorithms is proposed. This would estimate the orientation of the device and allow measurements from the smartphone&rsquo;s sensors to be expressed in the &lsquo;global frame of reference&rsquo; reducing the variability in the measurements and leading to more robust classification. Health peripherals (i.e. blood pressure monitor and weight scale) that seamlessly interface with AndroidTM smartphones via Near-Field-Communication enable patients to collect longitudinal health data on the smartphone; whilst the user interface can be leveraged to administer questionnaires designed specifically to gauge the psychological state of patients. These features can engage patients between scheduled rehabilitation sessions and encourage patients to complete the rehabilitation program. First-time participants in an outpatient cardiac rehabilitation program will be recruited to a randomised control trial (at Prince of Wales Hospital, Randwick) which will investigate if such an intervention can increase the retention rate of outpatients. Furthermore it may provide healthcare professionals with the information they need to determine if a patient&rsquo;s rehabilitation is contributing to the amount of daily physical activity observed and identify patients who are at risk of dropping out from the rehabilitation program based on daily estimates of their physical activity level.</p>
</tr>
</tbody>
</table>
<a name="FS20"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS20: Striding Towards Fewer Falls: Using Inertial Sensors to Analyse Gait and Estimate Fall Risk in Older Adults</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Kejia Wang</p>
<p>Prof. Nigel Lovell, Dr. Stephen Redmond, Dr. Lauren Kark</p>
<p>Graduate School of Biomedical Engineering</td>
</tr>
</tbody>
</table>
<p>Falls are a prominent cause of injuries and injury-related deaths for today&rsquo;s growing population of older adults, affecting a third of community dwelling residents at least once every year, and turning even the most ordinary of everyday environments into a hazard. Negative psychological effects and significant financial costs can also result. Accurately identifying individuals at high risk of falling may lead to more timely and tailored intervention for fall prevention. </p>

<p>Fall risk assessments today are usually conducted in clinical settings, employing often qualitative and/or subjective techniques to assess mobility and gait quality. Their limitations include restricted testing space, assessor subjectivity, bulky equipment and the white-coat effect. Lightweight, affordable magnetic and inertial measurement units (MIMUs) can help address these limitations by allowing activity-based assessments of health and fall risk to be conducted quantitatively, outside of the clinical space. This may provide a more accurate assessment of a person&rsquo;s typical gait in their natural environment and contribute to a better estimate of their fall risk. It is also worth quantifying gait on different walking terrains, which is lacking in the literature and is valuable, since daily life can present much more varied and challenging terrains.</p>

<p>We analysed the gait patterns of 96 older adults (age 84.1&plusmn;3.9 years, 50 male) who completed various semi-unsupervised free-living activities, including walking totalling 290 m, stair ascent, and stair descent, broken up between other daily activities and rests, at a self-selected speed, while allowing stops, minor path alterations, and the use of handrails and walking aids. They wore two Opal (APDM, Portland, OR) MIMU sensors, at the centre of their lower back and on the right ankle. These contained a triaxial accelerometer (&plusmn;6 g), a triaxial gyroscope (&plusmn;2000 rad/s) and a triaxial magnetometer (&plusmn;6 gauss), all sampled at 128 Hz. </p>

<p>Two approaches were employed to investigate whether gait parameters from the aforementioned sensor locations and activities can predict falls. In the first approach, measures of gait variability, step rate, and vigour, during gait on flat terrain, stair ascent and stair descent were tested for correlations with prospective falls, fall history, and physiological fall-related factors. These gait parameters were derived from the accelerations measured in each sensor&rsquo;s local frame of reference, with no correction for orientation relative to the body. In the second approach, automatic step detection and an attitude heading and reference system algorithm were used on the accelerometer, gyroscope and magnetometer data from the ankle sensor, to calculate the 3D trajectory of the ankle through space, in the global frame of reference. Spatiotemporal gait parameters such as stride length and height were calculated from these trajectories, and their correlations with falls and fall risk factors (as above) were similarly investigated. Preliminary results suggest that gait quality on stairs are stronger estimators of fall risk factors than gait on flat terrain, and that single gait variables are overall insufficient for predicting fall incidents, but may be useful to help characterise gait patterns that are suggestive of greater clinical fall risk. </p>
</tr>
</tbody>
</table>
<a name="FS21"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS21: Speech Affected by Depression: Paralinguistic Analysis, Modelling and Automatic Assessment</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Nicholas Cummins</p>
<p>Julien Epps</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Clinical depression is one of the most common illnesses in Australia; it is currently the fourth most frequently managed health problem in general practise. Despite this prevalence, diagnosis of depression due to a wide ranging set of associated medical symptoms is time consuming and subjective. An objective, automatic and low cost diagnostic tool is badly needed. Speech is a highly complex cognitive and muscular act but it does offer advantages as a diagnostic tool in that it is able to be measured cheaply, remotely, non-invasively and non-intrusively. My research demonstrates that speech affected by depression can be characterised by a combination of three effects; (i) a reduction in variability in speaking effort (ii) a lack of animation and (iii) a reduction in the number of distinct speech sounds produced. Further, my research also presents a novel 2-stage regression framework which exploits these changes to automatically predict a speaker&rsquo;s level of clinical prediction. Ultimately this fascinating area of engineering research can lead to simple and objective diagnostic aid for depression, helping reduce the large socio-economic costs currently associated with mental illness in Australia: imagine the benefits of a smartphone application that provides immediate feedback and therapeutic advice </p>
</tr>
</tbody>
</table>
<a name="FS22"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS22: Thin Film Thermal Conductivity Measurement and Modelling</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Alireza Moridi Farimani</p>
<p>Prof. Liangchi Zhang</p>
<p>Mechanical &amp; Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>Silicon on Insulator (SOI) is considered a feasible technology to extend Moore&rsquo;s law. Due to isolation from the silicon layer, SOIs have lower parasitic capacitance. This results in power consumption improvement and consequently higher performance can be achieved. The common dielectric insulating layers used in semiconductor manufacturing are silicon oxide (SiO2) or aluminium oxide (Al2O3). These insulating layers, although functioning as good dielectrics, may also shield the heat induced by the self-heating of the electronic devices and reduce the chip&rsquo;s performance. Aluminium Nitride (AlN) is a dielectric material which is proposed as the insulating layer in SOIs and it has comparable thermal conductivities to bulk silicon. However, the resulting temperature rise is still much more significant than those from theoretical calculations or from using bulk silicon. The primary reason for this could be the amorphous structure of the deposited AlN layer or it could be due to the interface thermal resistance.</p>

<p>In this research, a single crystal AlN thin film is first deposited on silicon substrate and the thermal conductivity of the thin film is measured by the modified 3&#969; method. The challenge is to characterize the extremely thin film which has a very high thermal conductivity property. In order to do this, a nano-strip is deposited on top of the specimen by electron-beam lithography (EBL) and physical vapour deposition (PVD) methods. Then, utilizing the 3&#969; method, the thermal conductivity of the specimen is measured.</p>

<p>Another issue in SOI semiconductor manufacturing is the interface thermal resistance. In order to be able to improve the interface, a method should first be developed to measure and characterise the interface thermal resistivity. The interface thermal resistance is characterized and measured by comparing the experiment and analytical models. The analytical and numerical models have been created by MATLAB and the finite element method (FEM) by ABAQUS. The results are verified by multi-stacks of AlN and silicon layers. Three and five stacks are measured and the interface thermal conductivity is characterized. </p>

<p>Overall, novel techniques to characterize the thermal conductivity of extremely thin films with very high thermal conductivities such as aluminium nitride or silicon films are developed in this study. The method is verified by analytical and numerical calculations.  </p>
</tr>
</tbody>
</table>
<a name="FS23"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS23: A Direct Numerical Simulation Study of a Turbulent Non-premixed Lifted Flame</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Shahram Karami</p>
<p>Associate Professor Evatt Hawkes</p>
<p>Photovoltaic &amp; Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>A turbulent lifted slot-jet flame is studied using direct numerical simulation (DNS). A one- step chemistry model is employed with a mixture-fraction dependent activation energy which can reproduce qualitatively the dependence of laminar burning rate on equivalence ratio that is typical of hydrocarbon fuels.</p>

<p>The qualitative structure of the flame is first examined, confirming some features that have previously been observed in experimental measurements as well as some which have not been previously discussed. Significant differences are observed comparing the present DNS representing a hydrocarbon fuel, and previous DNS representing hydrogen fuel. The statistics of flow and relative edge-flame propagation velocity components conditioned on the leading edge-flame locations are then examined. The results show that, on average, the streamwise flame propagation and streamwise flow balance, thus demonstrating that edge-flame propagation is the basic stabilisation mechanism. Fluctuations of the edge locations and velocities are, however, significant. It is demonstrated that the edges tend to move in an essentially two-dimensional elliptical pattern (laterally outwards towards the oxidiser, then upstream, then inwards towards the fuel, then downstream again). It is proposed that this is due to the passage of large eddies, as outlined in Su et al. [1]. However, the mechanism is not entirely two-dimensional, and out-of-plane motion is needed to explain how flames escape the high velocity inner region of the jet.</p>

<p>Next, the time-averaged structure is examined. The entrainment flow is shown to be diverted around the flame base causing locally upstream streamwise velocities. A budget of terms in the transport equation for product mass fraction is used to understand the stabilisation from a time-averaged perspective. It is found to be consistent with the instantaneous perspective, featuring a fundamentally two-dimensional structure involving upstream transport of products on the lean side, balanced by entrainment into richer conditions, while on the rich side, upstream turbulent transport and entrainment from leaner conditions balance the streamwise convection.</p>

<p>A complete analysis of the reasons behind the observed trends in the flame relative propagation velocity has been performed. The mean normalised edge-flame speed is less than laminar flame speed (at around 0.6 of laminar flame speed) and the edge-flame velocity fluctuations are mainly connected with strain rates, scalar dissipation rate, mixture-fraction curvature, product mass fraction curvature and the inner product . These quantities, as well as the average normal orientations and nature of the flame in terms of categorisation of the edge as premixed or non-premixed, go through cyclic fluctuations which appear to be connected with the passage of large eddies and the elliptical pattern of the on-average motion. Overall, the results provide strong support for the edge-flame theory of flame stabilisation, but point to significant roles played by large, coherent eddies in determining fluctuations of both the flow velocities and edge-flame relative propagation velocities, and thus the lifted height. </p>
</tr>
</tbody>
</table>
<a name="DF13"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF13: Distributed Model Predictive Control based on Dissipativity Theory</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chaoxu Zheng</p>
<p>Prof. Jie Bao</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Process control design in chemical engineering plays an important role, because of providing strategies or decisions to plant operation, wherein the challenge is how to tightly connect design of a chemical system and its control system. High integrity provides effective and efficient operation to a chemical plant, such as reducing useless by-product and recycling materials/energy. The possible integrity design can be obtained energy recovery network, network of process system. However, the number of operation units may increase complexity of a plantwide system. </p>

<p>In this research project, we develop a dissipativity-based framework for the analysis and control design of plantwide systems. In our approach, the analysis can be divided into two aspects. One is studying the plantwide system from network perspective, wherein the dissipativity theory is used to study the plantwide system as a combination of subsystem. The other one is design control based on model prediction (model predictive control MPC), which is well-known and successful industrial control scheme.</p>

<p>Dissipativity theory studies the dynamics characteristics of dynamical systems (such as electrical system, mechanical system, chemical system, quantum system or even mathematical systems) from input-output perspective. Simply speaking, the analysis &ldquo;summarises&rdquo; the behaviours of the system. For example, water evaporates at 100 oC, it wouldn&rsquo;t turn to ice at this degree. In our approach, dissipativity theory extracts the dynamics characteristics from the model of the system. Interestingly, from network perspective, a complex system can be studied by the analysis of the combination of its subsystems due to their input-output properties. This analysis can be easily achieved by using dissipativity theory, herein the dissipativity condition can be constructed follows the structure of complex system. </p>

<p>Model predictive control (MPC) is one advanced industrial optimal control scheme developed during 1960s', because of naturally handling constraints and tackling multivariable process. Industrial MPC implementation has been widely accepted in process industrial, such as refining, petrochemicals, chemicals production, mining, polymer and others. MPC is control strategy based on the idea of model prediction, where the system response can be estimated or calculated by using current information and the given model. MPC controller determine the control action by solving the constrained optimization, therein the system must be constrained by its model. Also, the process constraints can be naturally included in solving constrained optimization.</p>
</tr>
</tbody>
</table>
<a name="DF14"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF14: Personalize Air Pollution Exposure Using Wireless Sensor Network</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ke Hu</p>
<p>A/Prof.Vijay Sivaraman</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Urban air pollution is believed to be a major contributor to premature deaths and chronic illnesses worldwide. Current systems for urban air pollution monitoring rely on static sites with low spatial resolution, and moreover lack the means to estimate exposures for (potentially mobile) individuals in order to make medical inferences. My research firstly focuses on the design and evaluation of a low-cost participatory sensing system called HazeWatch that uses a combination of portable mobile sensor units, smart-phones, cloud computing, and mobile apps to measure, model, and personalize air pollution information for individuals. Secondly I combine air pollution and human energy expenditure data to give individuals real-time personal air pollution exposure estimates. In particular, I developed a personal air pollution exposure estimation system utilising the participatory air pollution monitoring system and energy expenditure data collected from wearable activity sensors. This system and applications will benefit the understanding of the relationship between air pollution exposure and personal health. Thirdly, to help individuals to manage their exposure, I developed a model which uses existing air pollution data from fixed sites and sensor network to estimate air pollution surface in Sydney for a whole year, then developed personalized services like an alternative route planning service for the public to reduce their exposure.  I believe my research can increase user engagement in exposure management, and better inform medical studies linking air pollution with human health.</p>
</tr>
</tbody>
</table>
<a name="DF15"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF15: Robust Fast Control of Vectored Thrust Aerial Vehicles via Variable Structure Methods</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Hiranya Jayakody</p>
<p>A/Prof. Jay Katupitiya</p>
<p>Mechanical &amp; Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>The popularity of Unmanned Aerial Vehicles (UAVs) has grown rapidly over the past several decades in many civil and military applications. Recent roles of UAVs include applications such as crop monitoring in broad acre farming, terrain mapping, aerial photography and videography, and monitoring of large structures, where one or several image sensors attached to the UAV platform collect and process thermal, visual and laser data to provide valuable information on the objects and ground below. A thrust vectoring aerial vehicle, a vehicle with the ability to change the direction of thrust generated while keeping the UAV body at a zero roll and pitch orientation, can serve well in such applications where the zero-roll-pitch vehicle body allows the sensors to capture stable image data without the aid of a gimbal, reducing the payload and cost as a result. The first part of this research focuses on developing a comprehensive dynamic model and a low level altitude and position control structure for a tri-rotor UAV with thrust vectoring capability, namely the Vectored Thrust Aerial Vehicle (VTAV).</p>

<p>Nonlinear unstable dynamics of UAVs require robust control methods to realize reliable stable flight. Special attention needs to be paid to the disturbances caused by sudden wind gusts during UAV flight, as well as to the uncertainty effects caused by complex aerodynamic parameters of the platform. Sliding Mode Control (SMC), a type of nonlinear Variable Structure Control (VSC) approach, has served well over the years in controlling UAVs and other complex dynamic systems while handling disturbances and uncertainties in a reliable manner. However, SMC inherently does not focus on fast set-point regulation or tracking, which is a desirable characteristic for UAVs as well as other robotic systems in general.</p>

<p>Taking this research gap into account, this work presents a novel adaptive control method which has the ability to produce fast set-point regulation control while maintaining robustness against external disturbances and parametric uncertainties. The controller is then applied to stabilize and navigate the VTAV. Both simulation and experimental results are provided to demonstrate the superiority of the proposed approach compared to SMC. The novel adaptive algorithm is applicable to many dynamic systems including inverted pendulum systems and robot arm manipulators. The same adaptive algorithm is later extended to optimize an existing second order SMC method named the sub-optimal algorithm. Simulation results are provided to present the performance of the proposed Adaptive Second Order SMC algorithm.</p>
</tr>
</tbody>
</table>
<a name="DF16"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF16: Hybrid Dual-Mode Low Voltage Dropout Regulator with Infinite Impulse Response Digital Filters</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>David Phoon</p>
<p>Professor Torsten Lehmann (Supervisor 1) and Tara Hamilton  (Supervisor 2)</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>As Complimentary Metal Oxide Semiconductor (CMOS) technology gets more sophisticated, the gain performance from such CMOS transistors drops. The poorer gain of such more sophisticated CMOS processes prevents the LDO error signal of the comparator from approaching the reference voltage. Most publications provided solutions to the trade-offs inherent in the model LDOs which require low dropout voltage, high output current, low no-load quiescent current and small output transient undershoots and overshoots. Studies of various methods to address trade-offs have been conducted and include pole-zero generation, analog and/or digital dynamic scaling, and pole splitting to cater for analog design limitations. None of these solutions for analog LDOs would adequately address the impending reduction in gain performance and consequently regulation of the LDO in the more sophisticated CMOS technologies. Our design overcomes the comparator gain limitation of more sophisticated CMOS processes by the use of a clocked comparator and the signal conversion from continuous to discrete time of the clocked comparator, which is sampled and integrated by the digital filter.</p>

<p>Furthermore while the area of analog LDOs has been extensively researched, there is significantly less research that relates to full digital LDOs or LDOs that incorporate digital components (&ldquo;Hybrid LDO&rdquo;) such as the one proposed. Digital LDOs can operate at significantly lower voltages than analog LDOs. Where required, the whole or part of an LDO could be replaced with digital circuits or devices benefiting from improvement in efficiency and reduced quiescent current demand. </p>

<p>Worst case regulation occurs when the LDO has lowest load current since the pole affecting the circuit is moved to the lower frequencies and affects the stability of the LDO. At higher load current, this pole is shifted to higher frequencies with greater phase margin. Hence, high load current, with lower efficiency but greater stability, is traded off against low load current with high efficiency but poor stability, in LDO design. However, such analog designs and hence solutions are unfortunately tightly coupled with the implementing CMOS technology. The pole locations of the integrators in analog designs do not scale well with frequency as much as in discrete-time design. </p>

<p>An LDO for regulation of a serialization/deserialization circuit in 28 nm TSMC CMOS technology, which cycles between low and high current loads, is proposed. We aim to show how the use of an a priori signal solves the trade off between regulation/stability and efficiency that prevents LDOs from getting a good transient response for the very wide dynamic range required for the proposed and other applications. This a priori signal will switch the LDO to utilize the optimal digital filter for the designed current load. This overcomes the issue of predicting the wide dynamic range of pole location, should the current be unconstrained and potentially leading to loop instability if not addressed. Since we can design the location of the load pole, we can have tighter control on the stability of the LDO, with a scalable design.</p>
</tr>
</tbody>
</table>
<a name="DF17"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF17: Path Tracking of Agricultural Vehicles</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Javad Taghia</p>
<p>Associate Professor Jayantha Katupitiya</p>
<p>Mechanical &amp; Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>In this research, path following control of autonomous farm vehicles is considered. Controllers are designed and implemented to provide generic solutions for agricultural vehicles in typical farm situations and for various types of vehicles. The study begins with modelling of the vehicles, continues with derivation of the offset models and control design and concludes with verification and evaluation of the proposed approaches in simulation and real life experiments. </p>

<p>The use of fully autonomous outdoor vehicles in the agricultural industry has great potential to bring about a number of advantages, especially to broad acre farming. Most of these advantages can only be harnessed with very high precision path tracking. Such accuracy will enable the use of the same autonomous system right throughout the cropping season for all tasks with no need for crop localization as the crop has been planted at known locations with sufficiently high accuracy. The outcomes of this research include modelling and control approaches that can ensure such accuracy. Undoubtedly, such systems can find many other applications in many other industries; however, the focus here is on agricultural vehicles. </p>

<p>The task of path following of an autonomous vehicle starts with modelling. A realistic and practical model provides the necessary basis for controller design. Such a model should incorporate the ability to take into account practical operating conditions such as a significant amount of slip that naturally exists outdoors, especially in agricultural operations. Kinematic models are used that incorporate slip for nonholonomic systems. However, when kinematic models are not sufficient, dynamic models are combined and forces are considered in the modelling approaches according to the topic of partial dynamics. In general, the aim is to address sufficient complexity for maximum robustness and generality. The models are applied using a modelling approach, which means that the models grasp the most important aspects of the vehicles and ignore the lesser properties with minor influence. To compensate for the compromise in modelling, the control design is focused on robustness, observation and adaptation. </p>

<p>Based on the types of agricultural vehicles and farming activities, this research obtains practical solutions for car like tractors with front wheel steering; tractors towing a trailer or implement and track vehicles with invariable skid steer. Both cases of the trailer/implement with and without steering and drive capabilities are studied.</p>
</tr>
</tbody>
</table>
<a name="DF18"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF18: Building Change Detection from Remotely Sensed Data Using Machine Learning Techniques</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Kaibin Zong</p>
<p>Arcot Sowmya</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Remote sensing data plays an increasingly important role in many applications. With the rapid development of cities, mapping systems usually suffer from obsolete scene details and require timely updating. Therefore, developing change detection technology has become critical. Among all types of changes, building change detection is an important problem. In this work, the focus is on building a reliable model for detecting building changes from remotely sensed data using machine learning techniques.</p>

<p>Starting from the pixel level, the effects of datasets and features on detection results are studied. Different learning algorithms were tested and experiments conducted on different training sets to evaluate their performance and sensitivity to unbalanced and noisy datasets. Then, the problem of multiple correlations among different image bands was addressed and kernel partial least squares method introduced to handle it. To remove false alarms, an object-based post processing method has been proposed in which all spectral, structural and contextual information are considered.</p>

<p>To exploit contextual relations, a novel interactive building change detection method has been proposed. Based on polygons marked by the user, corners are detected and a corner based Markov random field (MRF) model is built. The optimal solution of the MRF indicates building corners, and building changes are detected by linking the corners. To enhance performance, the current MRF model has been extended to a higher order. More corners are considered in the contextual term and the problem is solved by finding its optimal solution. All the models are built based on geometric features and without using colour information.</p>

<p>To automate the detection process and enhance performance, pixel level and corner based detections are then combined together. Two training sets are constructed, one for pixel level classification and another for building corner classification. These results provide an initial indication of changes and are then fused in two ways: one based on colour refined building corners and pixel level MRF, and another based on refined building corners and the corner based MRF model. Experimental results indicate the improved capability to detect building changes accurately.</p>
</tr>
</tbody>
</table>
<a name="EI22"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI22: Active Power Control in an Islanded Microgrid Using DC Link Voltage Status</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Md. Alamgir Hossain</p>
<p>Hemanshu R. Pota</p>
<p>SEIT, UNSW Canberra</td>
</tr>
</tbody>
</table>
<p>The environmental concern due to conventional energy sources motivates us to build microgrids with renewable energy sources that minimize the environmental, economic, and political issues. The distributed generators (DGs) in a microgrid consisting of renewable energy sources (such as solar and wind), and small-scale power generators, can assist to reduce the transmission and distribution cost and defer large investment to establish large-scale generators with new transmission lines [1]. The increasing penetration of DGs raises different issues concerning electrical planning and operation within distribution networks [2, 3]. An integrated approach is required to deal with the connection and operation of a DG unit in a network. To solve this issue, the microgrid concept is introduced [4]. The operation of a microgrid can be either islanded or grid connected mode. In islanded operation, the DG units are responsible to balance the power sharing and control the grid voltage in the network. The principle feature of power sharing is to ascertain that all the DG units share the load based on their power setting point and extractable power from the sources. During the autonomous operation of a microgrid, it provides power to load and, if possible, store power in the energy storage system (ESS) for future use. As the majority of the DG units within the microgrid are connected to the power network via voltage source inverters (VSIs); the controlling of a VSI is an important task in the microgrid control system [5]. In the islanded mode of DG operations, voltage controlled inverters are required to control the voltage amplitude and frequency as well as power sharing in a network. The conventional P/f [6, 7] droop control or communication infrastructure based strategy is implemented in VSIs to control power balancing, transient power, and stability. However, in the low voltage microgrid, this control approach requires much more effort than the P/V control [8]. In this paper, an alternative control approach&mdash;terminal network voltage based approach&mdash;that is based on a special feature of low-voltage microgrids is proposed for power balancing and transient condition during interruptions, e.g., load changes and unintentional islanding. This control method enables an energy storage system that employs a voltage-band at the dc busbar to maintain grid voltage stability for short period disturbances in a network. This voltage-band, applied to obtain maximum benefit of the storage system, depends on a storage capacity feature to avoid voltage limit violation. In addition, a linear quadratic regulator is employed as a voltage controller to track the reference grid voltage that is obtained from the proposed P/V droop control strategy. In the proposed control method, a long-term energy storage element, such as a battery, can also be used to regulate voltage and deliver deficit power in a microgrid. The proposed control method exhibits an effective voltage and power quality performance during disturbances.   References:</p>

<p>[1] P. Chiradeja and R. Ramakumar, &ldquo;An approach to quantify the technical benefits of distributed generation,&rdquo; Energy Conversion, IEEE Transactions on, vol. 19, no. 4, pp. 764&ndash;773, 2004. [2] C. Foote, G. Burt, I.Wasiak, R. Mienski, R. Pawelek, P. Gburczyk, and M. Thoma, &ldquo;A power-quality management algorithm for low-voltage grids with distributed resources,&rdquo; Power Delivery, IEEE Transactions on, vol. 23, no. 2, pp. 1055&ndash;1062, 2008. [3] Y. Li, D. M. Vilathgamuwa, and P. C. Loh, &ldquo;Microgrid power quality enhancement using a three phase four- wire grid-interfacing compensator,&rdquo; Industry Applications, IEEE Transactions on, vol. 41, no. 6, pp. 1707&ndash;1719, 2005. [4] R. H. Lasseter, &ldquo;Microgrids,&rdquo; in Power Engineering Society Winter Meeting, 2002. IEEE, vol. 1. IEEE, 2002, pp. 305&ndash;308. [5] T. L. Vandoorn, B. Meersman, L. Degroote, B. Renders, and L. Vandevelde, &ldquo;A control strategy for islanded microgrids with dc-link voltage control,&rdquo; Power Delivery, IEEE Transactions on vol. 26, no. 2, pp. 703&ndash;713, Apr. 2011. [6] H. R. Pota, &ldquo;Droop control for islanded microgrids,&rdquo; in Power and Energy Society General Meeting (PES), 2013 IEEE. IEEE, 2013, pp. 1&ndash;4. [7] W.R. Issa, M. A. Abusara, and S.M. Sharkh, &quot;Control of transient power during unintentional islanding of microgrids,&quot; Power Electronics, IEEE Transactions on , vol.30, no.8, pp.4573-4584, Aug. 2015 [8] T. L. Vandoorn, J. D. De Kooning, B. Meersman, and B. Zwaenepoel, &quot;Control of storage elements in an islanded microgrid with voltage-based control of DG units and loads&quot; International Journal of Electrical Power &amp; Energy Systems, vol. 64 pp. 996-1006, 2015.</p>
</tr>
</tbody>
</table>
<a name="EI23"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI23: Forecasting for Concentrated Solar Thermal Power Plants in Australia</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Edward Law</p>
<p>Dr Merlinde Kay, Dr Robert A Taylor, Emeritus Professor Graham Morrison</p>
<p>Photovoltaic &amp; Renewable Energy Engineering</td>
</tr>
</tbody>
</table>
<p>This study models the operation of a concentrated solar thermal (CST) plant in the Australian National Electricity Market (NEM) to examine the benefits of using 3-hour direct normal irradiance (DNI) forecasts to update initial bids made from 48-hour DNI forecasts. The benefits are higher financial value to the plant owner and higher plant reliability to the network operator. Financial value is calculated by summing revenue from generating electricity and subtracting reserve generation payments from failing to meet bids. Plant reliability is obtained by calculating the equivalent forced outage rate (EFOR) from the success and failure to meet bids. Success and failure to meet bids is determined using actual DNI. This study uses ground-measured DNI from Mildura, Australia, from 1 June to 30 November 2005. The modelling uses a CST plant model with a 1-hour time step. Individual 48-hour forecasts are produced from persistence of DNI, an autoregressive integrated moving average model, and The Air Pollution Model. New 48-hour forecasts are made daily. The 3-hour forecast is produced by using persistence of average forecast error over the previous two hours to modify the forecast DNI in the next 3 hours.</p>

<p>The forecast error tolerance, that controls when to make 3-hour forecasts, is varied over six values while maintaining the storage size at 7.5 hours and the solar field size at a solar multiple of 1.756. When the tolerance is below 400 W/m2, financial value increases by at least 22% and EFOR decreases by at least 47% compared with the baseline of never making 3-hour forecasts. A tolerance of 0 W/m2 achieves the greatest benefits, thus later simulations use this tolerance. In different sky conditions, as defined by the daily clearness index, results show that a tolerance of 0 W/m2 increases financial value by at least $960/day, $2100/day and $7100/day on average in clear sky, cloudy and overcast days, respectively, compared with the baseline. The same comparison shows that the EFOR decreases by at least 1 percentage point, 22 percentage points and 24 percentage points in clear sky, cloudy and overcast days, respectively. </p>

<p>When the solar multiple is varied in the range of 1 to 2 and storage size is maintained at 7.5 hours, increasing, the solar multiple increases financial value. A solar multiple of 2 has at least 34% greater financial value compared with when the solar multiple is 1. Increasing the solar multiple is not clearly shown to positively or negatively affect plant reliability. When storage size is varied in the range of 5 to 20 hours and the solar multiple is maintained at 1.756, increasing storage size increases financial value and plant reliability. A storage size of 20 hours has at least 8.7% higher financial value and at least 10% lower EFOR compared with when the storage size is 5 hours.</p>

<p>In conclusion, 3-hour forecasts increase the financial value and reliability of CST plants operating in the NEM. CST plant design should favour large storage sizes over large solar field sizes to increase both financial value and plant reliability.</p>
</tr>
</tbody>
</table>
<a name="EI24"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI24: Multilevel Multiphase Coupled Inductor Inverter</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Cheng Tan</p>
<p>John Fletcher Co. Supervisor: Prof. Faz Rahman</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Electricity has been essential in the development of civilization, and is indispensable in the implementation of renewable energy. Pulse-width-modulation inverters are utilized to convert direct current (DC) into alternating current (AC) and are important in areas such as grid-tied photovoltaic (PV) system and electric vehicles. </p>

<p>Multilevel voltage-source-inverters (VSI), which generate three or more output voltage levels, increasingly prevail academically and industrially. The reason they overwhelm their counterparts, which produce two-level output voltage, is that they have many advantages such as lower voltage stress, better harmonic performance and lower electromagnetic interference (EMI).</p>

<p>Multiphase VSIs are inverters that supply more than three phases. In addition to the qualities of the standard three-phase power-generating systems, multiphase inverter systems have exhibited their particularities such as lower torque ripple, better magneto-motive field (MMF) distribution and fault tolerance. These advantages enable multiphase VSIs to be increasingly attractive in industry and academy.   This thesis is established on a multilevel (three-level) multiphase (five-phase) PWM inverter system&mdash;a coupled inductor inverter (CII). Different from the conventional multilevel inverters, the researched inverter system involves magnetic components which are connected as individual coupled inductors for each phase leg. This enables the three-level inverters to avoid control of mid-point voltage on the dc-link capacitors which is a challenge in the control of multilevel inverters. The coupled inductors also free the CII from cross-over protection (dead-time) which is necessary for VSI to avoid a short circuit on the dc-link capacitors. As the dead-time is the main factor causing distortion on the output current, the CII is capable of providing less distorted output current.</p>

<p>The thesis investigates the five-phase CII system from the following aspects:</p>

<p>Firstly, modulating strategies. Multiphase inverters have more than one plane to distribute the harmonics, which means the modulating strategies are more complicated than three-phase inverters which only have one plane. Moreover, the strategies in the over-modulation area have not been explored, so this will be investigated in this thesis.</p>

<p>Secondly, the investigation of the coupled inductors. Although the control of mid-point voltage of the dc-link capacitors is avoided, the coupled inductors&rsquo; flux must be controlled. Thus, it is necessary to ensure the volt-sec balance of magnetic cores. Otherwise, the saturation on the cores will eventually damage the inverter system. The selection of the core and the power loss in the core are also investigated, as they affect the power density and the efficiency of the system.</p>

<p>Thirdly, elimination of the common-mode (CM) voltage: CM voltage is one of the side-effects of employing the PWM inverter systems. The thesis researches methods to reduce the CM voltage in the CII system with the ability of volt-sec balance. </p>

<p>Fourthly, analysis of the dc-link current on the dc-link capacitors. The current fluctuation on the dc-link influences the size of the capacitors thereby impacting the size of the inverter system. The thesis analyses the current fluctuation of three switching strategies so as to find the best method in the control of inverters.</p>

<p>Finally, the performance of driving a five-phase machine with the five-phase CII system is investigated.</p>
</tr>
</tbody>
</table>
<a name="EI25"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI25: Single-Phase Grid Connected Battery-Supercapacitor Hybrid Energy Storage System</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Damith Wickramasinghe Abeywardana</p>
<p>Dr Branislav Hredzak and Professor Vassilios G. Agelidis (Co-supervisor)</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Battery energy storage systems (ESS) enable the distributed renewable power generation by providing many services such as intermittency handling, time shifting and power quality improvement. Prevailing battery technology has limited power capability and, hence, a number of batteries may need to connect in parallel to meet the ESS power requirement. In single-phase applications, a battery has to handle a varying current depending on the output power requirement and a second-order harmonic ripple current component. Further, the battery has to withstand a switching frequency current component due to operation of the power converters. This kind of current variations and ripple components affect the lifetime of the battery.  Battery-supercapacitor hybrid ESS (HESS) becomes a promising way of battery lifetime extension and power capability enhancement. The battery technology has higher energy density with lower power capability and the supercapacitor has higher power capability with limited stored energy. Additionally, the supercapacitors have higher cycle-life. HESS combines the individual advantages of both the battery and the supercapacitor and create a single ESS with both higher power and energy capabilities. Two types of ESS configurations can be identified in the literature, namely, DC link based and direct AC line connected ESS. The direct AC line connected ESS were proposed as a way of eliminating the multiple power processing stages associate with the DC link based ESS. A boost inverter-based, direct grid-connected battery-supercapacitor HESS is proposed in this research. The low frequency power variations are allocated to the battery while the supercapacitor handles the balance power requirement. An extended Kalman filter based state of charge (SOC) estimation method is used to control the battery SOC within a safe operating region. Moreover, a supercapacitor voltage controller is used to control the supercapacitor voltage around a reference value. With the frequency based power allocation, the second-order harmonic current component is diverted to the supercapacitor. However, a continuous ripple current can adversely affect the lifetime of a supercapacitor due to its internal heating. The boost inverter topology contains two output capacitors which can be utilized to handle the second-order harmonic current. A current-feedback ripple reduction method is proposed to divert the second-order harmonic current component to the output capacitors. The proposed ripple current reduction method can reduce the ripple current even in output power transients, which is not possible with previously proposed ripple reduction methods for the boost inverter. Moreover, a phase-shifted, interleaved operation for the boost inverter is proposed to reduce the switching frequency ripple component in the DC source current. Operation of the proposed boost inverter based battery-supercapacitor HESS is experimentally verified using a laboratory prototype. The experimental results illustrate that the proposed HESS can allocate the low frequency power variation to the battery while supercapacitor responds to the balance power requirement. Further, the second-order harmonic ripple current is allocated to the output capacitors of the boost inverter and the switching frequency component in the DC side current is reduced with the interleaved operation.</p>
</tr>
</tbody>
</table>
<a name="EI26"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI26: Developing Bi/Er doped Fibers for Ultra Broadband Optical Amplification</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Amirhassan Zareanborji</p>
<p>Prof. Gang-Ding Peng</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Nowadays telecommunication networks and internet are becoming a natural background part of everyday life and effective factor for development. This rapid increment in global communication and data rate is driving demands for greater transmission capacity of the communication networks. Therefore to increment network capacity, bandwidth broadening in the optical communication system, as the transmission backbone, is an essential. With mprovement in fiber fabrication technology, the optical fibers are able to cover ultra-broadband channels. The main bottle necks for utilizing this capacity are the limitation in the operational bandwidth of the laser sources and optical amplifiers. Development of the active fiber at the core of fiber laser and optical amplifier is necessary to cover the accessible telecommunication window.</p>

<p>My research is about development of a Bi/Er co-doped fiber (BEDF) with ultra-broadband emission between 1000 and 1700 nm, covering O-, E-, S-, C-, and L-bands. Since a bismuth active centre (BAC) is generated by the unshielded outer shell electrons of bismuth, various energy level structures can be formed depending on the host glass composition. This feature will lead to complex spectral emission profiles consisting of multiple BACs and their individual contributions. Characterization of these unknown BACs is important for further improvement of this active media for industrial application. </p>

<p>I am working on the identification of BACs and effective co-dopants, based on the different tests (absorption, emission, and fluorescence lifetime) on the fabricated fiber. Due to the novelty and complexity of emission processes in BEDFs, we introduced two new measurements and processing methods for emission spectrum and fluorescence lifetime tests. In these methods, by combining traditional measurement methods and modern digital signal processing techniques, we are able to extract the characteristics of emission, excitation and fluorescence lifetime of active fibers. We characterized multiple active centres in BEDFs fabricated at UNSW with ultra-broadband emission. By processing these results, we are able to amend our fiber composition, dopant concentration and fabrication process, for further improvement in gain, bandwidth and flatness of these active media. They have great potential to use as ultra-broadband ASE sources, tuneable fibre lasers or amplifiers. These fiber lasers and amplifiers are capable of increasing significantly the transmission capacity of the current optical networks, to cover increasing demand in the future.</p>
</tr>
</tbody>
</table>
<a name="FS24"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS24: Open Channel Microneedles Fabrication by 3D Laser Lithography and Micromoulding Techniques</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Zahra Faraji Rad</p>
<p>Dr Robert Nordon</p>
<p>Graduate School of Biomedical Engineering</td>
</tr>
</tbody>
</table>
<p>Microneedle patch arrays are a necessary component for drug delivery and point-of-care diagnostics by addressing the clinical need for unskilled, painless collection of blood or interstitial fluid and delivering therapeutic molecules. For blood collection microneedle length must be at least 600 microns to penetrate the subcutaneous capillary bed. Optimal microneedle geometry for this application has not been achieved by Deep Reactive Ion Etching (DRIE) due to limitations and complexity of the process, particularly for microneedle lengths greater than 200 &micro;m. The aim of this study was to evaluate the utility of 3D laser lithography for the manufacture of master moulds, and to manufacture this geometry from medical grade thermoplastics by replica moulding. </p>

<p>Microneedle master moulds were fabricated by 3D laser lithography using the Photonic Professional GT system (Nanoscribe GmbH, Germany). Master microneedles were made from the UV-curable photoresists &ldquo;IP-S&rdquo; developed by Nanoscribe GmbH. A &lsquo;soft&rsquo; negative impression of the master was cast using silicone elastomer (polydimethylsiloxane, PDMS), followed by soft embossing of thermoplastic microneedles by a rheometer using PDMS as a negative mould. Thermoplastic pellets (Zeonor 1060R) were loaded onto PDMS moulds, heated above their glass transition temperature, compressed between two stainless steel plates and gently de-moulded after cooling to room temperature.  Preliminary studies have shown that it was not possible to emboss hollow microneedles by the soft embossing method as the central PDMS cores remains inside some of the internal channels after de-moulding. However open channel microneedles with microchannels connecting to a reservoir were easily replicated by soft embossing. For the first time a series of novel designs (Australian Provision Patent No 2014903523) of out-of-plane microneedles consisting of open channel networks and fluid reservoirs has been precisely manufactured and replicated. The elastomeric mould can be reused without affecting the quality of the final structure; the PDMS moulds remained undamaged after at least 22 replication cycles. The mechanical behaviour of thermoplastic microneedles were analysed by mechanical testing and Finite Element Analysis (FEA). It was concluded that polymeric microneedles fabricated from Zeonor 1060R have the mechanical strength to penetrate biological tissues without failure. Two phase flow and the level set method was used to model capillary driven filling of microneedles with blood. The rate of filling depended on the contact angle (<90&deg;), the viscosity of the fluid, and the capillary radius. It takes around 30 ms for a 700 &micro;m long microneedle to fill passively by capillary action.  We fabricated novel plastic microneedle arrays that are long enough to penetrate the subcutaneous capillary plexus by replication from a 3D lithographic master. This manufacturing technique allows implementation of geometric designs directly without manufacturing constraints imposed by machining or etching processes. The use of 3D laser stereolithography to create master prototypes, with replication by low cost soft-embossing is a major advance in the field of microneedle manufacture with great potential for large-scale manufacture of these novel devices.</p>
</tr>
</tbody>
</table>
<a name="FS25"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS25: Retinal Ganglion Cell Responses to Extracellular Microelectrode Stimulation – An In Vitro Study On Selective Activation and Spatial Activation Extent With High Frequency Stimulation and Different Electrode Configurations</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chih Yu Yang</p>
<p>Nigel Lovell</p>
<p>Graduate School of Biomedical Engineering</td>
</tr>
</tbody>
</table>
<p>There are 1.5 million people worldwide suffering from retintis pigementosa. The disease is characterised by the progressive loss of photoreceptor cells which eventually lead to blindness. However, the remaining retinal ganglion cells (RGCs) can still be artificially activated to send signals to the brain for visual perception. Therefore, bionic eye that uses electrical stimulation has been a promising technology for blind people to perceive light again. </p>

<p>There are two main challenges in current bionic eye design. The first challenge is to selectively activate RGCs that respond to either the appearance or disappearance of light (ON RGCs and OFF RGCs). Since these types of RGCs can be closely located, they experience very similar electrical stimulation strength. Therefore, we need to find stimulation strategies that favourably activate one type over the other to achieve selective activation. The second challenge is to restrict the spatial extent of activation of RGCs so that spatial resolution of bionic eye can be enhanced. </p>

<p> In order to solve the above-mentioned challenges and improve upon existing stimulation strategies for prosthetic vision, it is essential to first assess stimulation effectiveness by measuring the responses of stimulated RGCs in retinal tissue. Therefore, we used wild-type mice retinae as experiment model to study in vitro RGCs responses to extracellular microelectrode stimulation. The two recording methods used are patch clamping and calcium imaging.</p>

<p>We performed calcium imaging to record populations of RGCs responses. This method enables signals to be recorded simultaneously from a large number of neurons and thereby allows retinal spatial activation profiles to be determined for different stimulation strategies. Thus, the spatial activation profiles can be obtained for different stimulation strategies and quantitatively compared. Data have shown that the larger the stimulation strength, the wider the spatial activation extent. In addition, RGCs close to the stimulating electrode responded more vigorously. It is to be determined how strongly RGCs respond to light stimuli and thus, we can optimise electrical stimulation that makes RGCs give similar response intensity while restricting spatial activation extent. Furthermore, different electrode configurations such as bipolar and hexapolar electrode configurations are to be tested in addition to the normal monopolar configuration to see if spatial activation extent can be improved by these electrode configurations.</p>

<p>As for selective activation, it has been shown that, at high frequency stimulation, ON and OFF RGCs showed differential responses. However, experiments have only been shown under condition that stimulating electrode was close to the patch clamped RGC. Since clinically it is very hard to locate electrode close to certain RGCs, we thus want to investigate whether differential RGCs responses can still be obtained with stimulating electrodes at different locations, or rather, variations of differential responses as stimulating electrode location changes.</p>

<p>The final goal is to test whether both selective activation and reduced spatial activation extent can be observed simultaneously as more calcium imaging and patch clamp data are obtained. These data will be valuable to design better visual prosthetics.</p>
</tr>
</tbody>
</table>
<a name="FS26"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS26: 3D Neuronal Networks in Hydrogels for Living Bionic Device Interfaces</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ulises Aregueta Robles</p>
<p>Rylie Green</p>
<p>Graduate School of Biomedical Engineering</td>
</tr>
</tbody>
</table>
<p>Neurological disorders and several diseases can be treated by means of neuronal prostheses. These prosthetic devices aim to restore deprived neurological function by electrical stimulation of nerve tissue using metallic electrodes. Poor host tissue integration of neural prosthetic devices is one of the main factors hindering electrode performance. Tissue engineered electrodes aim to deliver a living cellular layer interfacing bionic devices with target tissue that can establish a synaptic connection with target nerve tissue and improve long-term host tissue integration. A dominant challenge to develop this technology is to create standalone neuronal cultures able to grow and mature within a degradable carrier. Hydrogels, regarded as tools for tissue engineering, can be used as cellular scaffolds for neuronal cultures. Therefore it has been proposed that a potential approach to address the aforementioned limitations is to develop a neural culture within a degradable hydrogel. Therefore the overall aim of this research was to use biosynthetic hydrogels to investigate encapsulation of neurons and glial cells within the electrode interface. To address this issue, the first approach of this research was to understand the culture conditions to support complex cultures of glial cells and neurons. Then it was critical to tailor hydrogel physical and biochemical properties to match extracellular niche. It has been hypothesised that providing glial cells with the conditions to grown and mature, they should be able, in turn, to provide neurons with the conditions to develop as well. Consequently, key design criteria for the supporting hydrogel includes, matching mechanical properties with glial tissue stiffness as well as degradation rate to allow cellular maturation. Poly(vinyl alcohol) functionalized with tyramine (PVA-Tyr) supports covalent incorporation of non-modified tyrosine rich proteins and can be tailored to mimic the biological environment while providing mechanical support. Since physical and mechanical properties of PVA-Tyr hydrogels are related to the percentage of macromer, then the second approach of this research was to understand mechanical and biological aspects on encapsulated glia in PVA-Tyr/sericin/gelatin hydrogels at different macromer concentrations.  Ongoing research explores glial cells potential to promote neuronal development by simultaneously culture neurons and neuroglia within the hydrogel. Future work will address guidance cues to direct neurite outgrowth and produce synaptic connections with target tissue.</p>
</tr>
</tbody>
</table>
<a name="FS27"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS27: Drug Delivery Vehicles Based on Albumin-polymer Conjugation for Cancer Therapeutics</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yanyan Jiang</p>
<p>Prof. Martina Stenzel</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>The albumin-polymer conjugation emerged in the 70s of last century. This technique in the pattern of albumin PEGylation provides more possibilities to design albumin hybrid nanoparticles. Since then, a large number of polymers have been exploited to conjugate with albumin. Albumin-polymer conjugates exhibit a desirable combination of properties deriving from both biological and synthetic materials, which can be individually tailored into a predictable effect.</p>

<p> We synthesized, modified and characterized albumin-polymer conjugates in regards of promoting therapeutic efficacy, selectivity and safety of anticancer drugs in this interdisciplinary field. When albumin was conjugated with hydrophobic polymers (e.g. PMMA and PCL), giant amphiphilic architectures could be developed, which stems from their self-assembly in aqueous media leading to the formation of new type of micelles or other aggregates. In the meanwhile, some hydrophobic anticancer drugs could be encapsulated into the polymeric core, such as curcumin and albendazole. Emphasis has been given to the strategies that were employed to achieve the albumin-polymer conjugation and the formation of the albumin-polymer hybrid nanoparticles which prove to be better-defined than the pure albumin vehicles. We have also highlighted the typical albumin-polymer hybrid particles as drug carriers and elaborated their intriguing properties compared to the traditional polymeric particles, their utilities to delivery cancer therapeutic agents as well as the desirable cellular uptake mechanism. Our results demonstrated that albumin introduces an element of selectivity towards cancer cell lines and the healthy cells are less affected while the micelles based on polymers lead to even higher toxicity to healthy cells compared to cancerous cells.</p>

<p>The advantage of this system can be described as simple and inexpensive while being efficient to select cancer cells and also able to deliver different types of drugs. The delivery of macromolecular platinum drugs into cancerous cells was also enhanced by conjugating the polymer to albumin. The monomers N-(2-Hydroxypropyl)methacrylamide (HPMA) and Boc protected 1,3-diaminopropan-2-yl acrylate (Ac-DAP-Boc) were copolymerized in the presence of a furan protected maleimide functionalized RAFT agent. The resulting polymer P(HPMA14-co-(Ac-DAP-Boc)9) was used as a macromolecular ligand for the conjugation to the platinum drug and formed nanoparticles. After coated nanoparticles, a superior activity was demonstrated compared to nanoparticles that were prepared from the macromolecular Pt drug alone.</p>

<p>In addition, we expanded the portfolio of albumin-based carriers to oligonucleotides while the design of the carrier still maintained the high selectivity of albumin. Novel biocompatible polyion complex micelles, containing bovine serum albumin (BSA), polymer and oligonucleotide, were synthesized as a generation of vectors for the gene transfection. </p>

<p>As a conclusion, the applications of albumin-polymer conjugates as a delivery system will be a significant addition to the new generation of cancer treatment and imaging. This fusion of biological and chemical properties establishes a critical role of albumin-polymer conjugates in the intersection of chemistry, biotechnology, nanotechnology, and medicine.</p>
</tr>
</tbody>
</table>
<a name="FS28"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS28: Preparation of Novel Materials With Synthetic Heterocyclic Catechol Derivatives Via The Self-Polymerization Regime of Dopamine</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ka Fan</p>
<p>Anthony Michael Granville</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>The mussel-inspired redox-facilitated self-polymerization technique has been found applicable to a wide range of catecholamines other than dopamine, which include norepinephrine, &#671;-3,4-dihydroxy-phenylalanine (&#671;-DOPA), as well as other synthetic dopamine derivatives. Under oxidizing conditions, these catecholamines transform into intermediates which share a common structure of 5,6-dihydroxy-indole (DHIn). Self-polymerization is triggered by further oxidation, which activates the C-4 and C-7 positions for &#960;-conjugation while mild cross-linking is achieved through the C-2 position to form polymeric materials with the base structure of poly(dopamine) (PDA).  The structure of DHIn and its ability to self-polymerize have inspired us to adopt the reaction regime to a range of synthetic compounds based on heterocyclic derivatives of catechol, including those of indazole and benzimidazole, for the preparation of novel materials which are analogous to PDA. These compounds are capable to copolymerize with dopamine, which would further extend the variety of material that can be prepared through various combinations. Despite the structural similarity between the resulting polymeric materials and PDA, they can exhibit different performance because of the additional functionality inherited from the respective monomer (i.e. differences in the five-member ring). To demonstrate the claims above, herein we focus on the self-polymerization of 5,6-dihydroxy-1H-indazole (DHI). We were able to prepare copolymers with DHI and dopamine in different molar ratios. Polymeric capsules fabricated from the copolymer in 1:3 DHI-to-dopamine ratio showed promising structural stability, while the cell growth inhibition study indicated that the incorporation of DHI has negligible impact on the already-low toxicity of PDA. DHI provides pyrazole moieties as coordination sites which can be potentially used for binding with a number of transition metals and therapeutics based on metal complexes. As a proof of concept, we demonstrated the capability of binding with copper as well as radioactive gallium-67. The copolymer also exhibit selective binding and release behaviour, in particular for gallium, upon change in pH of the environment. Such properties are of particular interest for preparing cargo delivery systems for theranostic agents in biomedical applications.</p>
</tr>
</tbody>
</table>
<a name="FS29"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS29: Quantitive Wear Particle Analysis for Osteoarthritis (OA) Assessment</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Meizhai Guo</p>
<p>Zhongxiao Peng, Megan Lord</p>
<p>Mechanical &amp; Manufacturing Engineering</td>
</tr>
</tbody>
</table>
<p>Osteoarthritis (OA) is one of the most common joint diseases that can cause pain, swelling, stiffness, joint deformities, and can be extremely debilitating. The burden of OA is expected to increase exponentially in coming decades due to an aging and increasingly obese population, with prevalence expected to reach three million Australians by 2032. During the progression of OA, the cartilage experiences a continuous degradation and releases wear particles into the surrounding synovial fluid. It has been reported that the wear particles generated during different stages of OA are distinct in terms of their shape and origination. The surface morphologies have been performed in animal models with limited analyses of particles generated during OA in human patients. The aim of this study is to quantitatively characterise the boundary and surface features of human wear particles present in the synovial fluid of patients without OA and with different OA conditions. These particle characteristics are then assessed to identify characteristics that would correctly classify particles to their clinical OA grade. </p>

<p>In this study, laser scanning microscopy was used to capture 3-dimensional (3D) images of 331 human knee particles from 24 patients with clinically classified no OA, mild (grade 1-2) or severe (grade 3-4) OA. The 3D images were analysed using 29 numerical parameters including amplitude, spatial, functional, hybrid and shape descriptors. Correlation analyses were then carried out to identify correlated parameters. Of the 29 parameters analysed, 19 were found not to be correlated and were selected for further grouping analysis. Discriminant analysis was used to group particles based on their differences. Five parameters were identified, including Density of summits (Sds), Form factor (F), Solidity (S), Area (A) and Material volume (Mv). These parameters relate to spatial and shape descriptors of the particles. These five parameters were then used to separate the particle samples into three groups (no OA, mild OA and severe OA). These five parameters were able to classify 57.7% of the particles into the same OA grades as the clinical classification. In conclusion, this study has the potential to develop an objective and minimally invasive method of OA diagnosis that may also be used to monitor disease progression.</p>
</tr>
</tbody>
</table>
<a name="DF19"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF19: Cryogenic Electronics for Quantum Computer Interface: A Low Temperature CMOS D/A Converter for Si Quantum Computer Controller Circuit</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Md Tanvir Rahman</p>
<p>Torsten Lehmann</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>My research work is focused on designing low power, high speed, cryogenic, current steering digital-to-analog converters (DAC). It is primarily designed for the triggering of quantum bits (qubits) which perform the quantum gate operations in a silicon quantum computer.  Si quantum computers use the principle of quantum mechanical phenomena, such as superposition and entanglement, to perform quantum logic, based on wide spread and matured Si technologies. Like digital computers characterized by classical bits 1 and 0, quantum computers also process information through qubits. The arrays of qubits, located in a quantum processor, must operate in deep cryogenic temperatures and these qubits also need very fast, pulse-based classical control circuitry for their initialization. A low power, 10 bit DAC operating in the GHz range and located in close proximity to qubits at or below 4.2 K while driving a 50 &#8486; load is a potential candidate for this purpose.  We have chosen partially segmented (4 bit LSB + 6 bit MSB) current steering DAC structures as these types of DACs can drive a resistive load without the help of any output buffer and, therefore, they provide a faster response than their counterparts and a balanced output in terms of speed, power and precision. To implement the converter for low temperature operation, CMOS Silicon-on-Sapphire (SOS) process is our primary choice due to its high inherent unity gain frequency, faster switching speed, and absence of hysteresis and reduced kink in cryogenic temperature. We use the industry standard EDA software Cadence and using Silanna&rsquo;s fully depleted 0. 5 &micro;m SOS CMOS process to design and fabricate our device. In the first prototype of our cryogenic converter we propose a unique current cell design procedure with a novel charge injection reduction technique. The converter is able to retain its complete monotonic linear behavior over the wide temperature range of 300 to 4.2 K. In the second prototype we propose another novel current cell design with less area and improved performance compared to the first prototype. This current cell also successfully works down at liquid He temperature (4.2 K) and drives a 50 &#8486; load with hundreds of ps rise time while operating from a 3 V power supply. It is worth mentioning here that this is the first time a DAC prototype is reported which is able to function at as low as 4.2 K temperature.</p>
</tr>
</tbody>
</table>
<a name="DF20"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF20: The Development of A Model Predictive Control Approach for Paste Thickeners</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chee Keong Tan</p>
<p>Prof. Jie Bao & Dr. Gӧtz Bickert</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>The processing of coal invariably involves at least one stage of solid-liquid separation, where thickeners are commonly employed to concentrate dilute suspensions. In the thickening process, a dilute suspension is fed to a paste thickener. Solids settle under the gravitational force and are removed as a concentrated underflow. Despite its apparent simplicity, the operations of paste thickeners have encountered two main challenges, i.e., (1) the presence of external disturbances as a result of irregular operating conditions and (ii) the co-existence of different settling zones, resulting in a complex mechanism of the process. This motivates the theme of this thesis to develop a modern control technique, in particular, model predictive control (MPC), to overcome the above mentioned challenges.</p>

<p>In this thesis, we adopt the sedimentation-consolidation model as the enabling tools to design a control structure for paste thickeners. Dynamical features of the process are studied and the model parameters are determined. The model is validated using industrial plant data and simulation results show that the model output is in a good agreement with plant data. This shows that the model can be used to design a control structure for further studies.</p>

<p>Based on the model, control studies have been carried out to explore potential improvements in process operation by implementing the MPC strategy. Simulation studies show that the proposed control approach can deliver a higher underflow solids concentration and a better regulated underflow removal rate than the existing operation. It is also demonstrated that taking into account the &ldquo;future&rdquo; time-varying input constraints in the MPC algorithm can help overcome the current control difficulty caused by the co-disposal of coal tailing and coarse reject. </p>

<p>The dynamical study of the model also shows that the process contains dynamics occurring at different timescales. To effectively handle this issue, an MPC scheme with a non-uniformly spaced optimization horizon is proposed. This approach implements the time intervals that are small in the near future but large in the distant future, allowing the fast, moderate and slow dynamics to be included in the optimization whilst reducing the number of decision variables. A sufficient condition for ensuring stability for the proposed MPC is developed. The proposed approach is demonstrated using a case study of an industrial paste thickener control problem. While the performance of the proposed approach remains similar to a conventional MPC, it reduces the computational complexity significantly.  </p>
</tr>
</tbody>
</table>
<a name="DF21"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF21: Coherent Feedback Networks For Generation and Distribution Of Entanglement</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Zhan Shi</p>
<p>Hendra Nurdin</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Research interest in quantum information processing is spurred by non-classical phenomena such as entanglement. Entanglement is a central resource to diverse quantum information applications such as quantum computation and quantum communication. This thesis focuses on Einstein-Podolsky-Rosen (EPR)-like entanglement between a pair of continuous-mode Gaussian fields. When the fields are entangled, they have correlations in quadrature-phase amplitudes. Such entanglement can be produced by a nondegenerate optical parametric amplifier (NOPA). The main scope of this thesis is to exploit coherent feedback networks in the form of the feedback interconnection of multiple NOPAs to generate and distribute entanglement in a power efficient manner. </p>

<p>The thesis has four core parts. The first part shows how entanglement can be generated via a coherent feedback loop that connects two spatially distant NOPAs over two transmission channels. Compared to a single NOPA and a two-cascaded NOPA system, the feedback connection not only improves the entanglement while consuming less power in lossless scenarios, but also increases tolerance to transmission losses under the same pump power.</p>

<p>The second part is to investigate the stability and entanglement of a dual-NOPA coherent feedback system under the effect of phase shifts in the transmission channels. It is shown that, in the presence of phase shifts, entanglement worsens or can vanish, but can be improved to some extent in certain scenarios by adding a phase shifter at each output with a certain value of phase shift. </p>

<p>The third part investigates linear quantum networks of N NOPAs, with N up to 6, which are interconnected in a coherent feedback chain. Each network connects two communicating parties over two transmission channels. This part aims to analyse stability, entanglement between two outgoing fields of interest, and bipartite entanglement of two-mode Gaussian states of cavity modes of the N-NOPA networks, under the effect of transmission and amplification losses, as well as time delays. It is numerically shown that, in the absence of losses and delays, the network with more NOPAs in the chain requires less total pump power to generate the same degree of entanglement.</p>

<p>The final part optimizes entanglement of a linear quantum system consisting of two NOPAs and a static linear passive network of optical devices. This passive network is represented by a 6&times;6 complex unitary matrix and connected in a coherent feedback loop with the two NOPAs. By taking the matrix corresponding to a dual-NOPA coherent feedback system proposed in the first part as a starting point, we employ a steepest descent method to find a complex unitary matrix at which the entanglement is locally maximized. By matrix decomposition, we find the obtained matrix is realized by a static optical network made of beam splitters. Subsequently, we look at a special case where the two NOPAs are interconnected with a passive network described by a 2&times;2 complex unitary matrix. The system is considered in the infinite bandwidth limit. It is shown that, the dual-NOPA coherent feedback system has a local optimality property for distributed generation of entanglement.</p>
</tr>
</tbody>
</table>
<a name="DF22"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF22: Dynamic Adaptation of HTTP-based Video Streaming using Markov Decision Process</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ayub Bokani</p>
<p>Prof. Mahub Hassan</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Hypertext transfer protocol (HTTP) is the fundamental mechanics supporting web browsing on the Internet. An HTTP server stores large volumes of contents and delivers specific pieces to the clients when requested. There is a recent move to use HTTP for video streaming as well, which promises seamless integration of video delivery to existing HTTP-based server platforms. This is achieved by segmenting the video into many small chunks and storing these chunks as separate files on the server. For adaptive streaming, the server stores different quality versions of the same chunk in different files to allow real-time quality adaptation of the video due to network bandwidth variation experienced by a client. For each chunk of the video, which quality version to download, therefore, becomes a major decision-making challenge for the streaming client, especially in vehicular environment with significant uncertainty in mobile bandwidth. </p>

<p>In this thesis, we demonstrate that for such decision making, Markov decision process (MDP) is superior to previously proposed non-MDP solutions. Using the publicly available video dataset and our own built comprehensive bandwidth datasets, we show that MDP achieves up to 15x reduction in video freezing events compared to a well-known non-MDP solution. However, despite of its significant benefit, MDP has a considerable computational cost which can be reduced with the three proposed approaches. The first approach recomputes the solution after downloading every k chunks. The second approach computes the solution once using global network statistics of a given region. The third approach recomputes the solution every x meters using offline statistics for each x meters of the road. The three approaches are compared using real-world 3G bandwidth and mobility traces and the best performance is achieved with x-MDP. We also consider a model-free MDP implementation that uses Q-learning to gradually learn the optimal decisions by continuously observing the outcome of its decision making. Using simulations and empirical evaluations, we find that MDP with Q-learning significantly outperforms MDP that uses bandwidth models.</p>
</tr>
</tbody>
</table>
<a name="DF23"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>DF23: Integrating Cache Locking and Task Scheduling for Real-Time Embedded Systems</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Wenguang Zheng</p>
<p>Hui Wu</p>
<p>Computer Science Engineering</td>
</tr>
</tbody>
</table>
<p>Cashes are widely used in modern processors to bridge the speed gap between processors and off-chip memory. In real-time embedded systems, the embedded software consists of a set of concurrent tasks. Tasks are often subject to timing constraints. The primary objective in designing such systems is to find a feasible schedule for the task set such that all the timing constraints are met. A basic requirement for finding a feasible schedule is to know the WCET (Worst-Case Execution Time) of each task. However, the utilization of caches makes it significantly harder to predict the WCET of a task as it is difficult to know at compile time whether the corresponding data or instruction of each memory access is in the cache or not. Cache locking is an effective technique for improving the predictability of caches. If a data object or an instruction is locked into the cache, it will not be replaced. Therefore, the access time of each locked instruction or data object is always one processor cycle. As a result, cache locking makes it much easier to compute the WCET of a task. </p>

<p>For a real-time embedded system typically with multiple concurrent tasks which are subjects to timing constraints such as release times and deadlines, a feasible schedule for the task set needs to be constructed. Cache locking makes task scheduling more complicated. On the one hand, in order to construct a feasible schedule, the task scheduler needs to know the WCET of each task. On the other hand, in order to compute the WCET of a task, the compiler needs to know the schedule in order to determine the cache sizes of both I-cache and D-cache for the task. Two tasks can share a section of cache if one task is not preempted by the other. As a result, the task scheduler and compiler need to interacts with each other and work cooperatively to find a feasible schedule and select a set of memory blocks of the data and instructions of each task as the locked cache contents. </p>

<p>Multi-core processors with two level caches are increasingly used in modern embedded systems. Examples of multi-core processors are MIPS32 74K and IBM Xenon. In a multi-core processor with two level caches, each core has an L1 I-cache and an L1 D-cache. All the cores share an L2 I-cache and an L2 D-cache. L2 caches make the cache locking problem more complicated as the compiler needs to select the memory blocks of data and instructions as the locked cache contents not only for the L1 caches, but also for the L2 caches. </p>

<p>We study the problem of integrating task scheduling and cache locking for a set of preemptive tasks with individual release times and deadlines on a multi-core processor with two level caches. Our objective is to construct a feasible schedule, select a set of memory blocks of data and instructions for each task and allocate them into the L1 caches and the L2 caches.  </p>
</tr>
</tbody>
</table>
<a name="EI27"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI27: Multidisciplinary Approach to Understanding Groundwater Changes in the Coal Mining Environment</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Katarina David</p>
<p>Dr Timms, Prof Baker and Dr Mitra</p>
<p>Mining Engineering</td>
</tr>
</tbody>
</table>
<p>Groundwater in the southern Sydney Basin is subject to competition between the industry, the need for water supply and the environment. The research area is located in the longwall operating mining area within the Sydney Water Catchment. Here several closed and operating underground coal mines, coal seam gas industry and private water supply bores coexist. In addition, the study area underlies swamps and wetlands which are part of the sensitive ecosystem.  Groundwater inflows into the longwall coal mine following seam extraction are generally predicted with reasonable accuracy by analytical and numerical methods. However, groundwater movement within the constrained zone above the drained fractured zone is less understood. Change in hydraulic parameters in the constrained zone due to undermining is very important due to interaction with shallow aquifers and surface ecosystems that may be vulnerable to changes in water quality and hydrology.  This project was initiated to provide more understanding into groundwater movement and the role of low permeability rock formations as a barrier to vertical flow in sedimentary basins. The research uses multilevel vertical pressure sensors and downhole geophysical data to explain changes in hydraulic properties associated with undermining, and related changes in groundwater flow by comparing pre-mining to post mining conditions in the constrained zone. The analysis of changes in hydrostratigraphic units indicates that redistribution of pore pressures occurs within these strata, with reduction in compressibility with depth and change in porosity. The structural changes occur throughout the full vertical profile above the longwall panel but are more significant in magnitude in homogenous sandstones than layered and bedded intervals. The change in porosity on a small scale is important and cannot be predicted without a detailed knowledge of geology and lithology. A novel porewater stable isotope technique was applied as part of multi-disciplinary investigations of hydrogeology and geomechanical behaviour of rock formations. The rock core samples were analysed for stable isotopes using Los Gatos analyser (calibrated with Los Gatos and VSMOW standards) and for Cl content in UNSW Australia analytical laboratory.  This research focused on conceptualising groundwater conditions and qualifying vertical and horizontal components of groundwater flow and seepage at the interface of high and low permeability rock strata. The study found that detailed vertical profiling using porewater stable isotopes, supported by other investigative methods, allows differentiation between hydrostratigraphic units and provides their detailed understanding within the southern part of the Basin.  The findings are important as they show that porewater stable isotope analysis can be a valuable method in the Australian setting where recharge water had enough variability in &#948;18O to introduce sufficient contrast. The method also has the advantage of being applied in the areas where it is not possible to have a suitable network of standpipe piezometers. Characterisation of groundwater flow in sedimentary basins using traditional groundwater methods can be greatly improved by the addition of porewater stable isotope analysis. Understanding the nature of the distribution, and changes in specific storage and flow is important for definition of an aquifer system and future management.</p>
</tr>
</tbody>
</table>
<a name="EI28"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI28: Spatial and Temporal Importance of Diffuse and Stream Recharge in Semiarid Environments: Implications for Integrated Water Management</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Calvin Li</p>
<p>Martin Andersen, Bryce Kelly, Gabriel Rau</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>In semi-arid environments with climate cycles driven by the El Ni&ntilde;o-Southern Oscillation (ENSO), groundwater often provides a reliable resource for agricultural and portable use. Yet over-development can stress aquifers and affect streamflow, especially during droughts. We examined the contribution of stream leakage to groundwater recharge in the Maules Creek Catchment, NSW. During the drought in the early 2000s, abstraction from alluvial aquifers had lowered groundwater levels by as much as 5m and thereby affected river at low flow conditions. This study quantifies the dominating recharge mechanisms, particularly the role of focused recharge during the recovery of depleted aquifers. </p>

<p>As a part of a long-term monitoring program funded by the NCRIS Groundwater Infrastructure project, groundwater levels have been recorded at Maules Creek Catchment in the Namoi Valley since 2007. The recent natural climatic transitions from dry to wet have been captured with high spatial and temporal resolution. Recoveries were reported following the wetter climate between 2010 and 2012. The climate and hydrology data were analysed to reveal the controlling recharge mechanisms.</p>

<p>Following the transition from El Ni&ntilde;o to La Ni&ntilde;a, multiple medium to large rainfall events (130-260 mm per month) in the headwater area caused streamflow and mountain-front discharge. A general recovery of groundwater levels was observed across the Maules Creek Catchment. Along ephemeral reaches of Maules Creek, where pumping-induced abstraction had created a zone of drawdown, transmission loss during flow events between 2010 and 2013 provided significant recharge and the water table rose to near or above pre-irrigation (~1980) levels. Away from Maules Creek, however, groundwater levels were 0.5-2m lower than pre-abstraction levels l and exhibited a declining trend since the 1980s. Moreover, sections of the Namoi River that changed from gaining to losing after the onset of large-scale abstraction did not restore to the pre- abstraction conditions, even after a groundwater rise of 1.5 m was observed.</p>

<p>It appears aquifers along disconnected stream reaches receive substantial recharge even during smaller runoff events. Results also suggested a complete recovery requires at least two consecutive wet years. By contrast, the impact of sporadic floods on groundwater levels near connected reaches (like the Namoi) has been demonstrated to be modest. Results show individual storms provide a limited contribution to aquifer recovery. However, groundwater does respond to large floods, and pumping does affect streamflow. These aspects both highlight the presence of good recharge pathways from ephemeral streams. Understanding these processes, their distribution and their temporal reoccurrence are essential for using the groundwater resource sustainably as a drought buffer into the future. Opportunities for implement managed aquifer recharge should be explored to enhance aquifer recovery and assist with &ldquo;banking&rdquo; water to drought-proof the irrigation community.</p>
</tr>
</tbody>
</table>
<a name="EI29"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI29: Advanced Characterisation of Dissolved Organic Nitrogen in Drinking Water Sources: Implications for Mitigating Against Nitrogenous Disinfection By-product</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Xiang Li</p>
<p>Rita Henderson, Stuart Khan, Cynthia Joll, Kathryn Linge</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Drinking water quality can be impacted by the presence of algogenic organic matter (AOM), rich in dissolved organic nitrogen (DON) that can act as precursors of nitrogenous disinfection by-products (N-DBPs). Better characterisation of algogenic DON is therefore important in order to optimise treatment processes so that N-DBP concentrations in treated water are minimised as they are more toxic than the regulated carbon DBPs. This study characterises the DON exuded by the green algae Chlorella vulgaris and the cyanobacteria Anabaena circinalis, Microcystis aeruginosa and Cylindrospermopsis raciborskii using traditional and advanced methods, which include ultrafiltration, measurements of total organic carbon and total nitrogen (TOC/TN), dissolved inorganic nitrogen (DIN), and size exclusion liquid chromatography detection with organic carbon and nitrogen detection (SEC-LC-OCD-OND). Potential N-DBP formation of three haloacetonitriles (HANs), two halonitromethanes (HNMs), two haloacetamides (HAAms) and N-Nitrosodimethylamine (NDMA) was evaluated by chlorination or chloramination of bulk and fractional AOM with a mass ratio of free Cl2:TOC of 8:2 and Cl2:N:TOC of 8:2:2, under lab conditions. Additionally, N-DBP formation mechanism during the chloramination was monitored by replacing the N source in AOM using 15N. Finally all the target N-DBPs was measured using GC-MS or GC-MS/MS. </p>

<p>Results showed that AOM primarily contained org-N contents at >50 kDa and around 1 kDa. Among the chlorinated AOM, only HANs were detected in N-DBP formation experiments. The algal species and fractions containing higher org-N content produced greater HAN. The fractional high molecular weight (HMW) AOM (1-50 kDa) of Anabaena circinalis had the highest yield of dichloroacetonitrile (DCAN) at 2.4 &micro;gL-1 or 1.2 &micro;gmg-1C-1. Among the chloraminated AOM, (&#8230;..still waiting for the results&#8230;.)</p>

<p>Overall, there was more N-DBP formation from HMW AOM fractions than low molecular weight (LMW) AOM fractions. </p>
</tr>
</tbody>
</table>
<a name="EI30"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI30: Dynamical Downscaling of Global Reanalysis for Hydrological Applications</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Ditiro Moalafhi</p>
<p>Professor Ashish Sharma & A/Professor Jason P. Evans</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Dynamical downscaling which entails output from the GCM/global reanalysis being used to provide lateral boundary conditions (LBCs) to a regional climate model (RCM) at higher spatial resolution able to simulate local conditions in greater detail is increasingly becoming an integral component of basin-scale hydrologic modelling mainly as a result of limitations of general circulation models (GCMs) or global reanalysis in reproducing surface variables (e.g. precipitation) at scales that are often required for hydrological applications. This is despite global reanalysis particularly doing well in capturing large-scale atmospheric circulation features and partly explains why there is very limited direct use of their products for rainfall-runoff modelling and evaluating hydrologic impact of climate change at basin-scale. Global reanalysis provide the best consistent data sets pertaining to the atmospheric circulation for many dynamical processes that are not easily observed. With their assimilated available observations, they are a valuable tool for studying weather systems and climate variability. Reanalysis products obtained through downscaling global reanalysis can offer computational advantage and greater detail compared with the current advanced regional reanalysis. Regional climate modelling studies often begin by downscaling a reanalysis dataset in order to simulate the observed climate, allowing the investigation of regional climate processes and quantification of the errors associated with the regional model. It is at this stage that if there are some significant biases, some bias correction can also be incorporated to improve simulations. This means that global reanalysis being downscaled should be carefully chosen to make sure that the most accurate LBCs are used which should subsequently improve the simulations. To date choice of reanalysis to perform such downscaling has been made either through convenience or based on performance of the reanalysis within the regional domain for well observed variables such as near-surface air temperature and precipitation. This could then result in inaccurate meteorological forcing that may be limited in representation of the hydrological cycle. The resultant inaccuracies will subsequently propagate through the hydrological model, resulting in inaccurate simulations that have direct consequences on operational usefulness, hydrologic design and water resources planning among others. Thus global reanalysis being downscaled should be carefully chosen to make sure that the most accurate LBCs are used which should subsequently improve the simulations. In this way, accuracy and reliability of the simulation products is enhanced. Use of such high resolution products with skill at basin scale cannot be overemphasized especially for semi-arid regions where understanding the interplay of dynamic hydrologic processes continue to present an insurmountable task. It is against this background that this research, the first of its kind, aims at producing a hydrologically useful representation of the climate over southern Africa through developing a &lsquo;generic&rsquo; framework in which 4-D relevant atmospheric fields which are the principal input to a RCM are evaluated to choose the best reanalysis dataset at the intended boundaries of the RCM for subsequent high resolution dynamical downscaling with demonstrations of hydrological applications at basin scale.</p>
</tr>
</tbody>
</table>
<a name="EI31"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI31: Modeling Risk Attitudes and Risk Perception in Driving Behaviour</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Jianhang Chen</p>
<p>Serkan Saydam and Paul Hagan</p>
<p>Mining Engineering</td>
</tr>
</tbody>
</table>
<p>Fully grouted cable bolts have been used in the mining industry for many years to keep the stability of underground excavations. However, failure of the cable bolting systems, especially the shear slippage along the cable/grout interface is still a problem, which is resulted by the load transfer mechanism.</p>

<p>To better understand the load transfer process between the cable bolt, the grout and surrounding rock mass, a new experimental approach was established in the laboratory. Specifically, a new laboratory short encapsulation pull test rig was built, which was able to overcome the shortcomings that occurred in previous tests. Being consistent with the field scenario, this test rig was composed of two parts namely, the anchor section and the embedment length. Within the anchor length, an internally threaded anchor tube was used to confine the grouted cable bolt, preventing any slippage. While, in the embedment length, cement-based grout was used to cast the test sample, simulating the rock to provide confinement on the cable bolt. A pair of split-tube was adopted to confine the test sample, providing a constant normal stiffness condition. More importantly, a locking key and two locking nuts were used to lock the anchor tube, the bearing plate and the embedment section together, preventing the cable bolt from rotating. </p>

<p>With the established test rig, the influence of different parameters on the load transfer behaviour of cable bolts was studied, including the sample strength, the cable surface geometry, the borehole size, the embedment length. The results showed the sample strength had a significant influence in deciding the bond failure of the modified cable bolting systems. When the weak sample was tested, shear failure occurred along the grout/rock interface. However, when the strong sample was used, bond failure occurred along the cable/grout interface. As for the standard cable bolt system, bond failure always occurred along the cable/grout interface.</p>

<p>A new analytical model was put forward to study the load transfer behaviour of cable bolts. This analytical assumed there was a uniform shear stress distribution along the cable/grout interface when short embedment length was used. A tri-linear model was adopted to analyse the elastic, softening and de-bonding behaviour of the cable/grout interface. This model was able to predict the maximum load transfer capacity of standard and modified cable bolts with different embedment length. Laboratory pull-out tests were used to validate this analytical model.</p>

<p>Numerical simulation work was conducted to analyse the pull-out behaviour of cable bolts. The software of Fast Lagrangian Analysis of Continua (FLAC) was used to evaluate the interaction between the grouted cable bolt and the confining medium. A spring-slider system was adopted to simulate the shear slippage on the cable/grout or the grout/rock interface. The numerical simulation results were compared with laboratory pull-out test results.  </p>
</tr>
</tbody>
</table>
<a name="EI32"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI32: A Joint Constitutive Model for Opened Rock Joints</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yingchun Li</p>
<p>Joung Oh, Rudrajit Mitra, Ismet Canbulat</p>
<p>Mining Engineering</td>
</tr>
</tbody>
</table>
<p>Discontinuities intersecting rock blocks, for the purpose of this study, referred as rock joints, control the strength and deformation properties of natural and engineering rock structures, e.g. rock slopes and underground excavations. The blocks become loosened and thus rock joints are opened due to normal confinement reduction after the sub-surface excavation. Joint opening reduces the interlocking between surface irregularities, which softens the stiffnesses in both normal compression and shearing. The thesis presented a full joint constitutive model for rock joints with various joint openings.</p>

<p>Experimental investigation was first performed. Rock joints specimens with two types of triangular asperities and two types of irregular surfaces were prepared in the laboratory. Normal compression tests and direct shear tests were carried out on rock joints with various initial openings. Results showed that with increasing degree of joint opening, rock joints allowed much larger normal closure between two contact surfaces and the shear strength was decreases appreciably. </p>

<p>Based on the experimental observations, constitutive laws for the opened joints subjected to normal loading and direct shearing were proposed. The joint normal deformability was represented by a semi-log formulation. The performance of the proposed equation has been validated against the experimental data as well as curves taken from famous literatures. A shear model was suggested originating from the principle of wear. The joint asperities were predicted to degrade gradually along with progressive shearing. The resulting model exhibited a pre-peak and post-peak softening preceded by a linear increase with elastic shear displacement. Experimental results had been well predicted by the proposed formulations in shear stress-shear displacement-dilation relationship.</p>

<p>The size of rock joints affected significantly the surface roughness controlling the shear features. The joint roughness in every scale were interdependent provided that natural joint are commonly in a fractal form. A roughness quantification approach was suggested for irregular surfaces at all scales by decomposing the geometric configurations into waviness and unevenness components. It was predicted by the proposed relationship that the rough surfaces would become less steep with increasing measuring length, which was consistent with numerous reports by previous studies.</p>

<p>The proposed constitutive law for opened rock joints were implemented in a Distinct Element Method code, UDEC. An engineering case was studied using the proposed model by comparing to the customary joint models. Stability analysis suggested that the constitutive model incorporating opening effect permitted more realistic and accurate representation in mechanical behaviour.</p>
</tr>
</tbody>
</table>
<a name="FS30"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS30: Improving the Performance of Nanocomposite Membranes By Incorporating Inorganic Fillers for Gas Separation</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Putu Sutrisna</p>
<p>Professor Vicki Chen; co-supervisors : Dr. Hongyu Li and Dr. Jingwei Hou</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Membrane separation technique has gained growing interest being a promising tool for gas separation processes. This is because membrane process, compared with other techniques like chemical absorption and cryogenic separation, does not involve phase change and requires smaller foot print. However, the utilization of membranes for gas separation process in large scale is impeded by the trade-off between gas permeability and gas selectivity. Combining polymeric material and inorganic filler to form mixed matrix membranes (MMMs) to improve permeability while maintaining selectivity is one of the keenly researched subject in recent years for performance improvement.</p>

<p>In addition to the effort to develop high performance MMMs, membrane operational stability under aggressive gas separation conditions is also important to determine the success implementation of MMMs in industrial scale. Take membrane based CO2 separation as an example, the membrane plasticization caused by the CO2 induced polymer chain swelling effect could lead to the increase in free volume and segmental mobility of the polymer, resulting in higher permeance of slow gas and reduction of selectivity thus affecting the long term performance. </p>

<p> Despite its importance, research into CO2 induced plasticization in MMMs are very limited. This research studied the CO2 gas permeation behaviour in two types of dense film MMMs using a glassy polymer (Matrimid-5218) and a block co-polymer (PEBAX-1657) as backbones, and zeolitic imidazolate frameworks (ZIF-8) as nanofillers, namely ZIF-8/Matrimid-5218 and ZIF-8/PEBAX-1657, in order to understand the CO2 plasticization phenomenon in MMMs. While a typical plasticization phenomenon in glassy polymer in Matrimid-5218 based-MMMs was observed in which dual sorption mode dominated CO2 permeation at low pressure followed by an increase in permeability at high pressure. The presence of ZIFs particles inhibited the increase of CO2 permeability because the polymer flexibility was reduced by the presence of ZIF particles. On the other hand, membrane plasticization was not observed in the PEBAX-1657 based-MMMs in that CO2 gas permeability decreased with the increase of feed pressure while the selectivity was relatively unchanged. </p>

<p>Contrast to most MMMs studies conducted with dense films with moderate gas permeance through MMMs, this study also developed composite hollow fiber membranes incorporating nanofillers (ZIFs) into selective material to improve gas permeability. Using a microporous PVDF hollow fiber as substrate, a thin layer of ZIF-8/PEBAX mixture was deposited on the top surface of substrate by dip coating method. The performance of nanocomposite membrane with inorganic filler showed improvement of gas permeability of 300 GPU with slightly lower gas selectivity at 33 compared to the composite membranes utilizing only pristine PEBAX-1657 and PEBAX-1074. </p>

<p>Keywords: plasticization, mixed matrix membranes, nanocomposite, gas separation, CO2 capture </p>
</tr>
</tbody>
</table>
<a name="FS31"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS31: The Durability of Geopolymer Concrete in Marine Environments</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Mahdi Babaee</p>
<p>A/Prof Arnaud Castel</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Geopolymer concrete (GPC) is considered as a potential sustainable, low-embodied carbon alternative for Ordinary Portland Cement concrete (PCC). While during the production of each tone of Portland cement, almost one tone of CO2 is released as the result of calcination of limestone and combustion of fossil fuels, an 80% reduction in carbon emission can be achieved using geopolymeric binders instead. They are formed by the reaction of an alkali activator and an aluminosilicate source such as Fly ash and/or Ground Granulated Blast Furnace Slag (GGBS). The final product is a highly cross-linked aluminosilicate binder which is comparable in mechanical strength to PCC. However, the main obstacle to apply GPC, as a rather new engineering material, for structural purposes and to commercialize these binders is the durability concerns; and the corrosion of embedded reinforcing bars remains one of the most controversial aspects of the durability of geoplolymeric binders. Corrosion of reinforcing bars has been a major concern for decades and using PCC still leads to serious problems with millions of dollars spent for maintenance, repair or replacement of damaged structures. To address this issue, this research is aimed to study the ageing of GPC as a low carbon concrete in chloride contaminated environments using corrosion (electrochemical) tests and numerical analysis. All the tests and studies are focused on the propagation phase of corrosion, which is the phase of active corrosion after depassiviation of embedded reinforcing bars. Performance of both fly ash based and slag based GPCs is investigated.  Due to the very different chemistry of geopolymer concrete compared to traditional Portland cement concrete, all governing parameters of the electrochemistry of steel corrosion in geopolymeric binders such as open circuit corrosion potentials, Tafel coefficients, polarization resistance, microcell and macrocell corrosion currents as well as the electrical resistivity of the samples are investigated and compared to the reported values for Portland cement concrete.  To assess the macrocell corrosion systems, a multiphysics finite element model is developed which solves the coupled system of the governing partial differential equations of moisture, oxygen and charge transfer. The model utilises the experimentally assessed corrosion kinetic parameters to assess the macrocell as well as microcell corrosion rates and also to study the potential and current density distribution across the electrolyte (concrete). So far the results show that although low calcium fly ash based GPC can perform as well as PCC in propagation phase of corrosion, some conventional reference values of corrosion parameters which are indicative of the severity of the steel corrosion in PCC might need some recalibration for GPC. Furthermore, all the commonly used electrochemical test methods can be successfully employed to assess corroding reinforced concrete members made up of geopolymeric binders within an acceptable level of accuracy.</p>
</tr>
</tbody>
</table>
<a name="FS32"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS32: Silicon Quantum Computing Devices</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Jason Hwang</p>
<p>Prof Andrew Dzurak</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>The processing power of modern computer has been growing since its invention, as a result of the  ever downscaling of transistors, the basic building blocks of a modern computer processor. However we are reaching  the limit where any further miniaturisation will see undesirable quantum effects starting to hinder  device operation. Rather than considering a solution to overcome these effects, we can actually  build a whole new class of computer that operates based on the laws of quantum physics. This gives  birth to the idea of a quantum computer.</p>

<p>Quantum bits, or qubits, which are the fundamental units of data representation in quantum  computing, harness quantum phenomena such as quantum superposition and entanglement to perform  computation on certain problems with unprecedented speed. Applications include cryptography,  artificial intelligence, modelling of molecules for pharmaceutical drugs, to name a few.</p>

<p>In this work, our qubit is defined by the spin states of a single electron confined in a silicon  metal-oxide- semiconductor (MOS) quantum dots, a highly promising platform for spin-based quantum  information processing due to long qubit coherence time and device fabrication technology being  compatible with that used in today&rsquo;s multi-billion semiconductor industry.</p>

<p>We have realized single qubit operations [1] in a MOS quantum dot system in isotopically purified  silicon. Single- shot readout of electron spin is performed with an adjacent single electron  transistor, while individual qubit control is achieved using electron spin resonance with an  on-chip microwave transmission line. The qubit coherence time reaches 28 ms with dynamical  decoupling sequence and attains control fidelity of over 99.6%, exceeding the thresholds required  for fault-tolerant quantum computing. In addition, gate voltage can tune the qubit operation  frequency over 10 MHz, which is 2000 times the electron resonance linewidth, opening up the  possibility of multiple-qubit addressability.</p>

<p>Most recently we have demonstrated two-qubit logic operation [2] via the exchange coupling of two  electron spins in the same quantum dot system. By combining switchable exchange interaction using  electrical pulses and single qubit rotations, we realise controlled-NOT (CNOT) operations and  measure clear anti-correlations in the two spin- probabilities of the CNOT gate by independently  reading out both qubits.</p>

<p>Quantum computation requires individual qubits to be coupled in a scalable manner, together with  universal and high-fidelity one- and two-qubit gates. Spin qubits in MOS quantum dots have  fulfilled these criteria and now offer the exciting prospect of a large scale quantum processor  based on this scalable platform.</p>

<p>Reference: [1]. M. Veldhorst, J.C.C. Hwang et al., &ldquo;An addressable quantum dot qubit with fault-tolerant  control-fidelity&rdquo;, Nature Nanotechnology 9, 981 (2014). [2]. M. Veldhorst, C.H. Yang, J.C.C. Hwang et al., &ldquo;A two qubit logic gate in silicon&rdquo;, Nature,  doi:10.1038 (2015). </p>
</tr>
</tbody>
</table>
<a name="FS33"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS33: Design and Operation of the Building Blocks of a Spins-in-silicon Quantum Computer</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Rachpon Kalra</p>
<p>Andrea Morello</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Quantum computers have the potential to revolutionize various fields such as computing, physics, chemistry, biology and even meteorology. The building blocks of these computers are called qubits, which are particles or systems that behave according to the laws of quantum mechanics. While there are many different platforms being explored for qubits, we at UNSW have been working on using the electron spin housed in a semiconductor structure. The fundamental operations of measuring and controlling these qubits have been demonstrated with very high reliability. Due to the delicate nature of quantum systems, their states can lose coherence due to environmental noise. We explored techniques to extend these coherence times created for other systems, and tested their performance on our electron qubits in the gated silicon nano-structure. We found that many of these techniques are ineffective due to their vulnerability to experimental imperfections. These imperfections include both unavoidable errors in applying the techniques themselves, and those inherent to the system. We established a threshold above which these techniques could successfully be used to extend the coherence times of electron spin qubits in a large-scale quantum processor. Having reached all the milestones for single qubit work, it is then necessary to consider interactions between multiple qubits in order to scale the processor. The existing proposals for operating two-qubit logic gates between two electron spins required extremely precise placement of both the spins and the control circuitry. This is technologically challenging even with state-of-the-art fabrication techniques. We therefore devised a scheme by which a different, but satisfactory, form of a logic-gate could be performed that is highly robust to positioning inaccuracies. This was made possible by the unique way in which we confine our electron spins. Beyond two qubit logic gates, the ability to transport qubits around the processor is highly advantageous. We devised a scheme for transport that is sufficiently robust to experimental imperfections. It also has the same advantage that it does not require atomic precision in the fabrication process. In summary, we have worked on the final milestone involved with single qubit work for spins in silicon, and devised schemes, for scaling up, that are in line with today&rsquo;s technological capabilities. </p>
</tr>
</tbody>
</table>
<a name="FS34"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>FS34: Electrical Detection of Single-atom Quantum Spectroscopy</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Solomon Freer</p>
<p>Andrea Morello</p>
<p>Electrical Engineering &amp; Telecommunications</td>
</tr>
</tbody>
</table>
<p>Modern information technology has developed rapidly over recent decades, in large part due to the exponential down-scaling of the size of the operating components which carry the &lsquo;bits&rsquo; of information. This empirical trend known as Moore&rsquo;s Law, has fundamental physical limits due to transistors now approaching the scale of the very atoms from which they are composed. At this limit, quantum effects, such as tunneling, superposition and entanglement, start to play a role in the device operation. For classical computation, the quantum uncertainties introduced make such devices incompatible with classical computation resources of memory and redundancy, which allow classical errors to be corrected. A quantum computer is a fundamentally different machine, which harnesses quantum uncertainty and the wave properties of quantum particles to solve problems that are classically intractable. However, in order to harness the power of these quantum uncertainties, classical errors due to temperature and environmental noise must be reduced or the &lsquo;quantum information&rsquo; contained in each &lsquo;quantum bit&rsquo; (qubit) will decay before it can be useful.   Qubits with long coherence times generally are mutually incompatible with fast manipulation times and since copying of quantum information is impossible, the dominant strategy to achieve memory storage longer than the coherence of your operational qubit is transferring quantum information from there to a longer-lived ancilliary qubit, i.e. a &rsquo;quantum memory,&rsquo; less susceptible to environmental noise. It is an advantageous property of donors in silicon to have qubits of both qualities in a single lattice site, with a highly stable nuclear spin and a readily manipulated electron spin. One outcome of our work has been to demonstrate storage and retrieval of a quantum state from a single donor electron spin to its host phosphorus nucleus in isotopically-enriched 28Si. We demonstrated a high fidelity memory process characterised via both state and process tomography and used dynamical decoupling sequences during the nuclear storage to extend the memory time. The results underlined the inherent versatility and high fidelity of our two qubit system.   However, it is commonly emphasised that an interface is needed between such &lsquo;static qubits&rsquo; and &lsquo;flying qubits&rsquo; such as photons. One area of my project has been working to introduce this interface to the donor electron spin qubit of P-in-Si. The benefits of optically, rather than electrically, addressing this qubit include overcoming the need for mK temperatures and high magnetic fields to achieve spin-dependent tunnelling. Under an applied magnetic field, the specific energy needed to create a bound exciton is dependent on both the donor&rsquo;s electron and nuclear spins, and the conditional absorption of resonant laser light allows one to read out and initialise both spins. Previous experiments have been conducted on a macroscopic ensemble of phosphorous atoms, but by employing silicon nanostructures for single-charge detection we have made progress, to detect the optical excitation and subsequent Auger-ionisation of a single phosphorous atom in order to utilise it as an individual qubit. </p>
</tr>
</tbody>
</table>
<a name="EI33"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI33: Modelling Disrupted Network Behaviour</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Kasun Wijayaratna</p>
<p>Professor S. Travis Waller and Dr Vinayak V. Dixit</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Transport models have been utilised as a planning tool to represent current traffic conditions and forecast future traffic scenarios to assist in decision making. These models assign traffic throughout a network based on concepts of equilibrium. It is assumed that users of the network make rational decisions based on accurate knowledge of the network characteristics and choose the best route available that minimises the travel cost. Under normal day-to-day conditions these models accurately portray the travel behaviour of road users. </p>

<p>Disruptions to a network can be a result of minor events such as breakdowns or accidents and can extend to major catastrophic events such as flooding due to adverse weather conditions or earthquakes. Under all these conditions, it is hypothesised that network users&rsquo; decision making on route choice will transform from the decision making applied during normal day-to-day travel. For an example, users may not select the shortest path or the path containing the minimum travel time due to the presence of the disruption. Accordingly equilibrium concepts used in current transport models are not applicable when disruptions are present on a network; especially when developing models for the purposes of incident mitigation or disaster planning. </p>

<p>The study considers that a disruption acts as a source of information for the road user. The presence of visual signals such as the appearance of brake lights or the observation of the incident site affects their travel behaviour and network utilisation. In addition further information is provided to network users regarding the presence of incidents and disruptions through GPS technology and the development of Intelligent Transportation Systems (ITS). In order to advance the current suite of modelling tools available to a practitioner, it is essential for transportation planning models to account for how users behave in light of these sources of information. Thus, the research project has focussed on understanding the impacts of information and information systems on users&rsquo; decision making in the context of disrupted networks. </p>

<p>The investigation has focussed on investigating driving behaviour in disrupted conditions, in particular how users value sources of online information. The study then uses the new found understanding to build upon equilibrium modelling concepts to develop novel static and dynamic traffic assignment methodologies which accounts for online information acquisition. The new modelling approach offers transport professionals the opportunity to better assess the vulnerability of road networks in the presence of disrupted conditions and accordingly improve traffic management practices and mitigation strategies. </p>
</tr>
</tbody>
</table>
<a name="EI34"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI34: Modeling Risk Attitudes and Risk Perception in Driving Behaviour</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Chenyang Li</p>
<p>Vinayak Dixit, Taha Hossein Rashidi</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Driving is a daily task for most people. Driving is simple, as long as one passes the exam and gets the driving license. Driving is also complicated, because there are so many factors influencing human driving behaviour. Some of the factors are obvious, such as spacing, velocity, acceleration. Besides that, driving behaviour is related to brain cognition and decision making of humans, which is a very sophisticated area. It is therefore practically significant to focus on driving behaviour.    Theoretical developments in decision-making research enable us to consider the driving behaviour from a new angle. However, the application of these theories in modelling the driving behaviour has been rarely seen. Established on the expected utility theory, a new driving behaviour model incorporating risk attitude and perception, two important concepts from behavioural economics is shown. </p>

<p>The aim of this research is to bring the decision-making dimension into the conventional traffic flow theory and to rebuild driving behaviour models in combination with well-developed expected utility theory. The proposed models are expected to better reproduce the real driving behaviour.</p>

<p>To achieve the goal, this research will be divided into four stages.</p>

<p>The first stage is to construct the theoretical models based on traditional traffic flow theory and expected utility theory. The proposed driving behaviour models mainly consist of car-following model, lane-changing model, gap-acceptance model. They are the fundamental driving tasks in the driving scenario of freeway. Risk attitude and perception are explicitly defined in the context of transport research. Constant Relative Risk Aversion (CRRA) model is adopted for the estimation of risk attitude. Risk perception is assumed to be related to the stochasticity of the spacing perceived by drivers.</p>

<p>At the second stage, the calibration of the proposed models will be done. Some real vehicle trajectory data are used for this purpose. So far, the Next Generation Simulation (NGSIM) data of the US Department of Transportation - Federal Highway Administration (FHWA) has been used for the calibration of the parameters. Stata, an excellent data analysis and statistical software, is adapted for processing the data and estimating the values of the parameters.</p>

<p>The next stage is the validation of the models. At this stage, the theoretical models will be programmed into C++ codes and built into AIMSUN for testing. AIMSUN is a widely-used software for transport simulation. It provides the Software Development Kit (SDK). It therefore is feasible to get access to its defaulted driving behaviour models and to modify them. After that, the proposed models could be validated in the simulated freeway of AIMSUN and compared with those traditional driving behaviour models.</p>

<p>Last but not least, the validated models will be built in the controlling system of the autonomous vehicle of Research Centre for Integrated Transport Innovation (rCITI) and the driving simulators of TRACSLab for collecting the data related to driving behaviour. In the autonomous vehicle, the proposed models would be used for analysing the information regarding surrounding traffic conditions and developing the optimal automatic-driving strategy, eventually making the driveless car come true. </p>
</tr>
</tbody>
</table>
<a name="EI35"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI35: Mobile Phone Distraction and Traffic Safety</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Asif Hassan</p>
<p>Dr Vinayak Dixit (S), Prof. Travis Waller (S), and Zhitao Xiong (C) </p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>With the growing technological influences mobile phone use while driving has increased dramatically across the world over the past few decades. As a result, drivers are getting distracted and therefore, crash likelihood increases. This study is divided into three parts. At the outset, this study explored the factors influencing drivers&rsquo; beliefs about mobile phone use while driving. Participants were 236 Australian drivers aged 17 to 76 years who owned mobile phones and held a valid Australian or overseas driver&rsquo;s license from 1-60 years. A self-reported survey captured drivers&rsquo; demographics, information related to mobile phone usage while driving (e.g., calling and text messaging frequency, purpose of using phone etc.) and driver&rsquo;s belief about phone use. Seventy percent of drivers reported using phone while driving. A probit model showed that young drivers are more likely to use mobile phone while driving. This study also explored drivers&rsquo; behavioural, normative, and control beliefs with regards to mobile phone use while driving. A multiple group Confirmatory Factor Analysis (CFA) was used to explore the effects of driving experience on the beliefs of mobile phone use. Initially, proposed model was not a good fit to the data. Numerous iterations were conducted to achieve good-fit and internally consistent measurement model. Results of CFA suggests that novice drivers are more influenced to use their mobile phone while driving based on their attitudes towards that behaviour not by the influence of their referent&rsquo;s approval. These findings are significant for insurance companies to determine their risk premium and for road safety practitioners to identify the disutility of mobile phone use in a transport network. </p>

<p>Furthermore, it investigated the impact of mobile phone distraction on right-turn gap acceptance behaviour in a signalized intersection and examined the behaviour of drivers operating a driving simulator while engaged in various distractions (i.e., hand-held mobile phone, hands-free mobile phone, texting, and baseline (no phone)). The subject was tasked with making a right-hand turn into incoming traffic, and the experimenter controlled the headways of oncoming traffic. Repeated measures ANOVA was applied to test the effect of distraction on different driving performance variables such as: speed, reaction time, rate of acceleration and deceleration, accepted or rejected gaps, lane changing frequency etc. Results revealed that drivers tended to choose a larger gap between the oncoming traffic streams when distracted. In addition, significant reduction in speed when distracted suggests the compensatory behaviour of drivers. Also, mobile phone use while driving influenced gap-acceptance behaviour such that variability was increased in driving speed. To further investigate the gap-acceptance behaviour of distracted drivers, experimental design provided opportunities for subjects to develop subjective beliefs about when it would be safe to turn, and it also elicited their attitudes towards risk. Probit model was used to model crashes in different gaps and various distraction conditions. The results indicate that drivers are more likely to crash when distracted, particularly when texting. These findings will improve the understanding of distraction on gap-acceptance behaviour which is the most critical in the determination of sideswipe crashes. </p>
</tr>
</tbody>
</table>
<a name="EI36"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI36: Traffic Information System Based Traffic Management and Control</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Tuo Mao</p>
<p>Vinayak Dixit, Lauren Gardner, Chen Cai</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>The traffic information system (TIS) is a subsystem in intelligent transport system (ITS). Traffic information system can be defined as an information system which involves the collection, processing and dissemination of traffic data.  As Bluetooth and navigation devices are widely used on latest produced vehicles, the concept of traffic information system has been risen to a new level which is vehicle infrastructure integration (VII) system. VII system utilizes dedicated short-range communications (DSRC) devices to carry and commute traffic information. VII system can be classified into two categories: vehicle to vehicle communication (V2V) and vehicle to infrastructure communication (V2I). Research in V2V communication system mainly focuses on safety improvement such as overpass assist, brake assist and lane change assist. On the other hand, V2I communication is mainly utilized for its real-time mobility data such as vehicle&rsquo;s velocity and position. In my research, research force has been made in the latest technology related V2I communication and its application in ITS environment. In data collection aspect, thanks to DSRC technology, information is not limited as speed, density and flow. More advanced information such as transit routes, transit schedules, vehicle routes, vehicle schedule, turning restrictions, speed limit, direction controls, lane closures, road diversions, delay time, travel time, new roads, accidents, incidents, and congestion can be obtained and collected by multi-source road side unit (RSU) or vehicle on-board unit (OBU).  In data processing aspect, it is worth researching how to process different kinds of traffic information in a brief, smart and fast way. In my research, vehicles&rsquo; route information is manly studied and an algorithm in freeway ramp metering is developed to quantify vehicles&rsquo; route choices. In this algorithm, vehicles&rsquo; routes are divided into sections along the freeway and sections along city arterial roads. The distance of each sections can be quickly obtained and be used in highway ramp metering and other highway traffic control strategy. In data dissemination prospect, thanks to LED traffic signals and variable message signs, city traffic planners have more capability of providing more traffic information to drivers. This is a great opportunity but it is also important to think one step further, due to the limit of driver&rsquo;s perception capacity and driver&rsquo;s perceiving time in the traffic flow, it is worth concerning that how driver&rsquo;s will interact with the additional provided traffic information before provision of such information. Under such concern, a driver&rsquo;s reaction model is studied in my research to imitate and predicted driver&rsquo;s reaction. In other word, traffic information system can function as a shortcut for driver to learn the network by the help of city traffic planner. According to route choice, a TIS based route choice model is based on driver&rsquo;s psychological decision making process. Assumptions are made that driver&rsquo;s route choice behavior is a learning process where driver&rsquo;s perception error is not fixedly distributed and can be reduced by daily based travel or learning. After refining the driver&rsquo;s reaction model, city planners can make most of this model to guide drivers learning city network and making smart choices. </p>
</tr>
</tbody>
</table>
<a name="EI37"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI37: Methodologies for Origin-Destination Travel Demand Estimation within a Strategic Traffic Assignment Model</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Tao Wen</p>
<p>S.Travis Waller, Lauren Gardner, Chen Cai</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>This thesis proposes a novel framework that uses the bi-level programming method, where the upper-level is either a new maximum likelihood estimation method or a least squares method, and the lower-level is the strategic user equilibrium assignment model (StrUE) which accounts for the day-to-day demand volatility. The maximum likelihood method proposed in this paper has the ability to utilize information from day-to-day observed traffic flows to provide a unique estimation of Origin-Destination demand distribution, while the least squares method is capable of capturing traffic flow variations. The proposed model is based on the premise that users gain knowledge of the demand distributions through their past travel experience. Using this knowledge, users seek to minimize their expected travel time and choose a strategy (i.e. travel route) accordingly. They then follow this strategy day-to-day, independent of the realized traffic demand. The network may therefore result in non-equilibrium assignment patterns, which is consistent with the lack of observed equilibrium in field networks. The StrUE model can take the demand distribution as input, and output a set of link flow distributions which can then be compared to the link level observations. The main contribution of the dissertation are highlighted as follows: 1. An extension of the strategic user equilibrium proposed by Waller et al. and Dixit et al is proposed. The proposed model relaxes the assumption of proportional OD demand, as it accounts for users&rsquo; strategic link choice under independently distributed OD demands. The convexity of the mathematical formulation is proved when each OD demand is assumed to follow a Poisson distribution independently; link flow distributions and users&rsquo; strategic link choice are also proved to be unique. Network performance measures are given analytically. It is illustrated that the model is capable of accounting for demand volatility while maintaining computation simplicity. 2. The capacity uncertainty is introduced in the StrUE model, which not only addresses day-to-day volatility in travel conditions resulting from short-term demand uncertainty, but also incorporates the impact of capacity uncertainty. Specifically, the demand and capacity variability are represented independently using assumed known distributions. 3. A methodological framework is employed to model the day-to-day learning process of road users, and the corresponding system performance over time with a focus on the impact of specific new developments. Travellers assume an initial demand distribution, and incrementally update it based on their day-to-day travel experiences. Bayesian Inference is used to update the travel demand distribution, and the strategic user equilibrium model is used to compute the underlying traffic assignment pattern.  All the optimization problems are proved to be convex mathematically, numerical analysis is conducted to illustrate the efficiency and robustness of the proposed framework. References:</p>

<p>Waller, S.T., et al., Linear Programming Formulation for Strategic Dynamic Traffic Assignment. Networks and Spatial Economics, 2013: p. 1-17. Dixit, V., L.M. Gardner, and S.T. Waller. Strategic User Equilibrium Assignment Under Trip Variability. in Transportation Research Board 92nd Annual Meeting. 2013. </p>
</tr>
</tbody>
</table>
<a name="EI38"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">EI38: The Demand and Supply Modelling of Carsharing Systems</td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Sisi Jian</p>
<p>Dr. Vinayak Dixit, Professor S. Travis Waller</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Carsharing, as an alternative to private vehicle ownership, has spread worldwide in recent years due to its potential of reducing congestion, improving auto utilization rate and limiting the environmental impact of emissions release. However, the growth of carsharing has brought some issues to it, such as low vehicle utilization rate and imbalanced vehicle stock distribution. These issues are mainly due to the interplay effect of demand and supply of carsharing systems. Thus, to better plan its deployment, this research develops a comprehensive decision support system considering both demand and supply sides of carsharing. The system involves user behavior modeling and carsharing network optimization.</p>

<p>From the demand side, it is critical to understand what factors affect users&rsquo; behavior to determine the most efficient allocation of resources within a carsharing program. Most studies have built demand prediction models without characterizing the supply within modeling frameworks. However, this is not accurate due to the interdependency of vehicle availability and carsharing trip demand. To bridge this gap, this study investigates users&rsquo; vehicle selection behavior which is constrained by the supply of vehicles at each carsharing facility. A Spatial Hazard Based Model (SHBM) is applied to modeling the vehicle selection process. In the SHBM model, &ldquo;distance to a vehicle&rdquo; is considered as the prospective decision criteria that carsharing users follow when evaluating the set of alternative vehicles. In addition, user socio-demographic attributes, vehicle characteristics, and trip information collected from the Australian carsharing company GoGet are utilized to parameterize the shape/scale/location parameter of the hazard function. A number of forms of parametric SHBMs are tested to determine the best fit to the data. The accelerated failure time model with a Log-logistic distribution was found to provide the best fit. The estimation results of the coefficients of the parameters can provide a starting point for carsharing organizations to optimize their pod locations and types of cars available at different pods to maximize usage.</p>

<p>From the supply side, the study focuses on the one-way carsharing system which provides users more flexibility on returning stations. It has been proven to attract larger market demand than the traditional round-trip service. However, the main challenge faced by such system is the vehicle stock imbalance problem due to the uneven distribution of user demand. This study proposes an optimization model integrating with a discrete choice model to address this problem. The model accounts for the interdependent relationship between demand and supply. User demand is influenced by the availability of carshare vehicles; meanwhile conversely, the demand further changes vehicle availability and vehicle stock distribution. The model determines the optimal relocation decisions to maximize the profit for operators that offer both one-way and round-trip services. The model is applied to the network of GoGet to evaluate the impacts of different pricing and capacity policies on system profit. The results indicate that one-way trip price has a more significant impact on system profit than pod capacity. The maximum profit occurs when the price of one-way trip is around four times higher than round trip. </p>
</tr>
</tbody>
</table>
<a name="EI39"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI39: Development of Steel-Timber Composite (STC)  Floors for Large Scale Construction</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Amirhossein Hassanieh</p>
<p>Hamid Vali Pour, Stephen  Foster</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>The building industry is responsible for 40% consumption of natural resources and 30% consumption of energy. Furthermore, approximately 30-40% of solid waste and greenhouse gases are generated by the construction industry. Accordingly, there is a need for developing efficient construction methods and structural systems which are robust and reliable as well as economical and sustainable with the minimum impact on the environment. Timber is a renewable resource and it has the lowest embodied energy and the ability to sequester carbon throughout its lifecycle. Furthermore, the life cycle greenhouse gas emission and the cost of production for engineered wood are lower than steel and reinforced concrete. In addition to operational energy efficiency, constructing structures in timber can significantly reduce their carbon footprint. In this paper, a novel and efficient structural system that comprises steel beams and prefabricated timber slabs is developed and tested under short-term service and ultimate limit state loading conditions. In the proposed steel-timber composite (STC) system, bolt and self-tapping screws are employed to transfer shear between steel beam and prefabricated timber slab and provide a composite connection with a near full composite action. The proposed composite system has the advantage of being light-weight compared with other types of composite floors (i.e. steel-concrete and timber-concrete composites) and also it can significantly improve speed of construction and accordingly reduce the cost of constructing tall buildings. Furthermore, the proposed steel-timber composite (STC) system can immensely facilitate deconstruction and future reuse and/or recycling of materials.</p>
</tr>
</tbody>
</table>
<a name="EI40"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI40: Finite Element Model for Analysis of Buckling and Post-buckling Behaviour of Continuous Concrete Pavement Blow-ups</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Yang Yang</p>
<p>Prof. Mark Bradford</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>The paper presents a finite element model for the linear analysis of concrete pavement blow-ups which is also well known as concrete pavement thermal buckling. Concrete blow-ups has been a problem that troubled concrete pavement construction area for more than 100 years, although it is known that the main cause is rise of temperature, until 1980s, Kerr AD had firstly presented a widely accepted analytical analysis for this problem. It should be noticed that during Kerr&rsquo;s research, several assumption have been made which were actually against the reality, and because of lack of experimental study and finite element analysis, the influence of these assumption, and thus the results of Kerr&rsquo;s analysis reliability is still questionable. In this paper, a finite element model is developed using a beam laid on foundation model with contact behaviour between them to describe the concrete pavement thermal buckling phenomenon. The finite elements analysis process has been proposed by Riks Method via Abaqus. According to the finite element analysis result, good agreements have been archived when the results were compared to the analytical results of Kerr AD, hence, it could be concluded that Kerr AD&rsquo;s formulation could predict the buckling and post-buckling behaviour of continuous concrete pavement under thermal loading quite accurately.</p>
</tr>
</tbody>
</table>
<a name="EI41"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI41: Disaggregate Behavioural Land Use Modelling: Integration of Housing Search, Job Search and Households’ Dynamics</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Milad Ghasrikhouzani</p>
<p>Prof. S.Travis Waller, Dr. Taha Hossein Rashidi</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>The study on land use pattern is a topic of interest in a variety of disciplines including transport engineering, urban planning and economics. Modelling people&rsquo;s decisions on residential location and job location, enables planners to examine the relative impact of various policies on movements in residential and job location , as they affect travel pattern and economic system of households as well as the economy of the society. Previous studies can be categorized into two groups: there are a few studies focused on the one-way impact of one of these relocation decisions on the other one, and there are less number of studies attempted to systematically look at the mutual impact of these two relocation decisions.  This study focuses on the mutual interaction between job and home relocation decisions of household members using a system of equations approach which returns the probability of residential relocation and job relocation for a specific point of time. The model for residential relocation decisions in this study not only contains socio-demographic attributes of individuals and their past life events, such as marriage or divorce, but also encompasses the probability of job relocation; and vice versa. In addition to the simultaneous system of models for residential relocation and job relocation decisions, this study introduces a thorough approach for incorporating the effects of the two decisions on each other. The majority of previous studies indicated an encouraging influence by job relocation decision on the likelihood of residential relocation. However in specific occasions this relationship is not true. For instance, if residential relocation is made to decrease commute distance/time it would not encourage a further job relocation. That is to say, if the relocation results in a more pleasant daily commute for individuals, it would not motivate them to organize a new relocation. Therefore, the proposed system of models is designed to capture the reason of relocation as well. The most challenging part of developing a model for residential relocation is obtaining a rich data set. This study utilizes the survey of Household Income and Labour Dynamics in Australia (HILDA) which is a panel data and contains details information of socio-demographic attributes of nearly 20 thousand individuals whose information is obtained from 2001 and it is planned to be collected until 2016. HILDA also provides the history of peoples&rsquo; residential relocation and job relocations as well as the reason of relocations providing required data for developing the aforementioned system of models. </p>
</tr>
</tbody>
</table>
<a name="EI42"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI42: Will Flooding Intensify in a Warmer Climate?</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Conrad Wasko</p>
<p>Ashish Sharma</p>
<p>Civil &amp; Environmental Engineering</td>
</tr>
</tbody>
</table>
<p>Precipitation intensities are expected to increase with climate change. However, assessing the impact of climate change on future flood risk is a complicated task. Flooding does not just depend on the intensity of the precipitation. The timing of the precipitation, its spatial extents, and whether or not it has been dry or wet recently can dictate the flood response. Till now, studies have focussed on the single metric of precipitation intensity when projecting how climate change will impact future flooding. Here, we show that changes to future precipitation extremes are more complicated than simply assuming a uniform change to the precipitation amount. By generating patterns of precipitation that adequately capture spatial and temporal changes with temperature increase we propose a holistic method of flood projection for a future climate. </p>
</tr>
</tbody>
</table>
<a name="EI43"></a>
<tbody>
<tr>
<td width="100%">
<table>
<tbody>
<tr>
<td style="background:#AAECB3;" width="118">
Title:</td>
<td style="background:#AAECB3;" width="501">
<strong>EI43: Nitrifying Biofilm Control Through the Utilisation of Catalytically Generated Nitric Oxide</strong></td>
</tr>
<tr>
<td style="background:#AAECB3;" width="118">
<p>Name:</p>
<p>Supervisor:</p>
<p>School:</td>
<td style="background:#AAECB3;" width="501">
<p>Vita Wonoputri</p>
<p>Rose Amal, Sanly Liu, May Lim</p>
<p>Chemical Engineering</td>
</tr>
</tbody>
</table>
<p>Bacteria commonly aggregate together and live as a complex multicellular body encased within a matrix of extracellular polymeric substances known as biofilms. Biofilm resistance to antimicrobials and disinfectants cause widespread problems in clinical and industrial settings, as they are difficult to eradicate using conventional methods. For instance, up to 1000 ppm of chlorine is needed to reduce the number of bacteria in biofilms, while only 10 ppm is needed to eradicate their free-swimming (planktonic) counterpart. Moreover, the use of antimicrobials has been shown to trigger the growth of biofilm with enhanced resistance, which drives the need of a novel anti-biofilm agent. One promising example is nitric oxide. Nitric oxide is a free radical gas that acts as a signalling molecule in many biological systems. Nitric oxide can act as anti-biofilm agents through two mechanisms depending on the concentration. At high concentration, nitric oxide can kill bacteria or inhibit cell proliferation, while at low concentration it signals the biofilm to move or disperse in a non-toxic manner, which allows the cells to revert to planktonic lifestyle and restore their sensitivity towards antimicrobials. Most importantly, the use of nitric oxide has not been shown to trigger the growth of more resistant bacteria. Although the use of nitric oxide shows a lot of potential, its utilisation is still limited by the need to control its release. Most studies have focused on the use of materials that can store and release nitric oxide, which only have limited longevity depending on the amount of nitric oxide that can be loaded up front. One alternative is to use catalytic technology that can convert endogenous nitrite to nitric oxide via contact with active catalyst surfaces in the presence of appropriate reactants. Herein, a copper(II) complex species embedded in a PVC matrix was used as the active surface. The copper(II) complex can react with appropriate reducing agents, such as ascorbic acid and iron(II), and the subsequently formed copper(I) will react with nitrite forming nitric oxide. The presence of copper catalyst is able to enhance and prolong nitric oxide generation compared to the presence of nitrite and reducing agents alone. Moreover, this method allows control over the amount and timing of nitric oxide generation by varying the concentration of the reactants and the time of addition, respectively. The generated nitric oxide is able to suppress biofilm formation without any loss of viability, as revealed by surface coverage and viability analysis using confocal laser scanning microscopy. This indicated that the generated nitric oxide works as a cell proliferation inhibitor. When biofilm was already formed, the combination of copper(II)-nitrite-ascorbic acid/iron(II) is effective at removing biofilm via cells dispersal, indicating that a different mechanism is employed depending on the biofilm growth stage (before or after biofilm formation). This study highlights the use of catalytically generated nitric oxide as a promising method to control biofilm formation on surfaces.</p>
</tr>
</tbody>
</table>
</script>
</body>
</html>
